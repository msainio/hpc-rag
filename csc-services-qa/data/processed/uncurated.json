[
    {
        "idx": 0,
        "question": [
            "Have I clicked the edit mode on?**"
        ],
        "answer": [
            "- A: Probably not yet.. â†–ï¸"
        ],
        "source": "csc-enveff-20230412"
    },
    {
        "idx": 1,
        "question": [
            "Slides available after the course?**"
        ],
        "answer": [
            "- A: They sure are! You will also have the access to this HedgeDoc document (save the link).",
            "    - The slides and tutorials are available here: https://csc-training.github.io/csc-env-eff/",
            "    - Access to e-lena (and the quizzes there) may discontinue.",
            "- We encourage you to share and use the material also in your own courses. The material is in git, and we welcome all feedback and edit suggestions (pull requests)!"
        ],
        "source": "csc-enveff-20230412"
    },
    {
        "idx": 2,
        "question": [
            "Has the Zoom meeting already started? I'm just gettin \"The meeting has not started\" -note.**"
        ],
        "answer": [
            "- A: Make sure you have the full Zoom link when connecting! It's long and contains the password :)"
        ],
        "source": "csc-enveff-20230412"
    },
    {
        "idx": 3,
        "question": [
            "Should we be able to access the slides in e-elena? I can only see the first slide.**"
        ],
        "answer": [
            "- A: Try navigating with the arrow keys :)"
        ],
        "source": "csc-enveff-20230412"
    },
    {
        "idx": 4,
        "question": [
            "So many credendials... Which ones do I need and where?!?**"
        ],
        "answer": [
            "- A: You need",
            "    - **CSC credentials** to Puhti/Mahti",
            "    - **Haka OR Virtu credentials** (=your university/institution credentials) to my.csc.fi",
            "    - **Haka credentials** (=your university/institution credentials) to eLena platform",
            "    - contact us if you don't have HAKA/Virtu in use"
        ],
        "source": "csc-enveff-20230412"
    },
    {
        "idx": 5,
        "question": [
            "I have forgotten my CSC username and password**"
        ],
        "answer": [
            "- A: In my.csc.fi you can check your CSC username. If you have forgotten your CSC password, check this link: https://docs.csc.fi/accounts/how-to-change-password/ -Note that it takes some time for the password to update to Puhti, if you need to change that :)"
        ],
        "source": "csc-enveff-20230412"
    },
    {
        "idx": 6,
        "question": [
            "Depending on the place you say to obtain acces to Puhti and Allas or Puhti and Mahti, should we have all three of them (just in case)?**"
        ],
        "answer": [
            "- A: On the course there is one tutorial in which you can try moving data from Puhti to Mahti. It is an optional task so you don't necessarily need Mahti."
        ],
        "source": "csc-enveff-20230412"
    },
    {
        "idx": 7,
        "question": [
            "Is there a way to paste text into linux terminal using (Finnish) keyboard only. (i mean without using right click). I am using PUTTY. On windows, how about `control + shift + C` for copying and `control + shift + V` It still does not work. :(**"
        ],
        "answer": [
            "- A: In Putty (and MobaXterm) paste works with shift + insert."
        ],
        "source": "csc-enveff-20230412"
    },
    {
        "idx": 8,
        "question": [
            "What is the price for billing units in commersial projects?**"
        ],
        "answer": [
            "  - A:     More about pricing of commercial projects: https://research.csc.fi/purchasing The base package costs 1190 EUR (VAT 0 %). It includes: 20 000 BUs, 4 user accounts. Note that LUMI has a different (more affordable) pricing: https://www.csc.fi/solutions-for-business"
        ],
        "source": "csc-enveff-20230412"
    },
    {
        "idx": 9,
        "question": [
            "What is the \"Jupyter for Courses\" on Puhti for?**"
        ],
        "answer": [
            "  - A:     This is kind of a placeholder on Puhti web interface to customise your own notebooks.For example when you build a notebook with your own python environment, it is possible to render your notebook application module or environment through \"Jupyter for Course\". More information can be found here: https://docs.csc.fi/computing/webinterface/jupyter-for-courses/"
        ],
        "source": "csc-enveff-20230412"
    },
    {
        "idx": 10,
        "question": [
            "Would any kind of cluster be called a supercomputer or when is the point reached?**"
        ],
        "answer": [
            "  - A: Many definitions given by our specialists :) Traditionally maybe smaller clusters in universities are/were called clusters, and bigger clusters in supercomputing centers were called supercomputers. Another characteristic to a supercomputer could be the queueing system (we hear more about that tomorrow), or the way the nodes are inter.connected."
        ],
        "source": "csc-enveff-20230412"
    },
    {
        "idx": 11,
        "question": [
            "For the Breakout rooms, what's the definition of beginners and not so beginner?**"
        ],
        "answer": [
            "  - A: Let's see after the talk :) Idea is to get roughly equal amount of participants in all rooms, and perhaps get those taking their very first steps with CSC services/clusters/supercomputers to one room, so maybe if you only got your credentials and logged in to Puhti and run your first unix commands for the very first time ever now for this course, you could go to the beginners room :)"
        ],
        "source": "csc-enveff-20230412"
    },
    {
        "idx": 12,
        "question": [
            "Which disk area shall I use if I'm reading and writing a lot of huge files (like several 5-10 GB fil)?Would $LOCAL_SCRATCH bring a performance boost?**"
        ],
        "answer": [
            "  - A:How large is huge?",
            "  - scratch is the correct one.",
            "  - You can try local_scratch. It may be faster, depending on what is the bottleneck of your work."
        ],
        "source": "csc-enveff-20230412"
    },
    {
        "idx": 13,
        "question": [
            "How the path can be saved using alias?**"
        ],
        "answer": [
            "  - A: By saving the path, do you mean something like `export PATH=/some/path/here:$PATH`?",
            "  - I would maybe not save it using an alias, but perhaps define a function that takes the path to be added as an input. E.g.",
            "```",
            "add_path () {",
            "    export PATH=\"$1:$PATH\"",
            "}",
            "```",
            "- Then path can be added with `add_path /my/test/path`",
            "- If by saving path you mean just a shortcut for `cd`, try `alias mypath=\"cd /path/to/my/important/directory\"` and then run `mypath` to quickly move there.",
            "- ðŸ’¡ Hint: You can use your folder under /scratch for the rest of the tutorials. You can save the path using an alias (with cd or echo) or somewhere in your notes. --> But this is in the tutorial",
            "-"
        ],
        "source": "csc-enveff-20230412"
    },
    {
        "idx": 14,
        "question": [
            "What means \"  Where:"
        ],
        "answer": [
            "   S:  Module is Sticky, requires --force to unload or purge\" after csc-tools module?**",
            "  - A: Modules marked as \"Sticky\" are not removed with command `module purge`. To remove them you have to use command `module --force purge` This is to protect essential modules from accidental removal."
        ],
        "source": "csc-enveff-20230412"
    },
    {
        "idx": 15,
        "question": [
            "In Puhti and Mahti, is it faster to get/send data from/to allas than from other servers, e.g. an university server?**"
        ],
        "answer": [
            "  - A: This depends on many factors. Some of the factors that affect the rate of file transfer is nature of files (lot of small files or few big files), speed of your university network and client (winscp/a-tools/rsync) you are using. The transfer of data within csc environment (Puhti, Mahti and Allas) is usually very fast."
        ],
        "source": "csc-enveff-20230412"
    },
    {
        "idx": 16,
        "question": [
            "I need to leave the session unfortunately, can you post the prep for next day instruction somewhere? Thank you it would be great if this is possible.**"
        ],
        "answer": [
            "  - A: Thank you for joining today! See you tomorrow at 9:00, same Zoom URL :) No specific preps needed!"
        ],
        "source": "csc-enveff-20230412"
    },
    {
        "idx": 17,
        "question": [
            "Is there some convenient way (like md5sum) to ensure that the files are not corrupted while moving them, for example, between Puhti and Allas?**"
        ],
        "answer": [
            "  - A:Not in a convenient way. You can store the checksum value in a file using 'md5sum' command on Puhti or Mahti before storing data to allas. Once you retrieve the data later, you can then check the checksum value again to see if you have preserved integrity of files."
        ],
        "source": "csc-enveff-20230412"
    },
    {
        "idx": 18,
        "question": [
            "When I use a-put to upload the  files to Allas, if it fails due to storage limit or time limit, will it create fragments and how to detect and remove them?**"
        ],
        "answer": [
            "  - A: a-put will try to check the availability of required space on Allas for your object before uploading. In case uploading fails for some reason, a-put command also deletes the partially uploaded files. Alternatively, you can also double check the same with a-list command (a-list bucket_name/beginning_of_the_object).",
            "- [x] **What are the ameta files created in Allas?**:",
            "- A: As discussed, it is a metadata file decscring the bucket. Files with  _ameta extension are automatically created when you upload files with 'a-put' command. These are analogous to the metadata files created by other swift/rclone tools"
        ],
        "source": "csc-enveff-20230412"
    },
    {
        "idx": 19,
        "question": [
            "In the Allas tutorial it's said that \"a-flip is meant for files that need to be published only temporarily, for example for a one-time share\". Are the files shared with this command automatically removed or became private after some time?**"
        ],
        "answer": [
            "- A: Yes, files uploaded with a-flip will be deleted once they are older than two days, see https://docs.csc.fi/data/Allas/using_allas/a_commands/#a-flip (the age will be checked when you run a-flip again)."
        ],
        "source": "csc-enveff-20230412"
    },
    {
        "idx": 20,
        "question": [
            "For the file name, should it must be without space?**"
        ],
        "answer": [
            "- A: This is highly recommended practice (not necessary though). Spaces in filenames can create problems when running Linux commands if not escaped properly",
            "- A: If we have for example file \"my file.txt\" and try to look at the content with cat command `cat my file.txt` cat will actually think there are two files: \"my\" and \"file.txt\". So we have to use quotes `cat \"my file.txt\"` or escape `cat my\\ file.txt`. There will be similar problems if file names contain characters that are used as part of commands (e.g. various brackets). While they can be handled with escape and quotes it can get a bit messy and spaces and reserved characters are best avoided in file names."
        ],
        "source": "csc-enveff-20230412"
    },
    {
        "idx": 21,
        "question": [
            "If I need to use a fast local disk ($local_scratch) - what is the conventional way to point it to the program? Do the programs provide own parameters for this purpose (like --tmp)?**"
        ],
        "answer": [
            "  - A: The programs don't provide own parameters (like --tmp)to refer to local scratch by default but you can point to $LOCAL_SCRATCH as a temporary directory when needed. One conventional option is to actually go to the local scratch directory on compute node and perform all your data analysis in that folder as shown roughly below:",
            "```",
            "- cd $LOCAL_SCRATCH (and also copy your data to the scratch folder and perform all preprocessing on data if needed there)",
            "- your program command (and refer the data in local scratch folder as input to your programme and direct all results to the local scratch area)",
            "- mv results_folder /scratch/<project>/$USER/  # remember to copy your results to scratch area",
            "```",
            "all of the steps have to be described  either in batch script (https://csc-training.github.io/csc-env-eff/hands-on/disk-areas/disk-areas-exercise-fastdisks.html) or as part of interactive jobs (https://docs.csc.fi/support/faq/local_scratch_for_data_processing/)"
        ],
        "source": "csc-enveff-20230412"
    },
    {
        "idx": 22,
        "question": [
            "What is a Docker container image?**"
        ],
        "answer": [
            "- A: You will learn more about containers later in this course. Docker is one of the popular container platforms. And the  docker image is a read-only template (or actually a layered file system in case of docker) that contains everything (=all runtime environment) for creating a container. In a way, you can think of container image as a standalone (executable) file to run your application."
        ],
        "source": "csc-enveff-20230412"
    },
    {
        "idx": 23,
        "question": [
            "After installing the metabat program with tykky, should I always add the bin directory to the $PATH before starting to work with program again?**"
        ],
        "answer": [
            "- A: Yes. You are right - one should to add 'bin' directory path to $PATH variable before calling metabat program",
            "- A: With Tykky installed software it is highly recommended to only add them to PATH when actually using them. The installation directories often contain common things like Python. Having those in your PATH can cause conflicts with other software."
        ],
        "source": "csc-enveff-20230412"
    },
    {
        "idx": 24,
        "question": [
            "Should I see from the documentation if the program can use more than one core? Is there some examples?**"
        ],
        "answer": [
            "  - A: Usually, yes. You can check the different flags (cores/threads) for your programme in that programmes documentation. But sometimes, it may not be obvious from documentation. You need to become familiar with the code you are using so that you know how it is best used; serial or parallel. There will be more information on different parallelization schemes / possibilities in the last lecture."
        ],
        "source": "csc-enveff-20230412"
    },
    {
        "idx": 25,
        "question": [
            "Could you recommend some nice tutorials to learn grep and awk?**"
        ],
        "answer": [
            "  - A: Here's CSC's own Linux guide which might be helpful to get started: https://docs.csc.fi/support/tutorials/env-guide/"
        ],
        "source": "csc-enveff-20230412"
    },
    {
        "idx": 26,
        "question": [
            "As the conda environments are containerized with tykky, is it enough to just delete the container when not needed anymore?**"
        ],
        "answer": [
            "  - A: Yes. You can just delete the whole installation folder."
        ],
        "source": "csc-enveff-20230412"
    },
    {
        "idx": 27,
        "question": [
            "Is it possible to leave this open over weekend? At least, I would like to do all the exercises in tutorials and it would be nice to have an option to ask.**"
        ],
        "answer": [
            "- A: Ok, we can do this, only for you nice & brave people <3 We will keep it open for editing until Monday. I would also like to remind that you can continue the exercises and join the weekly support sessions (every Wednesday at 14-15) to ask about the course things too! You are ALWAYS welcome to these sessions, and you'll meet our friendly faces there :) https://ssl.eventilla.com/usersupportcoffee"
        ],
        "source": "csc-enveff-20230412"
    },
    {
        "idx": 28,
        "question": [
            "Is it OK to share the course material to a collegue?**"
        ],
        "answer": [
            "- A: Absolutely! This material will stay here: Collection of slides and material Also, the online self-learning course in Elena is free and available for everyone, it contains the same material + some quizzes to check your learning: Course page in eLena platform And it is totally ok to join the weekly user support sessions to ask about the course exercises as well!",
            ">"
        ],
        "source": "csc-enveff-20230412"
    },
    {
        "idx": 29,
        "question": [
            "Is it possible to have sudo rights inside a container?**"
        ],
        "answer": [
            "  - A: Only if you have sudo rights in the host. When run with user rights, you only have user rights in the container."
        ],
        "source": "csc-enveff-20230412"
    },
    {
        "idx": 30,
        "question": [
            "Is it correct that single tasks can be split to at most 20 CPUs on Puhti only, or is there any option to spool up more CPUs for a single task? I would imagine that this is not possible since the 2x20 CPUs on a single node are not sharing the same chache, right?**"
        ],
        "answer": [
            "- A: Each Puhti node has 40 physical cores (2 CPUs with 20 cores each), so you can launch at most 40 threads from a single task."
        ],
        "source": "csc-enveff-20230412"
    },
    {
        "idx": 31,
        "question": [
            "What are the differences between /scratch and $LOCAL_SCRATCH**:"
        ],
        "answer": [
            "   - A: Some differences:",
            "Technical: /scratch is on Lustre parallal file system. $LOCAL_SCRATCH is on a physical NVMe solid state disk installed in the node",
            "Speed: /scratch is slower and performance can suffer especially if accessing many small files. $LOCAL_SCRATCH is fast and can speed up jobs that rely heavi on disk I/O.",
            "Visibility: /scratch is visible from all nodes. $LOCAL_CRATCH is specfic to a single node.",
            "Permanence: Project /scratch directory will exist for the life time of the project. Files wil exist 180 days (if unused). $LOCAL_SCRATCH allocation is specific to a job. After the job finishes, the allocation and files will be deleted. Remember to copy all necessary files to /scratch as part of the job."
        ],
        "source": "csc-enveff-20230412"
    },
    {
        "idx": 32,
        "question": [
            "After containerizing a Conda environment with Tykky, is the correct way to use it just to add the bin to the path and start to use different tools?**"
        ],
        "answer": [
            "- Yes! These \"binaries\" for the different tools are actually wrapper scripts that do quite a bit of things automatically for you and finally call the actual binaries within the containers. Nothing else needs to be done."
        ],
        "source": "csc-enveff-20230412"
    },
    {
        "idx": 33,
        "question": [
            "Have I clicked the edit mode on?**"
        ],
        "answer": [
            "- A: Probably not yet.. â†–ï¸"
        ],
        "source": "csc-enveff-20230919"
    },
    {
        "idx": 34,
        "question": [
            "Slides available after the course?**"
        ],
        "answer": [
            "- A: They sure are! You will also have the access to this HedgeDoc document (save the link).",
            "    - The slides and tutorials are available here: https://csc-training.github.io/csc-env-eff/",
            "    - Access to e-lena (and the quizzes there) may discontinue.",
            "- We encourage you to share and use the material also in your own courses. The material is in git, and we welcome all feedback and edit suggestions (pull requests)!"
        ],
        "source": "csc-enveff-20230919"
    },
    {
        "idx": 35,
        "question": [
            "Has the Zoom meeting already started? I'm just gettin \"The meeting has not started\" -note.**"
        ],
        "answer": [
            "- A: Make sure you have the full Zoom link when connecting! It's long and contains the password :)"
        ],
        "source": "csc-enveff-20230919"
    },
    {
        "idx": 36,
        "question": [
            "Should we be able to access the slides in e-elena? I can only see the first slide.**"
        ],
        "answer": [
            "- A: Try navigating with the arrow keys :)"
        ],
        "source": "csc-enveff-20230919"
    },
    {
        "idx": 37,
        "question": [
            "So many credendials... Which ones do I need and where?!?**"
        ],
        "answer": [
            "- A: You need",
            "    - **CSC credentials** to Puhti/Mahti",
            "    - **Haka OR Virtu credentials** (=your university/institution credentials) to my.csc.fi",
            "    - **Haka credentials** (=your university/institution credentials) to eLena platform",
            "    - contact us if you don't have HAKA/Virtu in use"
        ],
        "source": "csc-enveff-20230919"
    },
    {
        "idx": 38,
        "question": [
            "I have forgotten my CSC username and password**"
        ],
        "answer": [
            "- A: In my.csc.fi you can check your CSC username. If you have forgotten your CSC password, check this link: https://docs.csc.fi/accounts/how-to-change-password/ -Note that it takes some time for the password to update to Puhti, if you need to change that :)"
        ],
        "source": "csc-enveff-20230919"
    },
    {
        "idx": 39,
        "question": [
            "Depending on the place you say to obtain acces to Puhti and Allas or Puhti and Mahti, should we have all three of them (just in case)?**"
        ],
        "answer": [
            "- A: On the course there is one tutorial in which you can try moving data from Puhti to Mahti. It is an optional task so you don't necessarily need Mahti."
        ],
        "source": "csc-enveff-20230919"
    },
    {
        "idx": 40,
        "question": [
            "Can FileZilla, WinSCP or Putty be used to transfer files directly to Allas from my local machine?**"
        ],
        "answer": [
            "- I think you can access Allas from your browser, at least to download objects, but not sure if you can upload anything",
            "- I think you cannot upload files to Allas via any of those listed file transferring tools, you can use/acces the Allas from https://sd-connect.csc.fi"
        ],
        "source": "csc-enveff-20230919"
    },
    {
        "idx": 41,
        "question": [
            "Can I use VSCode remote explorer to connect to Puhti ?**"
        ],
        "answer": [
            "- A: I have not tried it myself, but should be possible",
            "- A: See here: https://docs.csc.fi/support/tutorials/remote-dev/"
        ],
        "source": "csc-enveff-20230919"
    },
    {
        "idx": 42,
        "question": [
            "Am I right with the assumption about BU: if I apply for too much memory, the set memory gets billed? But with runtime/CPU usage just the used one gets billed?**"
        ],
        "answer": [
            "- A: Yes. This is correct. You can think of it this way: what has been allocated for you, i.e. what is not available for others, is billed. The memory is allocated for you _for the duration of the job_ and not available for others and is billed. If the job ends before the time you specified in the batch script, the resources are freed and they are again available for others --> not billed."
        ],
        "source": "csc-enveff-20230919"
    },
    {
        "idx": 43,
        "question": [
            "Does using SSH for Puhti mean you can access files directly from your own computer or do you still need to store the data you want to use in Allas/csc services?**"
        ],
        "answer": [
            "- A: SSH for Puhti means here \"just\" a terminal connection. You'll get the command line and you can access files etc. The File browser is separate.",
            "- **I didn't quite understand. If I use SHH from my work laptop, so I'm accesing Puhti through terminal, am I then able to for example do DNA sequence aligment on FASTQ files on my local disk or do these files need to be stored in CSC for me to be able process them with Puhti?**",
            "- A: Yes, you need to copy the files to CSC to be able to use the Puhti software and computing capacity. With ePouta, the situation might be different, but it's not covered here."
        ],
        "source": "csc-enveff-20230919"
    },
    {
        "idx": 44,
        "question": [
            "I have problems when connecting through ssh to Puhti with the foot terminal emulator. When running commands such as \"clear\" I get an error message saying that \"foot\" is an unknown terminal type. Thanks in advance!**"
        ],
        "answer": [
            "- A: This is probably slightly more complicated issue that would require more information. Probably this application is not compatible with Puhti. Maybe you could try using the Puhti web interface for the duration of the course?",
            "- Me again: thanks for the answer! Indeed the web interface works great and I can also use a different terminal emulator. I managed to solve my problem by copying the cofiguration files (`/usr/share/terminfo/f/foot*`) of my emulator to Puhti (`~/.terminfo/f/`)."
        ],
        "source": "csc-enveff-20230919"
    },
    {
        "idx": 45,
        "question": [
            "How can I download the material from Git?. It saves only as html.**"
        ],
        "answer": [
            "- A: Do you refer to the course slides? These can be viewed in the browser as pages (navigate with arrow keys).",
            "- I mean if I want to download the slides to use them in my own courses. So far I can only view them in the browser.",
            "- A: Yes, since they are html files, you can only view them using a browser. Html files can be converted to PDF using e.g. chromium browser if needed.",
            "- Q: Thank you. It does save as PDF but you need to do that slide by slide (tedious). Are you aware of an alternative batch solution?"
        ],
        "source": "csc-enveff-20230919"
    },
    {
        "idx": 46,
        "question": [
            "If I want to install a tiny software (e.g., home-made script or .exe), what should I do?**"
        ],
        "answer": [
            "- A: This depends a lot. If it is just e.g. a Python script, you can copy it to the supercomputer and try to run it using some of the Python modules available (or install a Python environment of your own). With .exe file (executables/binaries), it should support Linux operating system. If this is the case and the application is just a serial (or threaded) application, you can try to run it directly. Otherwise you should probably recompile the application to ensure that it performs well. See https://docs.csc.fi/computing/installing/ for more info."
        ],
        "source": "csc-enveff-20230919"
    },
    {
        "idx": 47,
        "question": [
            "What should I exepct to see after enabling the Remote graphics option -X when connecting with ssh? (I am using Xterminal on windows)**"
        ],
        "answer": [
            "- A: `-X` option will enable to forward graphics from the supercomputer to your local display. So you should not expect to see anything after ssh, but if you try opening e.g. a picture with `eog` it should work. If not using the `-X` option you would get an error like `cannot open display`"
        ],
        "source": "csc-enveff-20230919"
    },
    {
        "idx": 48,
        "question": [
            "Do you intend to add GPU optimized tensorflow-federated libraries in the future? I am currently using one installed myself.**"
        ],
        "answer": [
            "   - A: Normal Tensorflow we already have (GPU optimized of course). We don't have TensorFlow Federated yet, although I suspect it can be easily added on top of the normal module by a user with a command: `pip install --user tensorflow-federated`. If you think it should be included in the CSC-provided module, you can send an email request to servicedesk@csc.fi."
        ],
        "source": "csc-enveff-20230919"
    },
    {
        "idx": 49,
        "question": [
            "What is the best approach to keep track of the directories & objects stored in Allas and Puhti? I have experience backing up files in Allas before scratch purge, but it is quite complicated to keep track which file is backed up and which is not. What I am doing is to delete the entire directory which I have backed up in Allas and download the same object back to refresh the 180 days timer (mainly to keep the dependent path viable).**"
        ],
        "answer": [
            "- A: There are many ways to organize your files and to keep track of what is / should be backed up and what is disposable, and you might need to do some experimenting and soul-searching to find the solution that works the best for you, but some options could be e.g.",
            "    1. Mentally define some specific directories (say, `data`, `plots`, `scripts` and whatnot) as important, regularly back those up and commit to making sure that no important data is stored outside those directories",
            "    1. Back up \"everything\" (e.g. your home directory and the project's scratch directory) but exclude some specific directories (like `tmp` or `testing` or `wip`) that you use for storing temporary files",
            "    1. The first option, but all the important directories are contained within one directory for easy backing up (so you have one directory, e.g. `backed_up`, that contains `data`, `plots` etc)"
        ],
        "source": "csc-enveff-20230919"
    },
    {
        "idx": 50,
        "question": [
            "Is there somewhere examples and/or instruction to use the $LOCAL_SCRATCH, i have never seen this in the csc-instructions ?**"
        ],
        "answer": [
            "- A: Yes, see for example here: https://docs.csc.fi/computing/running/creating-job-scripts-puhti/#local-storage",
            "- I would need more instructions than just the one line code to be added to the job submission script, not very good if you're not advanced user",
            "   - A: Please check this for description and relation to other storage options: https://docs.csc.fi/computing/disk/#compute-nodes-with-local-ssd-nvme-disks Do you have a specific question, e.g. what do you want to achieve?",
            "   - A: The ML guide has one example script: https://docs.csc.fi/support/tutorials/ml-data/#fast-local-drive-puhti-and-mahti-only"
        ],
        "source": "csc-enveff-20230919"
    },
    {
        "idx": 51,
        "question": [
            "how do you see the disk usage in ALLAS**"
        ],
        "answer": [
            "  - A: Our lecture/tutorial on ALLAS will come later in this course. For now, one easiest way to find ALLAS disk usage is to check on our pouta web page (HORIZON interface). First, login at www.pouta.csc.fi with your CSC account and then select a project under which you have uploaded some files to allas before. On the left hand side panel, select 'Object store' and  then 'Container' tab. You can then see different objects (within containers) you have created. If you click an object, you can then see the disk space used by the object.",
            "  - *a-info* command on Puhti/Mahti gives you a overview of disk space used for data objects in different buckets. Unfortunately, there is no way to check the amount of diskspace left in your ALLAS quota"
        ],
        "source": "csc-enveff-20230919"
    },
    {
        "idx": 52,
        "question": [
            "my permission was denied when trying to copy files to Mahti. I tried using my csc password, what could be wrong?**"
        ],
        "answer": [
            "  - A: Mahti service might not have been activated for the project you are using. You can check in MyCSC web portal.",
            "  - - Ahh yes that makes sense. Thanks!"
        ],
        "source": "csc-enveff-20230919"
    },
    {
        "idx": 53,
        "question": [
            "Hmm I tried to run command `$echo $TMPDIR` and it outputs blank, so nothing. Is this normal ?**"
        ],
        "answer": [
            "  - A: On login nodes, one should be able to see something like the following: /local_scratch/$USER.",
            "  - okay so there is some kind of problem, since I assume that using ssh and normally logging in to Puhti will use login node?",
            "  - A: yes. If you login using ssh, you will end up in login nodes",
            "  - What should I do to this problem ?",
            "  - A: Problem was solved in breakout room",
            "  - great !"
        ],
        "source": "csc-enveff-20230919"
    },
    {
        "idx": 54,
        "question": [
            ": How do I estimate time and resources that I will use when using the billing calculator? Is it a matter of experience to know how much different jobs will take or are there examples somewhere?**"
        ],
        "answer": [
            "- A:  Time and resources are very much depend on your job. So your experience with your job can only help you. If you have jobs that take quite long time and heavy resources, you can start with small datasets and then extrapolate the resources accordingly to bigger datasets.  BUs are also computed for data storage. You can use BU calculator to get some estimates for storing data.",
            "- So if I have never run a job in Puhti, or in general, before, how can I estimate GPU/time etc?",
            "- That is challenging to get any realistic estimates without actually running your job.",
            "- A: As the first estimate, you could use the same time it takes for your calculation on your own computer (or whatever you are using now). The benefit of HPC is, that you can throw more resources at the same problem - either as a truly parallel job (openMP, MPI) or running many at the same time (farming). In practice, you'll learn to make better estimates with experience and if you didn't apply enough initially, you can always apply more later."
        ],
        "source": "csc-enveff-20230919"
    },
    {
        "idx": 55,
        "question": [
            "For the completion of the course (i.e. getting the course certificate): Do I need to click also on all the ressources in the learning platform or just make the quizzes? Because the progress bar seems to depend on the resource clicking as well?**"
        ],
        "answer": [
            "- The progress bar function has been acting up lately :(  Quizzes + the feedback should be enough to get the certificate."
        ],
        "source": "csc-enveff-20230919"
    },
    {
        "idx": 56,
        "question": [
            "How do I actually use the billing unit calculator? The example in the quiz features GPUs as well and how do I account for those? And the billing unit calculator does not seem to combine memory and task/CPU calculation? But the quiz question only want one result?**"
        ],
        "answer": [
            " - A: In the BU calculator you can add multiple jobs into a single estimate. The calculator is also a way to look at the relative BU cost of different resources (or you can look at the BU formula from Docs CSC: https://docs.csc.fi/computing/hpc-billing/ ). i.e. How significant is the CPU or Memory based billing unit cost, _if_ the job is using GPUs? Note also, that with BUs we try to encourage researchers to use the resources efficiently: reserve what you need but not more."
        ],
        "source": "csc-enveff-20230919"
    },
    {
        "idx": 57,
        "question": [
            "Where should conda package containers be installed if we would like everyone on a project to be able to use them?**"
        ],
        "answer": [
            " - A: In */projappl/<project>/*  directory. You will learn more about it later in the course"
        ],
        "source": "csc-enveff-20230919"
    },
    {
        "idx": 58,
        "question": [
            "I have created several buckets in Allas that each contain many files. Is it possible to compress these into a single .zip within Allas? Or what would be the best way to do this?**"
        ],
        "answer": [
            " - A:  ALLAS objects are immutable  and can't be modified within ALLAS.  You have to bring the data out of ALLAS, zip it outside and upload it again. Some tools like a-put can archive (using tar) before uploading. a-put command also has compression option with ```-c```flag. More info here: https://docs.csc.fi/data/Allas/using_allas/a_commands/#a-put-uploads-data-to-allas"
        ],
        "source": "csc-enveff-20230919"
    },
    {
        "idx": 59,
        "question": [
            "For the first running of the task, how to approximate how much resources needed?**"
        ],
        "answer": [
            " - A: Feel free to test with small resources just to see how far you can succeed with your job. It is a good idea to understand your programme/software/tool before using it. You can for example check if your program supports multiple threads/cores, needs huge/low memory or supports GPUs or not etc. These resource requirement also changes based on the amount of input data. Sometimes you have to tune your resources based on wall-clock time permissible on the partition you are using in supercomputing environment. Yes. this is a bit of exploration process in the beginning especially if you are handling complex software"
        ],
        "source": "csc-enveff-20230919"
    },
    {
        "idx": 60,
        "question": [
            "Just out of curiosity, how is the billing unit calculated in relation to the actual cost of the resources used?**"
        ],
        "answer": [
            " - A: You can check the actual costs in relation to BUs in my.csc (https://my.csc.fi/billing-unit-calculator). Please note that BUs are just CSC's way of finding how much resources are being used and these resources are free-of-charge for open science research."
        ],
        "source": "csc-enveff-20230919"
    },
    {
        "idx": 61,
        "question": [
            "Regarding the resource usage, it would be very helpful to have a step by step guide / example how to do the calculation, testing and finally running the job. It is hard to account for all possible scenarios, but some guidelines, examples ?**"
        ],
        "answer": [
            " - A: Thanks for the feedback!"
        ],
        "source": "csc-enveff-20230919"
    },
    {
        "idx": 62,
        "question": [
            "Can I individually allocate the RAM for each core of the job?**"
        ],
        "answer": [
            " - A: In SBATCH directives of your batch script you can use ```--mem-per-cpu= ```flag.  Try ```sbatch --help ```for more help."
        ],
        "source": "csc-enveff-20230919"
    },
    {
        "idx": 63,
        "question": [
            "Simple thing, how do I open the slides in the course info page, I can see the tab, the hyperlink is working, but the slide inside is only with the first covering page**"
        ],
        "answer": [
            " - A: Try navigating with the arrow keys (left and right)! :)",
            " - Ahh ok, sorry, didn't check the arrow keys haha"
        ],
        "source": "csc-enveff-20230919"
    },
    {
        "idx": 64,
        "question": [
            "Is this course linked to university credit system? If so, what should we do to receive the credits?**"
        ],
        "answer": [
            " - A: Not directly, but you can get a course certificate, which recommends 0.5 credits. You can then take this certificate to your faculty, and they will (usually) then mark you the credits. To get the certificate, you need to complete all the quizzes in e-lena AND give the course feedback. After these steps, you should be able to download the course certificate in e-Lena."
        ],
        "source": "csc-enveff-20230919"
    },
    {
        "idx": 65,
        "question": [
            "How do I know if I have to use collated parallel IO method or when running the simulation \"normally\" is sufficient? The software I use is openfoam.**"
        ],
        "answer": [
            " - A: Collated method should be used specially when number of your output/results files is large. If your mesh is large, which normally implies lot or sub domains, lot of time (or iteration) steps, and all data, including lot variables, is written on disk, then, handling writing in those numerous output files become hard for Lustre file system, which is used on CSC's servers. It is good question of where is the limit when collated method should be used. if the number of sub domains (= processes) with non-collated method become larger than 120, I would use collated method - I don't see reason why not. Feel free to contact me, so we can discuss more about your case. Esko JÃ¤rvinen, CSC, esko.jarvinen@csc.fi"
        ],
        "source": "csc-enveff-20230919"
    },
    {
        "idx": 66,
        "question": [
            "Off-topic question: Any tips for keeping track of different versions of the models?**"
        ],
        "answer": [
            " - A: If you are referring to machine learning models, take a look at MLflow. We also have a guide on how to use MLflow on Puhti: https://docs.csc.fi/support/tutorials/ml-workflows/",
            "- Thank you for the link! Actually, my models are simpler fluid dynamics simulations but I test e.g. different meshes and then try to compare the results to see if they have any differences. Maybe github is suitable for this kind of version control?",
            "- **(not an instructor)**: Hi, I do work on the same topic using `OpenFOAM` and I rely heavily in `git` for version control of models. As long as your code uses text based files as input it works very well. Just be careful with the amount of information you're uploading. For example, I avoid uploading any results.",
            "- Tip: Code Refinery offers some training and tips related to version control, they have one course ongoing at the moment, and you can hop on-board :) https://coderefinery.org",
            "    - **(same not instructor)**: is this course only online? A: Yep, I think so! Code Refinery has this pretty effective & nice \"whole world is invited\" approach to their knowledge sharing <3. Thanks!! I'll be there :-)",
            "- Thank you for the tips!"
        ],
        "source": "csc-enveff-20230919"
    },
    {
        "idx": 67,
        "question": [
            "Have I clicked the edit mode on?**"
        ],
        "answer": [
            "- A: Probably not yet.. â†–ï¸"
        ],
        "source": "csc-enveff-20231128"
    },
    {
        "idx": 68,
        "question": [
            "Slides available after the course?**"
        ],
        "answer": [
            "- A: They sure are! You will also have the access to this HedgeDoc document (save the link).",
            "    - The slides and tutorials are available here: https://csc-training.github.io/csc-env-eff/",
            "    - Access to e-lena (and the quizzes there) may discontinue.",
            "- We encourage you to share and use the material also in your own courses. The material is in git, and we welcome all feedback and edit suggestions (pull requests)!"
        ],
        "source": "csc-enveff-20231128"
    },
    {
        "idx": 69,
        "question": [
            "Has the Zoom meeting already started? I'm just gettin \"The meeting has not started\" -note.**"
        ],
        "answer": [
            "- A: Make sure you have the full Zoom link when connecting! It's long and contains the password :)"
        ],
        "source": "csc-enveff-20231128"
    },
    {
        "idx": 70,
        "question": [
            "Should we be able to access the slides in e-elena? I can only see the first slide.**"
        ],
        "answer": [
            "- A: Try navigating with the arrow keys :)"
        ],
        "source": "csc-enveff-20231128"
    },
    {
        "idx": 71,
        "question": [
            "So many credendials... Which ones do I need and where?!?**"
        ],
        "answer": [
            "- A: You need",
            "    - **CSC credentials** to Puhti/Mahti",
            "    - **Haka OR Virtu credentials** (=your university/institution credentials) to my.csc.fi",
            "    - **Haka credentials** (=your university/institution credentials) to eLena platform",
            "    - contact us if you don't have HAKA/Virtu in use"
        ],
        "source": "csc-enveff-20231128"
    },
    {
        "idx": 72,
        "question": [
            "I have forgotten my CSC username and password**"
        ],
        "answer": [
            "- A: In my.csc.fi you can check your CSC username. If you have forgotten your CSC password, check this link: https://docs.csc.fi/accounts/how-to-change-password/ -Note that it takes some time for the password to update to Puhti, if you need to change that :)"
        ],
        "source": "csc-enveff-20231128"
    },
    {
        "idx": 73,
        "question": [
            "Depending on the place you say to obtain acces to Puhti and Allas or Puhti and Mahti, should we have all three of them (just in case)?**"
        ],
        "answer": [
            "- A: On the course there is one tutorial in which you can try moving data from Puhti to Mahti. It is an optional task so you don't necessarily need Mahti."
        ],
        "source": "csc-enveff-20231128"
    },
    {
        "idx": 74,
        "question": [
            "Hi, I am in Chicago for a last minute conference. Thought I could stay on but canâ€™t as it is 2 am and I have a presentation tomorrow. Hope there is a recording of this**"
        ],
        "answer": [
            "- A: Thereâ€™s no separate recording, but the lecture videos, slides and hands-ons are available in the github page, and all the material + the quizzes etc can be accessed in eLena platform, so this course works well as self-study also!"
        ],
        "source": "csc-enveff-20231128"
    },
    {
        "idx": 75,
        "question": [
            "After I run a  job on Puhti and the job closes, my home directory is filled with temp files created during the job (not a part of the input/output). Can I delete these?**"
        ],
        "answer": [
            "- A: If you do not need them (they do not contain important info), then yes it's a good idea to delete them. The Home quota is also quite small, so if there's a lot of temp files being written might make sense to tell the program to write them to the local disk (`$LOCAL_SCRATCH`) instead. Writing a lot (thousands) of small files to the shared file system (e.g. home or scratch) might also worsen performance significantly, so using the local disk is recommended if dealing with large amounts of temporary files."
        ],
        "source": "csc-enveff-20231128"
    },
    {
        "idx": 76,
        "question": [
            "Hi, I can connect to mahti in vscode, but cannot connect with puhti. Terminal connection with ssh works but I think it is related to some files...?NÃ¤yttÃ¶kuva 2023-11-28 103719**"
        ],
        "answer": [
            "- A: Would it be possible that VSCode tabs and windows from previous app sessions of mahti are not closed before connecting with Puhti?"
        ],
        "source": "csc-enveff-20231128"
    },
    {
        "idx": 77,
        "question": [
            "I tried to setup SSH keys (generated with puttygen), but get the following error on login: \"Server refused our key\". I've added the public key both to my MyCSC profile and to ~/.ssh/authorized_keys**"
        ],
        "answer": [
            "- A: Just checking one thing: does your public key in file ~/.ssh/authorized_keys starts something like this: ```ssh-rsa  ...... ```? [Yes]",
            "what kind of file permissions you have ? ```ls -l .ssh/authorized_keys``` [-rw-rw----, this may be the problem?]",
            " could you try to set to have the following permissions: ``` -rw-------.``` [Did not help.]",
            " [I got it working now :) It was a stupid user error on my part, had selected the wrong private key file in Putty. Thanks for the help.]",
            " Great that you got it workng... we all do such mistakes.."
        ],
        "source": "csc-enveff-20231128"
    },
    {
        "idx": 78,
        "question": [
            "Could you explain please, there is this limitation \"do not read plenty of small files\", but it supposed to be ok to read multiple from @LOCAL_SCRATCH. So if the script is running from let's say projappl, but the folder with the files is in local_scratch, is it ok to read them iteratively? Am I missing smth? Thanks!**"
        ],
        "answer": [
            "- A:  Yes. Main idea is that when your application (which can be installed on /ProJappl but can be called from anywhere) requires reading (or writing) thousands of small files, LOCAL_SCRATCH (=local SSD drive which are designed for faster reading and writing of files) is very efficient (= you can get improved wall-clock time). On lustre parallel file system (e.g., on /scratch/..), it would take more time for the same job and also causes a bit of overhead on file system."
        ],
        "source": "csc-enveff-20231128"
    },
    {
        "idx": 79,
        "question": [
            "In the Slurm bash script example, srun was placed before each command (e.g. srun hostname; srun sleep 60). Is it needed?**"
        ],
        "answer": [
            "- A: It is used to tell the command to actually use the resources that were requested. This is important if you request e.g. multiple compute cores to use. If forgotten, the job would just run using a single core and the other requested resources would idle and thus be wasted."
        ],
        "source": "csc-enveff-20231128"
    },
    {
        "idx": 80,
        "question": [
            "What's the difference between srun and sbatch?**"
        ],
        "answer": [
            "   - A: Sbatch is used to submit a batch job script. Srun is used to submit job directly. Anything in the #SBATCH lines in a batch job script can be given as command line arguments for srun. Srun can be handy for submitting simple jobs, but for more complex jobs, writing a batch job script is probably clearer. Batch job scripts also give you a record of what you did, so they can make troubleshooting easier."
        ],
        "source": "csc-enveff-20231128"
    },
    {
        "idx": 81,
        "question": [
            "With the exercise on batch jobs, trying to submit the job (sbatch my_serial.bash) results in an error with \"Invalid account or account/partition combination specified\". The project name should be correct at least.**"
        ],
        "answer": [
            "- My colleague advised me to change the partition to \"fmi\", and now it worked."
        ],
        "source": "csc-enveff-20231128"
    },
    {
        "idx": 82,
        "question": [
            "Can I run something like sacct from jupyter or in interactive mode when there is no batch job submitted? How to know how much billing units I consume or if the requested resourses are optimal?**"
        ],
        "answer": [
            " - A: `sacct` and `seff` can be used for any job ids irrespective of whether they have already completed or are still running. Note that the results might however be accurate only after the job has finished. Shell commands like sacct and seff can in principle be used in jupyter by adding an exclamation point before the command, e.g. `!sacct -j 1234567`. Seff will print the consumed billing units and to know if the requested resources were used optimally, check the CPU (or GPU efficiency) as well as the memory efficiency. These should ideally be close to 100%, although such a high efficiency is typically not possible. Just make sure that the efficiency is not too low. On mahti 50% CPU efficiency is also very good, since the calculation takes into account multithreading."
        ],
        "source": "csc-enveff-20231128"
    },
    {
        "idx": 83,
        "question": [
            "Not a question, but a tip. Didn't see any information about here in the course material, but based on previous Slurm use, I find it very useful to get email notifications about jobs starting and finishing with the following additions to the job submission script:**"
        ],
        "answer": [
            "```",
            "#SBATCH --mail-type=ALL # Send you an email message when your job starts and ends. Can also be e.g. BEGIN or END for just one or the other",
            "#SBATCH --mail-user=X@Y.Z # If you want to specify a custom email address (otherwise I believe it is the one associated with your CSC account)",
            "```",
            " - Good tip, thanks for sharing!"
        ],
        "source": "csc-enveff-20231128"
    },
    {
        "idx": 84,
        "question": [
            "Question regarding to data storage and sharing: Human data should be encrypted if it is put to internet. Do you have some encryption tools?**"
        ],
        "answer": [
            "  - A: You can find the instructions for encoding your sensitive data here:  https://docs.csc.fi/data/Allas/allas_encryption/. You can also find more information on handling sensitive data services here: https://docs.csc.fi/data/sensitive-data/ and pay attention to SD connect service: https://docs.csc.fi/data/sensitive-data/sd_connect/"
        ],
        "source": "csc-enveff-20231128"
    },
    {
        "idx": 85,
        "question": [
            "Sorry I most likely just missed this point, but could you re-iterate the prices/quotas for Allas storage for academic use? Thanks**"
        ],
        "answer": [
            " - A: https://research.csc.fi/pricing-and-resources",
            " - https://research.csc.fi/billing-units",
            " - https://research.csc.fi/quotas",
            " - Default quotas for free academic use: Storage\t10 TiB, Buckets per project\t1 000, Objects per bucket\t500 000",
            " - If you are using CSC's services free-of-charge and need to store more than 200 TiB in Allas, please contact servicedesk@csc.fi in order to agree with CSC about storage terms and possible costs."
        ],
        "source": "csc-enveff-20231128"
    },
    {
        "idx": 86,
        "question": [
            "When I pushed a new text file from puhti to allas with a-put, it automatically added extension .tar. Is this the way it should work?**"
        ],
        "answer": [
            " - A: No. The tar extension should be added only if you add a directory to Allas with a-put."
        ],
        "source": "csc-enveff-20231128"
    },
    {
        "idx": 87,
        "question": [
            "Is there any advantage of using rclone over a-commands, or the other way around?**"
        ],
        "answer": [
            " - rclone is typically faster as it does not do the checks and pre-prosessing that a-put/a-get does.",
            " - a-put/a-get provide you easier syntax and some automatized checks and pre/prot-prosessing steps. Also in the case of batch jobs a-commands allow you to automtaically re-generate the Allas token ( if the original Allas connection in opended with allas-conf -k)."
        ],
        "source": "csc-enveff-20231128"
    },
    {
        "idx": 88,
        "question": [
            "How to know if I am requesting correct amount of resources for an interactive desktop session? I did not see those sessions with sacct even though I have run them before the course.**"
        ],
        "answer": [
            "- A: sacct shows by default only jobs that were run on that day. To query for older jobs, use the `-S` flag (for Start), e.g. `sacct -u $USER -S2023-11-01` to see all your jobs that started after 1st Nov 2023. Remember to not query too long time intervals since the command is a quite heavy database operation."
        ],
        "source": "csc-enveff-20231128"
    },
    {
        "idx": 89,
        "question": [
            "What is the command to make a data already available in allas (but not in puhti) public, can it be done?**"
        ],
        "answer": [
            " - `a-publish` will make the file public. If you just want to share a file temporarily, you can also use `a-flip` (the file will become unavailable after a while).",
            " - note that access rights are controlled on bucket level: you can either publish all the objets in a bucket or none of them.",
            " - you can use https://pouta.csc.fi interface or `a-publish -b bucket-name` to make a bucket public."
        ],
        "source": "csc-enveff-20231128"
    },
    {
        "idx": 90,
        "question": [
            "Is it possible to request software to be added to the modules maintained by CSC? Or have some software installed by CSC personnel to own projappl directory? These installations are quite difficult without Linux or coding knowledge.**"
        ],
        "answer": [
            "  - A: You can always ask. We will assess the situation case by case and decide wheher to add the software to our selection or not. Even if we choose not to make a public installation, we can provide you with detailed instructions on how to install it yourself. We can't install it in your /projappl for you, since we specialists do not have read or write access to your directories. (This is also good to keep in mind when writing a help request to servicedesk: You need to include or somehow share the files you want us to take a look at. We can't simply check them from your /scratch etc.)"
        ],
        "source": "csc-enveff-20231128"
    },
    {
        "idx": 91,
        "question": [
            "I use snakemake pipelines in a lot of my work, and see it is available preinstalled. What would be the best approach for using it efficiently on Puhti? Can one use Snakemake's `--slurm --default-resources slurm_account=<project name?> slurm_partition=<partition>` parameters to launch the pipeline and have it submit the individual jobs to Slurm? Do you happen to have some example launch script available?**"
        ],
        "answer": [
            "- A :(I am not a specialist in Snakemake workflows so I will answer at general level) one can use natively implemented slurm executor for your Snakemake pipeline and you can check the toy example here: https://yetulaxman.github.io/containers-workflows/hands-on/day4/snakemake.html. However, if you have lot of small job steps (or sub tasks) in your workflow, we advice submitting your job as a normal batch job with proper resoiurces  and use *local*   execution instead.  This means, all of your the sub tasks will be run inside of the same resource allocation. If you have too many small jobs (say several hundreds or even thousands), our suggestion in your case is to use HyperQueue executor (instead of slurm) where workers submit all of your sub-jobs to the same resource allocation (= everything happens in one batch job). For this, you should use the `--cluster` option and e.g. HyperQueue, see https://docs.csc.fi/apps/hyperqueue/#using-snakemake-with-hyperqueue"
        ],
        "source": "csc-enveff-20231128"
    },
    {
        "idx": 92,
        "question": [
            "I want to train and retrain models on Puhti, and track metrics with MLflow which is running in Rahti. For this I need to be able to change env variables. Is it possible to create conda environments or what would be the go-to solution?**"
        ],
        "answer": [
            "- A: If you mean that you need to set environment variables for the runs in Puhti, you can do that with the command `export VARIABLE=value` where \"VARIABLE\" is the name of the variable to set and \"value\" the value you want to set it to. This command can be given in the Slurm batch job script or in the terminal before launching the job. In fact we have a tutorial for using MLflow which shows some examples: https://docs.csc.fi/support/tutorials/ml-workflows/#mlflow-tracking-server",
            "- A: Conda shouldn't be needed for this, unless you need to install some packages from the conda repositories. MLflow should be included in our PyTorch module, and can also easily be installed by yourself with `pip install`."
        ],
        "source": "csc-enveff-20231128"
    },
    {
        "idx": 93,
        "question": [
            "I'd like to learn more how to use containers. Could you advise a best/most compatible with CSC infrastructure (online) course for that? I can see that there is a MOOC at UH (TKT21036 - TKT21038) on Docker, do you think it's helpful? Any other options?  Also, Kubernetes is often mentioned together with Docker; what is this? Is it used for academy-oriented DS projects? Thanks!**"
        ],
        "answer": [
            "- A: Ari-Matti and Laxman have been arranging containers courses at CSC. Next one is not currently scheduled, but here's the course material for the previous one: https://yetulaxman.github.io/ContainersHPC2023/",
            "- This is probably the MOOC you meant?https://devopswithdocker.com",
            "  - A: One introductory level course (bit heavy on biofield though) could be the following: https://yetulaxman.github.io/ContainersHPC2023/.  HPC systems don't entertain the deployment of docker containers as the docker engine require root access for running the container. However, concepts of docker are quite useful for understanding otehr containers as well. Singularity/Apptainer containers are HPC-friendly and do not require root access for running container. You can convert a docker image to Apptainer/singularity.",
            "  - A: Kubernetes is container orchestration system that can be used e.g. to automate starting/stopping containers. It's typically used in a setting where you provide services as containers.CSC uses similar but bit different framework called, OpenShift (which is a paid platform service from Red Hat UNLIKE Kubernetes (aka, k8s) is an open-source application from Google) in Rahti container cloud (https://rahti.csc.fi/)"
        ],
        "source": "csc-enveff-20231128"
    },
    {
        "idx": 94,
        "question": [
            "Is it possible to use Tykky tool to put different conda installations into the same container?**"
        ],
        "answer": [
            "- A: In theory, yes. But in practice, if you have two complex conda environments, each with conflicting python packages, tykky installation would more likely either fail or one of the environments may not work as intended."
        ],
        "source": "csc-enveff-20231128"
    },
    {
        "idx": 95,
        "question": [
            "I am using vs code to connect to puhti. How could I visualize /scratch/project using EXPLORER on the left hand side.**"
        ],
        "answer": [
            "- A:"
        ],
        "source": "csc-enveff-20231128"
    },
    {
        "idx": 96,
        "question": [
            "I am in the processing my MATLAB code for parallel computing but I will like to start running my code with a single core until its paralized. Maybe I might need a bit of assistance.**"
        ],
        "answer": [
            "- A: How to proceed here might depend a bit on what exactly you are trying to compute. Perhaps you should send an email to servicedesk@csc.fi and include your script there and then we could give you some guidance? You can also have a look at our Matlab documentation https://docs.csc.fi/apps/matlab/"
        ],
        "source": "csc-enveff-20231128"
    },
    {
        "idx": 97,
        "question": [
            "Have I clicked the edit mode on?**"
        ],
        "answer": [
            "- A: Probably not yet.. â†–ï¸"
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 98,
        "question": [
            "Slides available after the course?**"
        ],
        "answer": [
            "- A: They sure are! You will also have the access to this HedgeDoc document (save the link).",
            "    - The slides and tutorials are available here: https://csc-training.github.io/csc-env-eff/",
            "    - Access to e-lena (and the quizzes there) may discontinue.",
            "- We encourage you to share and use the material also in your own courses. The material is in git, and we welcome all feedback and edit suggestions (pull requests)!"
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 99,
        "question": [
            "Has the Zoom meeting already started? I'm just gettin \"The meeting has not started\" -note.**"
        ],
        "answer": [
            "- A: Make sure you have the full Zoom link when connecting! It's long and contains the password :)"
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 100,
        "question": [
            "Should we be able to access the slides in e-elena? I can only see the first slide.**"
        ],
        "answer": [
            "- A: Try navigating with the arrow keys :)"
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 101,
        "question": [
            "So many credendials... Which ones do I need and where?!?**"
        ],
        "answer": [
            "- A: You need",
            "    - **CSC credentials** to Puhti/Mahti",
            "    - **Haka OR Virtu credentials** (=your university/institution credentials) to my.csc.fi",
            "    - **Haka credentials** (=your university/institution credentials) to eLena platform",
            "    - contact us if you don't have HAKA/Virtu in use"
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 102,
        "question": [
            "I have forgotten my CSC username and password**"
        ],
        "answer": [
            "- A: In my.csc.fi you can check your CSC username. If you have forgotten your CSC password, check this link: https://docs.csc.fi/accounts/how-to-change-password/ -Note that it takes some time for the password to update to Puhti, if you need to change that :)"
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 103,
        "question": [
            "Depending on the place you say to obtain acces to Puhti and Allas or Puhti and Mahti, should we have all three of them (just in case)?**"
        ],
        "answer": [
            "- A: On the course there is one tutorial in which you can try moving data from Puhti to Mahti. It is an optional task so you don't necessarily need Mahti."
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 104,
        "question": [
            "Do you record the sessions?**"
        ],
        "answer": [
            "- A: No. However, the lecture videos, slides and hands-ons are available in the github page, and all the material + the quizzes etc can be accessed in eLena platform, so this course works well as self-study also!"
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 105,
        "question": [
            "How to add a remote repository for git? I would like to use git for the scripts I write on the CSC servers and push/pull them from a remote repo**"
        ],
        "answer": [
            "- A: If you want to add your own content to the materials of the course, you could fork the course repository https://github.com/csc-training/csc-env-eff  and clone your own fork to your working environment (eg supercomputer Puhti)",
            "- General: You can use git the same way you would use it in your own computing environment (from the command line). So for adding a remote repository, you can either create a repository on your favorite platform, eg GitHub and then clone it via ssh to Puhti, or other computing environment. OR you can initialize the repository on Puhti and then push it to a new remote repository.",
            "- For setting up ssh keys you can use the command `ssh-keygen` on Puhti login node shell and store them to your users home directory."
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 106,
        "question": [
            "Do you have an example script for how to use snakemake on Puhti? I would like to use snakemake but haven't worked on a server before that uses batch jobs (after reading snakemake docs, I'm still pretty clueless on how to use the 'Job properties'; so an example script would be nice)**"
        ],
        "answer": [
            "- A: We are currently working on an example for a snakemake tutorial (soon also in CSC docs): https://github.com/CSCfi/csc-user-guide/blob/d720852e6e12f57d843095585781f775fbde1280/docs/support/tutorials/snakemake-puhti.md",
            "- **Follow up question:** It is really nice to see that you guys are working on this! I looked at the snakemake tutorial - and I understand it is a job in process - but I have several questions to the setup. Would it be possible with a course or seminar on this- using snakemake in CSC?",
            "--- I would also be really interested in such a course!!",
            "    - A: **Very good that you ask!** We don't have anything for snakemake at the moment. We only have a Nextflow (another workflow tool) hackathon coming up in April: https://ssl.eventilla.com/event/9kgb0 and possibly something on workflows for geoinformatics in late spring, keep checking our training calendar. General idea of running both nextflow and snakemake in CSC supercomputing environemtn is about the same. So you can register for hackathon (and the event is free-of-charge)"
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 107,
        "question": [
            "Where can I find the slides from the video \"Study Tips for Using CSC HPC environment effectively\"? (https://video.csc.fi/media/t/0_d7trmsru/455250) - Ah ok but there are several hyperlinks in there that we cannot access now!**"
        ],
        "answer": [
            "- A: They are here! https://a3s.fi/CSC_training/00_study_tips.html"
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 108,
        "question": [
            "Is there a nice way to automatically record the software versions you used when loading a module in your pipeline? (similar to a conda environment lock file)**"
        ],
        "answer": [
            "- A: Not really in that sense. But all our modules come with specific versions. It is a good idea to load each module along with specific version in your batch script. If you don't mention a version, it is the latest stable version.",
            "- A: If you include all \"module load\" commands in uour batch job script the load messages get saved in the SLURM out file. You could also include \"module list\" command. AS mentioned above it is always good idea to load modules with version number as the default version will change over time."
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 109,
        "question": [
            "Is it essential to have Haka or Virtu credentials for the course? I've got only CSC credentials.**"
        ],
        "answer": [
            "- A: No, the CSC account is the important one for this course"
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 110,
        "question": [
            "How to know which open source/online software I am allowed to download/install/compile/run on CSC super computers (Puhti mainly)? I need several software packages which are from the Github account of a known collaborator. Is is possible to ask permission for specific packages from someone at CSC personally (email)?**"
        ],
        "answer": [
            "- A: When CSC installs a software tool in supercomputing environment we look for licence file which gives information on whether we can install for free on CSC systems. If they are opensource/free you are free to install as needed. For some reason if you are not clear, you can check with CSC by sending an e-mail to our service desk. You can browse licenses in our software stack in CSC documentation (https://docs.csc.fi/apps/by_discipline/)"
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 111,
        "question": [
            "Does developing codes in the server consume billing units?**"
        ],
        "answer": [
            "- A: Whenever you use the computing resources, billing units are consumed. However, if you do not have high resource needs, the amount of billing units used is very minimal."
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 112,
        "question": [
            "Is the Visual Studio Code in the Puhti/Mahti web interface compatible with GitHub Copilot? How about Pouta?**"
        ],
        "answer": [
            "- A: It is not available by default, but there are open source copilot-like plugins you can try. Pouta is a cloud service where you can manage your own virtual machine, i.e. you have to set up everything yourself. So in principle it is compatible yes.",
            "- A: If you want to try something like this, might be good to send an email to servicedesk@csc.fi as it might require some tricks.",
            "- A: The official one should be possible to use, just has to be manually installed using the \"Install from vsix\" option in the extensions tab (in the three dots menu there) after downloading the extension from the Visual Studio Marketplace."
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 113,
        "question": [
            "Can I use a singularity container in Puhti?**"
        ],
        "answer": [
            "- A: Yes. Singularity is now called Apptainer. Check this documentation: https://docs.csc.fi/computing/containers/overview/"
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 114,
        "question": [
            "Do I need a license to use the pre-installed applications on Puhti, for example matlab?**"
        ],
        "answer": [
            "- A: Depends on the software. Check the Docs page for each application, e.g. https://docs.csc.fi/apps/matlab/",
            "- A: You can list all the software by license: https://docs.csc.fi/apps/by_license/"
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 115,
        "question": [
            "How do we know what resourses an application needs? Or what resources will be needed to run a certain analysis?**"
        ],
        "answer": [
            "- A: Bit difficult to suggest on a general level. Good starting point is to check application documentation where you may find whether the application is memory intensive, parallelisable or has specific resource needs.  Once you get some idea about the software you can start with small data and perform some pilot experiments with minimal resources. You can then scale up the resources and data as needed. Only experience will help us here. Obviously, you can always ask the developer of the tool to know more information instantly.",
            "   - A: We will discuss this a bit tomorrow. You can also check this FAQ entry: https://docs.csc.fi/support/faq/how-much-memory-my-job-needs/",
            "   - FOLLOW UP QUESTION: And how about a specific analysis? Let's say if we need to calculate some genetic diversity estimates, how to know how much resources does it take per Kb/Gb of a sequence/SNP data?",
            "    - A: Let's see if Thursdays session answers this question, if not please post it again in the bottom of this document"
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 116,
        "question": [
            "I think we will get presentation slides latter ?**"
        ],
        "answer": [
            "- A: All presentations and tutorials are available: https://csc-training.github.io/csc-env-eff/"
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 117,
        "question": [
            "Can you please explain again about GPU NVIDIA processor in Puhti or Lumi?**"
        ],
        "answer": [
            "- A: Puhti and Mahti have GPU NVIDIA processors while LUMI has AMD GPU processors."
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 118,
        "question": [
            "I'm sorry if I missed it, but what is the main difference between projappl and scratch directories except for the automatic cleaning in scratch? Where is it better to store files that we want to share with other project members?**"
        ],
        "answer": [
            "- A: Both projappl and scratch are shared with all project members, we suggest to keep all working files in **scratch** and use projappl only for files related to installations, configuration files etc. I.e. files that are necessary for a longer period of time. Inactive research data should be stored in Allas (will be covered later in this course)."
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 119,
        "question": [
            "How can you check your disk workload? Like how do you know if you need to use/switch to these fast local disks?**"
        ],
        "answer": [
            "- A: Good question. It might not be easy, but if you know that your application will be reading/writing thousands of files or in general just doing a lot of I/O then that's at least much more efficient on fast local disk. So you need to know the details of your workflow.",
            "- A: It's difficult to see directly, but you could check resource usage with \"seff\" command. Low CPU efficiency can be an indication that time is spent waiting for the disk, not computing. If you have low CPU efficency try same analysis with local disk and compare performance. We will talk about this tomorrow."
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 120,
        "question": [
            "Can only the project manager/owner apply for more disk resources into an existing workspace?**"
        ],
        "answer": [
            "- A: Yes, only the project manager can apply for a disk quota change."
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 121,
        "question": [
            "From zoom chat: One question about the cleaning of storage in putti every six months, will all the files in /scratch and /projappl be erased? Since the /projappl is a sharing directory.  Thanks!**"
        ],
        "answer": [
            "- A: Files that have not been used in six months will only be deleted from /scratch. Files in your $HOME directory or /projappl will not be removed. Each time the cleaning happens, we will notify by email in advance and give instructions how you can check which files will be removed so that you can move them to e.g. Allas."
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 122,
        "question": [
            "About ssh key tutorial: I follow the steps and end up getting the following error message **(I have replaced my actual ID with \"userID\")**: /usr/bin/ssh-copy-id: ERROR: /users/userID/.ssh/config: line 2: Bad configuration option: usekeychain ERROR: /users/userID/.ssh/config: terminating, 1 bad configuration options**"
        ],
        "answer": [
            "- A: You need to use your real CSC username, it will not otherwise work.",
            "    - From zoom chat: Hi, I left question 25, but the person replying didnâ€™t seem to catch that I have just changed my user id to â€œuserIDâ€ in the question for privacy purposes. I get the error with my real ID",
            "    - A: Aha ok! Did you do these steps on your local machine or on Puhti? The tutorial assumes you're using MobaXterm on your own machine and you might get problems if using PuTTy as the steps are a bit different, see https://docs.csc.fi/computing/connecting/#ssh-keys-with-putty",
            "    - I'm using the terminal on my local machine (MacOS)",
            "    - Ok, then we might need to take a closer look. Maybe you could ask this in the next breakoutroom session?",
            "- Did you have some kind of configuration file (/users/userID/.ssh/config) with the the following content:?",
            "```",
            "   Host Puhti",
            "  Hostname = puhti.csc.fi",
            "  User = your CSC username",
            "  PreferredAuthentications publickey",
            "  IdentityFile ~/.ssh/puhti_ssh_key.key",
            "  ```",
            "and then run the following command on Mac terminal:",
            " `ssh Puhti`",
            " - I did not, but I used the default name and location. I also tried adding this content to the config file and I still get the error. This is my current config file (again, my real ID is changed to userID) I also tried with ~/id_ed25519:",
            "```",
            "Host Puhti",
            "\tHostname = puhti.csc.fi",
            "    User = userID",
            "    PreferredAuthentications publickey",
            "    IdentityFile /users/userID/.ssh/id_ed25519.pub",
            "Host *",
            "  \tUseKeychain no",
            "    AddKeysToAgent yes",
            "```",
            "If your userID is CSC username and id_ed25519.pub is your private key, then it would work.",
            "can you try the following command directly instead ?:",
            "```",
            "ssh -i /users/userID/.ssh/id_ed25519.pub  cscusername@puhti.csc.fi",
            "```",
            "UserID = CSC username, following the tutorial, I assume the key is id_ed25519. With the command above I still get this error.",
            "```",
            "/users/userID/.ssh/config: line 7: Bad configuration option: usekeychain",
            "/users/userID/.ssh/config: terminating, 1 bad configuration options",
            "```",
            "Ok. Thanks for the clarification. can you add the following line in the config file and check if it works?:",
            "```",
            "UseKeychain yes",
            "```",
            "Still the same error with \"UseKeychain yes\"",
            "What if you comment out or delete the \"UseKeychain\" line?",
            "You can try. It is bit tricky that I don't have any \"UseKeychain\" related configurations but still works.  I am not able to reproduce the problem.",
            "is the line 7 in the config file is \"UseKeychain no\" ?"
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 123,
        "question": [
            "Is the SSH required? I could not get that to work, but I understood that Puhti and Mahti can be used from the web interface and then the SSH is not required? I use windows computer and tried to create SSH with Putty**"
        ],
        "answer": [
            "- A: Yes, if you use the web interface, then you do not need to set up SSH keys since you will be authenticating by logging in using CSC account (or Haka/Virtu). The tutorial is for setting up and using the SSH keys for terminal access from your local computer so the steps outlined there should be done on your local machine. The tutorial also assumes use of MobaXterm on Windows. For PuTTy, see https://docs.csc.fi/computing/connecting/#ssh-keys-with-putty"
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 124,
        "question": [
            "Related to Q24. Can you clarify is 180d per file or any activity in folder? i.e. If only one file is accessed in the folder, does it \"protect\" other files from deletion?**"
        ],
        "answer": [
            "- A: The cleaning is done by file, so such protection will not work ;-). For details, see https://docs.csc.fi/support/tutorials/clean-up-data/#automatic-removal-of-files"
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 125,
        "question": [
            "Do you have theme based forums where one can raise or look up previous questions? It feels a bit silly to ask what I would believe are common questions - eg how to run python or R based scripts of the CSC cluster - in its own email to you guys. Thus, is there \"we who use R/python/jupyter on the CSC\"-forum where one has gathered theme-specific questions?  It is not always one can attend the Wednesday sessions and the website is huge, it is not easy to find specific tutorials. Forums might be helpful.**"
        ],
        "answer": [
            "- A: Good point! We do have some Frequently asked questions (FAQ) in our documentation, but not our own forum as such. We collect all course questions too, as well as the weekly support questions, but they are not ordered by theme, and therefore not easy to find. We use both courses and support sessions for updating FAQ once in a while though: https://docs.csc.fi/support/faq/",
            "- However, there is no stupid question, and we are also happy to answer questions that probably have been asked a hundred times. If we have a good docs page about the topic, you will get the link to that too :)"
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 126,
        "question": [
            "How to access to jupyter from web interface? I tried, but not quite sure how does it work. can you show us? Thanks!**"
        ],
        "answer": [
            "- A: Sure. we can show you as a small demo sometime during this course (or in break out rooms). Please remind us if this has not happened in Thu session."
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 127,
        "question": [
            "When we use the export SCRATCH= and export PROJAPPL= commands, where do these paths get exported to?**"
        ],
        "answer": [
            "- A: They get stored into these variables. I.e. `export SCRATCH=/scratch/project_2001234` would mean that you can use `$SCRATCH` whenever you want to refer to the path `/scratch/project_2001234`. E.g. `cd $SCRATCH` would make you move into this scratch directory. Note the `$` sign, which is used in Linux shell to denote a variable.",
            "- FOLLOW UP QUESTION: Does it only work with SCRATCH and PROJAPPL or can we use it for exporting any path? And if yes, can we give it any name, e.g export STRUCTURE=/scratch/project_2001243/structure",
            "    - Yes this would also work. Just be careful to not overwrite any important variables, e.g. PATH or PYTHONPATH, you can check if a variable already exist with `echo $VARIABLENAME` which either prints the variable name or nothing if it does not exist"
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 128,
        "question": [
            "When the tutorial has $USER, do we need to type it as it is, or should we type our actual username? When I tried creating a new directory (in the first tutorial today) using $USER it created an empty file with my user name instead of a new folder.**"
        ],
        "answer": [
            "- A: `$USER` is actually an environment variable which stores your CSC username as its value on Puhti. So it is a convenient shorthand to use if you don't want to type your username. It is different for each user (as each user has their own username). So to answer the question, type it as is. content inside  brackets (e.g. `<project>`) should be replaced somehow (e.g. by your actual project id).",
            "- [x] **In this tutorial (https://csc-training.github.io/csc-env-eff/hands-on/disk-areas/disk-areas-tutorial-lue.html), right after step 6, if I type \"lue --display-level=<n> $HOME\", I get an error \"-bash: n: No such file of directory\". Then if I replace n with the name of any of the folders, it just says \"-bash: /user/..: Is a directory\" but it does not give info as general lue $HOME command did. Could you please explain how this --display-level=<n> works?**",
            "- A: The display level parameter (â€“display-level) of *lue* tool refers to the depth of folders hierarchy that the tool should report.  You can try with â€“display-level=1 and then â€“display-level=2. You can then see the difference. You can start to see the information of disk usage on subfolders as increase the depth.",
            "- [x] **The command echo $LOCAL_SCRATCH does not give an output, instead there is a blank line. Why is that? (end of step 3 of this tutorial: https://csc-training.github.io/csc-env-eff/hands-on/disk-areas/disk-areas-tutorial-fastdisks.html)**",
            "- A: I believe that you were trying to type the **echo** command on login nodes where we have only $TMPDIR, not the $LOCAL_SCRATCH. The $LOCAL_SCRATCH should be visible in interactive nodes as well as on some of compute nodes on Puhti. Mahti CPU nodes don't have local  disks (and hence no variable, $LOCAL_SCRATCH, exists)."
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 129,
        "question": [
            "Will there be another course after this one? Maybe more project oriented?**"
        ],
        "answer": [
            "- A: Yes. there will be bit more advanced and extended version of this course.",
            "- Follow our training calendar for updates: https://www.csc.fi/training#training-calendar"
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 130,
        "question": [
            "I see in MyCSC that the project file allocation is almost full, at 99%. What should I do?**"
        ],
        "answer": [
            "- A: You should figure out where you have a lot of data. For this, the tutorial from yesterday is useful, https://csc-training.github.io/csc-env-eff/hands-on/disk-areas/disk-areas-tutorial-lue.html. Then, when you know this, delete the unnecessary files and move those that you still need somewhere else, e.g. Allas.",
            "- There are 6 or 7 people in this project, maybe I should ask the project admin?",
            "- Indeed, it's good to communicate within your group and ask your colleagues to also check their files. It is of course also possible to apply for more quota, but the best option is always to first consider if all those files are really needed. https://docs.csc.fi/accounts/how-to-increase-disk-quotas/"
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 131,
        "question": [
            "Why the login node shell is used for these tutorials? Shouldn't the computing node be used for these?**"
        ],
        "answer": [
            "- A: here we use batch job submissions on login nodes just to submit the jobs to cluster where actual computation happens in compute nodes.",
            "- A: Todays tutorials are tasks that you would normally do in the login shell (preparing and submitting batch jobs). Some other tutorials may be tasks that you would normally do in an interactive shell. This, hower, is sometimes problematic in a course situation, as the interactive resources are limited. Therefore we have designed the tutorials light enough to be run on the login nodes."
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 132,
        "question": [
            "Sending jobs to the cluster via snakemake - is not really advised without calling snakemake by a batch script, even if you allocate appropriate amount of cores etc. Can you try to explain why this is?**"
        ],
        "answer": [
            "- A: Snakemake can generate a large number of very short batch jobs and this puts load on the batch job system. In worst case scenario every step (e.g. checking if a file exist) is run as a separate batch job. So you should reserve some resources and let Snakemake do its thing within that reservation, but don't let Snakemake talk to the batch job system directly.",
            " - **follow up question: Thanks for that clarification. I don't feel that message comes across - not letting things interact with the cluster without batch script- given that one can load the snakemake module, and other modules, that are otherwise built to be able to send jobs to a cluster etc. But in snakemake you can specific cores etc, and my question therefore; shall resource allocation not be done at all and *only* be specified in the batch script (or shall it *also* be specified in the snakemake commands for resources)? Is it wrong to specify it within snakemake (even when I call snakemake from a batch script)?**",
            "   - A: The problem is the Snakemake Slurm executor. Generally speaking don't use the `--slurm` option.",
            "   - A: Just to elaborate a bit, you may use a slurm option if you are sure that each rule takes considerable wall-clock time and you have fewer such rules. And then expect to stay in queue for the execution of each rule as snakemake submits each of those rules  to cluster as a separate job (=slurm way of working).  On the HPC cluster, if you have thousands of small jobs,  each job creates some overhead on slurm database and the otehr performance issues. Submitting tiny job on a cluster cluster can be overkill. On the otherhand, even when you execute everything in one batch job,there will be local executor submitting the jobs to locally (=in the same job allocation) without needing to queue for each rule on that compute nodes. As and when you have very hight-thoughput case (like working with hundereds of samples), you can use Hyperqueue executor with multiple nodes to perform your jobs. So bottomline is that you are still using cluster capabilities but in a efficient manner."
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 133,
        "question": [
            "What do you do with batch jobs when you don't know how long your job might take? For example with #SBATCH --time=00:02:00 ?**"
        ],
        "answer": [
            "- A: You can test beforehand in an interactive job for example. Run a minimal example and try to extrapolate based on the runtime.",
            "- A: Overestimating time is not as bad as overestimating other resources. The job will end when the last process of the job ends and all reservations are freed."
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 134,
        "question": [
            "Why is the number of cores per node called \"--ntasks\" instead of \"--cores\", like \"--nodes\"? Or is \"--ntasks\" something different that simply the number of cores per node?**"
        ],
        "answer": [
            "- A: A single task may use multiple cores. The number of cores one task uses is specified using `--cpus-per-task` option. So it's a bit different. You could for example use --ntasks=40 and --cpus-per-task=1 to use all cores in one Puhti node. Or you could also do --ntasks=2 and --cpus-per-task=20, or --ntasks=1 and --cpus-per-task=40. In all cases here you're using all cores, but the details of the parallelization will be different (tasks are related to MPI ranks while cpus-per-task are actually the number threads that each MPI rank launch, but this is already a bit more advanced topic).",
            "    - **Follow up question**: If I have a parallel job that is not parallelized with OpenMP or MPI (say, Matlab's parfor/parpool), does it matter how I request the number of CPUs I want?",
            "    - A: I do not know the specifics of how Matlab is parallelized, but I would assume it uses either or both of MPI and OpenMP under the hood. These are the main parallel programming standards as far as I'm aware. Please see our matlab documentation: https://docs.csc.fi/apps/matlab/"
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 135,
        "question": [
            "When is Putty recommended to use? Is it the same as MobaXterm?**"
        ],
        "answer": [
            "- A: It is a matter of preference mostly. MobaXterm has much more features and can thus be more useful. PuTTY is a quite simple client that more or less just allows you to do SSH (i.e. no file transfer functionality e.g.)."
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 136,
        "question": [
            "Is it better to overestimate time rather than the number of cores? The resources will be free anyways after the job is finished, altough time allocation is not?**"
        ],
        "answer": [
            "- A: Neither is optimal, but in this case it would be better to overestimate time, because you are only billed for the time you actually use and after your job ends the resources are freed for others to use. Cores that are reserved but not used are away from others and wasted if you do not utilize them efficiently. The main downside of requesting too much time is that you might queue for the resources for longer."
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 137,
        "question": [
            "What threshold is considered too inefficient for CPU useage (in terms of percentage) ?**"
        ],
        "answer": [
            "- A: The rule of thumb is that your job should get *at least* 1.5 times faster when you double the amount of resources. So as a percentage, the parallel (CPU) efficiency should be at least 75%"
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 138,
        "question": [
            "Does seff display some mean memory usage for a job? Sometimes seff shows that a job has not used all of the reserved memory (e.g. seff shows memory usage of approximately 40%) but still the job fails while running with an out of memory error and increasing the allocated memory for the job seems to fix this, even though seff showed that not all of the allocated memory were used in the first place. Could this be due to memory demand spiking highly in the course of running the job, even though most of the time not using all of the allocated memory?**"
        ],
        "answer": [
            "- A: Seff is not 100% accurate and can miss shortlived memory spikes, unfortunately. Seff tries to give you the maximum memory usage (i.e. the memory that your job actually needs/used), but it can fail. Mean memory estimate would not be that useful because the needs are defined by the maximum."
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 139,
        "question": [
            "Is there a way to restore my home directory to its state at the time my account was created (somehow like a fresh install)?**"
        ],
        "answer": [
            "- A: Not really, but this tool might accomplish sort of what you might be looking for: https://docs.csc.fi/support/tutorials/using_csc_env/",
            "- A: It will not delete files for you or anything like that, but it can be used to reset your .bashrc file for example"
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 140,
        "question": [
            "Is there any documentation for running MATLAB (array or parpool) jobs from the Terminal as batch-jobs? I think the documentation I found only considers using MATLAB Parallel Server from local GUI/workstation. I managed to modify the array.sh-script to run a simple MATLAB code (= prints input) as an array job, if the input is given as numbers, but I haven't managed to give ${SLURM_ARRAY_TASK_ID} as input. I simply added \"module load matlab\" and \"srun matlab -nojvm -nosplash -batch 'TEST();'\", but I don't know if these are the wanted MATLAB settings and how to input the array task ID. The error I get is \"Invalid text character. Check for unsupported symbol, invisible character, or pasting of non-ASCII characters.\"**"
        ],
        "answer": [
            "- A: I am not a matlab expert, but did you check our documentation here: https://docs.csc.fi/apps/matlab/ ?",
            "- **Follow up question**: Yes, I read those pages and I think they only advice how to use MATLAB Parallel Server from local GUI/workstation (sorry if i missed something). But if I have 200 array jobs that run for 50 hours, I would rather have them running somewhere in the background (Terminal).",
            "- I asked our Matlab specialist Sampo to get back to this! If he has not time to do that right now, I'd suggest you send a ticket about it to servicedesk@csc.fi. Sorry that we have no Matlab expert now on call on this course!",
            "- **Follow up question**: No problem at all, great to know that help is available!",
            "- A (from Sampo): We have only five licenses available for MATLAB outside the parallel server, thus you can't run your array jobs effectively with them. Array jobs are also possible to launch via MATLAB Parallel tool. We can provide you an example, please send your question to servicedesk@csc.fi.",
            "- **Follow up question**: Thank you very much, this answer was very helpful! So I if use MPS instead of Terminal, I could run, for example, 1000+ array jobs (15 mins, 50 GiB) effectively? I already made some tests with MPS, thus I will continue those and contact servicedesk with a better formulated question. Many thanks!",
            "    - Asking servicedesk sounds good! Please note that there is limits on how many jobs a user can submit per day and at the same time."
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 141,
        "question": [
            "How do you know with which protocol a data object was uploaded to Allas?**"
        ],
        "answer": [
            "- A: There is no 100% sure way to check this. However, if you have over 5 GB files you can check if they are stored as several object called as segments. If that is the case, then Swift protocol has been used. If you se just one big object, then S3 protocol has been used."
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 142,
        "question": [
            "Is there a way of soft linking my data from allas to my working directory at /scratch/project_200â€¦/user/ working directory?  (e.g., link -s /allas/data /scratch/project_200â€¦/user/)**"
        ],
        "answer": [
            "- A: No.",
            "- Sidenote: Some geospatial tools that are based on `GDAL` however, let you read data directly from Allas: https://docs.csc.fi/support/tutorials/gis/gdal_cloud/ ; but it is not too many tools that support this."
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 143,
        "question": [
            "How do we get the certificate of the course? In the eLena platform it says that the certificate is \"Not available unless: The activity Course feedback, live course is marked complete\", but that is also unavailable.**"
        ],
        "answer": [
            "- A: We will check that and send you instructions via email. Thank you for letting us know!",
            "- EDIT: There was an error in the course settings, now the \"Course feedback, live course\" should be available you, if you selected the Feb2024 group in the top of the page! Thank you very much for pointing this out!"
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 144,
        "question": [
            "I try using the commands cd/scratch_projectname and /cd_projappl_projectname. Why nothing happens but the text \"No such file or directory\"?**"
        ],
        "answer": [
            "- A: After logging into CSC platform with your account, you can migrate to the `/scratch` partition of your project as follows:",
            "`cd /scratch/project_projectnumber`",
            "For ex. If you have a project `project_1000110`, it should be:",
            "`cd /scratch/project_1000110`",
            "Similarly, for migrating to the `/projappl` partition of your project, it would be:",
            "`cd /projappl/project_1000110`"
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 145,
        "question": [
            "Type your question there!**"
        ],
        "answer": [
            "- A: Answer will appear here"
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 146,
        "question": [
            "Type your question there!**"
        ],
        "answer": [
            "- A: Answer will appear here"
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 147,
        "question": [
            "Type your question there!**"
        ],
        "answer": [
            "- A: Answer will appear here"
        ],
        "source": "csc-enveff-20240207"
    },
    {
        "idx": 148,
        "question": [
            "Have I clicked the edit mode on?**"
        ],
        "answer": [
            "- A: Probably not yet.. â†–ï¸"
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 149,
        "question": [
            "Slides available after the course?**"
        ],
        "answer": [
            "- A: They sure are! You will also have the access to this HedgeDoc document (save the link).",
            "    - The slides and tutorials are available here: https://csc-training.github.io/csc-env-eff/",
            "    - Access to e-lena (and the quizzes there) may discontinue.",
            "- We encourage you to share and use the material also in your own courses. The material is in git, and we welcome all feedback and edit suggestions (pull requests)!"
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 150,
        "question": [
            "Has the Zoom meeting already started? I'm just gettin \"The meeting has not started\" -note.**"
        ],
        "answer": [
            "- A: Make sure you have the full Zoom link when connecting! It's long and contains the password :)"
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 151,
        "question": [
            "Should we be able to access the slides in e-elena? I can only see the first slide.**"
        ],
        "answer": [
            "- A: Try navigating with the arrow keys :)"
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 152,
        "question": [
            "So many credendials... Which ones do I need and where?!?**"
        ],
        "answer": [
            "- A: You need",
            "    - **CSC credentials** to Puhti/Mahti",
            "    - **Haka OR Virtu credentials** (=your university/institution credentials) to my.csc.fi",
            "    - **Haka credentials** (=your university/institution credentials) to eLena platform",
            "    - contact us if you don't have HAKA/Virtu in use"
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 153,
        "question": [
            "I have forgotten my CSC username and password**"
        ],
        "answer": [
            "- A: In my.csc.fi you can check your CSC username. If you have forgotten your CSC password, check this link: https://docs.csc.fi/accounts/how-to-change-password/ -Note that it takes some time for the password to update to Puhti, if you need to change that :)"
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 154,
        "question": [
            "Depending on the place you say to obtain acces to Puhti and Allas or Puhti and Mahti, should we have all three of them (just in case)?**"
        ],
        "answer": [
            "- A: On the course there is one tutorial in which you can try moving data from Puhti to Mahti. It is an optional task so you don't necessarily need Mahti."
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 155,
        "question": [
            "Do you record the sessions?**"
        ],
        "answer": [
            "- A: No. However, the lecture videos, slides and hands-ons are available in the github page, and all the material + the quizzes etc can be accessed in eLena platform, so this course works well as self-study also!"
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 156,
        "question": [
            "What kind of task can be done in login nodes directly, in addition to submitting batch  jobs?**"
        ],
        "answer": [
            "- A: You can use login node to get access to interactive nodes(https://docs.csc.fi/computing/running/interactive-usage/). You can also do some light-weight pre-processing of data in temporary directory in login node ( you have to move your data to $TMPDIR and then do preprocessing) before actual analysis. There are other useful things like checking disk usage and finding the information of different projects (commands: csc-workspaces; csc-projects) while being in login node."
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 157,
        "question": [
            "What does it mean 'to run jobs in parallel'? Would they be related jobs that need information from eachother for example or something else?**"
        ],
        "answer": [
            "- A: Running in parallel means that the progam you're using has been programmed such that you can split the workload into smaller pieces, each being processed at the same time using multiple processes (cores). This is the fundamental principle of supercomputing, i.e. we split a heavy calculation into smaller pieces, compute them in parallel, and then combine the results at the end.",
            "- A: Regarding information exchange, there are different types of parallel jobs. Sometimes the smaller pieces we split the job into are completely independent, in which case no information needs to be exchanged between them. This is sometimes called \"embarrasing parallelism\" or \"trivial parallelism\". Usually, however, the processes are not fully independent and information must be passed between the individual processes. Communication between processes is a very important topic when talking about parallel computing and is one of the reasons why you typically cannot just use arbitrarily many cores for your calculation, but the performance will level out at some point after which the performance does not improve anymore. There will simply be too much communication between the processes."
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 158,
        "question": [
            "When I sign in to puhti via ssh on my linux computer, am I accessing puhti via a login or compute node?**"
        ],
        "answer": [
            "- A: Login node (aka, head node or user access node (uan))"
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 159,
        "question": [
            "If I have a process that requires to read huge amount of data, is Mahti a good choice? I parallelized my code, but due to insufficient RAM it throws errors (I first used 32 cores in parallel in my code, then 8, then 4, none worked). However in Puhti with less cores (8) it works.**"
        ],
        "answer": [
            "- A: Mahti is a good choice if your application leverages lot of parallelism (18 cores) but if you need lot of memory, Puhti has better options for selecting nodes with different memory. Mahti node has fixed memory of 256 GB. As per reading huge files, Lustre (parallel file system on Mahti and Puhti) is fine. Please also note that Mahti resources are provisioned as per node basis (i.e., smallest allocation one can get is at least one node)"
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 160,
        "question": [
            "Why Puhti, MAHTI, LUMI are not considered to be cloud enviroment as well?**"
        ],
        "answer": [
            "- A: Typically a cloud environment is exclusively available to you, whilst supercomputing environments are shared systems with even up to thousand simultaneous users. Supercomputers use batch job (queuing) systems for this reason, and they typically can also give access to much larger resource allocations because of this compared to cloud resources."
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 161,
        "question": [
            "While accessing puhti via ssh (login node) how do I switch to a compute node?**"
        ],
        "answer": [
            "- A: By submitting batch jobs to queue or requesting an interactive session. This will be covered in more detail in batch jobs part of the course and in \"running jobs\" in docs.csc.fi."
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 162,
        "question": [
            "While using the puhti jupyter, if I have around 2 TB of files to process, how do I use CSC storage to manage it. I want the files on CSC aswell.**"
        ],
        "answer": [
            "- You have to first tranfer 2 TB of your data to the scratch area of Puhti (if you are a project manager, you can increase the scratch disk space in mycscs portal if needed)"
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 163,
        "question": [
            "Hi, I've had a bit of a technical problem in ne part of the prereq course, but I wasn't sure who to email about it.**"
        ],
        "answer": [
            "- A: Sorry to hear that! Servicedesk@csc.fi and weekly support sessions are always good places to ask questions. What kind of trouble you had?",
            "- Q: The links to the videos in the documentation area under each topic don't open for me. I try to login with my CSC credentials but get an error that my username has not been found. (It's the same username and password I use for the my.csc.fi website). Specifically these two videos: First Glimpse of the Shell 1-4; Nano, Emacs, Vim. Should I send an email about it or is this an access issue that can be resolved from the owners of the course?"
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 164,
        "question": [
            "In the nodes you said there are several cores, are those \"threads\" or does each core has 2 threads like in local cpus?**"
        ],
        "answer": [
            "- A: Yes, each node has multiple cores (40 on Puhti, 128 on Mahti) and each physical CPU core has two hardware threads.",
            "- A: It should be noted that hyperthreading is not that useful in most HPC use cases. In most cases you should use 1 cpu/core per software thread."
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 165,
        "question": [
            "I have been using the normal ssh  command ssh <username>@puhti.csc.fi    # replace <username> with your CSC username, e.g. myname@puhti.csc.fi to login to puhti but nowadays it works while adding ssh -m hmac-sha2-512? I'm using windows.**"
        ],
        "answer": [
            "- A: The need to explicitly specify the message authentication code (MAC) is related to a bug in some windows ssh clients. See more details at https://docs.csc.fi/support/faq/i-cannot-login/#why-is-my-ssh-client-saying-corrupted-mac-on-input"
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 166,
        "question": [
            "Can we set read-only permissions to entire folders?**"
        ],
        "answer": [
            "- A: Sure, for example `chmod -R g-w my_folder` would make a folder `my_folder` and its contents read-only for your UNIX group members (i.e. others in your CSC project). `-R` activates the recursive mode."
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 167,
        "question": [
            "When I use the lue module in my home directory, it shows that there was permission denied for 444 files. Is this normal or could there be a problem? I would've thought that I would have apermission on everything on my $HOME (that's also what's shown in the tutorial). Correction: 444 is the number of files, none were denied. I kept misreading it for 15 minutes. Sorry and thank you!**"
        ],
        "answer": [
            "- A: Ok! :) Yes, all files in your $HOME should be owned by you and thus there shouldn't be any permission denied errors. If such a thing would anyway happen, only way to fix it is to contact servicedesk@csc.fi"
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 168,
        "question": [
            "hi! Should I have access to the course project in csc? I couldn't see it in puhti. If that's the case could you please add me? ðŸ™‚**"
        ],
        "answer": [
            "- A: If you have another project, you can also use that for the course! If you don't have any project, let me (Maria) know, or follow the instructions in the \"prerequisite mini-course\"!"
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 169,
        "question": [
            "Is the private SSH key restricted to one computer only? I have two computers, one for work (issued by HY) and the other is personal. If I connect to Puhti using the SSH key, do I have to generate two private/public key pairs for respective computers?**"
        ],
        "answer": [
            "- A: Technically it is possible to copy the private key to another computer (e.g. by moving it on a USB stick), but it's recommended to have separate keys for each computer. This has a couple of advantages:",
            "    - The secret key never leaves your computer, so it's harder to steal it (think: losing the USB stick, someone getting it from the cloud service you uploaded it to for transfer...)",
            "    - If you find that the key is compromised (e.g. you accidentally paste its password somewhere you didn't want to), it's easy to remove its public key part from the target machine while still retaining easy access from your other computer"
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 170,
        "question": [
            "Are module handling recommendations (related to conda etc.) also relevant to cloud services like e-/cpouta?**"
        ],
        "answer": [
            "- A: Not really: Pouta does not use modules like the supercomputers do, as when you have a Pouta virtual machine, you are in complete control of it: you can install any software you like. It also doesn't use Lustre, so you don't need to worry about the small files created by conda installations. If you use different software with conflicting dependencies (e.g. two Python programs that need different versions of the same package), you of course need to use conda or other such tool for separating those environments each other, but no need to worry about Tykky or `module load`s or such."
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 171,
        "question": [
            "I'm not sure if this is the right time to ask this but here is goes. I recently needed to use scikit-bio in jupyter-lab in puhti which wasn't available. Is there any method to load these specific packages for python in my own environment?**"
        ],
        "answer": [
            "- A: In this case you could install scikit-bio and jupyter-lab yourself using e.g. the Tykky tool. See https://docs.csc.fi/computing/containers/tykky/ and also https://docs.csc.fi/computing/webinterface/jupyter/#tykky-installations."
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 172,
        "question": [
            "Shall we complete the e-learn course \"csc computing environment\" as well, i.e. is watch videos, do all tutorials and quizes etc.?**"
        ],
        "answer": [
            "- A: The materials there are pretty much the same stuff that we cover during the course days, so watching the videos and re-reading the slides etc is not mandatory. Completing the \"extra\" tutorials is also up to you, we recommend those that seem relevant to you! If you want the certificate for completing the course, you must complete the quizzes though."
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 173,
        "question": [
            "There seems to be a bug in the first quiz of the e-learn course (Topic 2: Quiz: HPC environment). I cannot fill in the first blank.**"
        ],
        "answer": [
            "- A: The \"Quiz: HPC environment\" one? I cannot reproduce the problem. Have you noticed that you don't need to type anything: the answer selection is available for drag'n'dropping on the right side of the text. If you are working on a mobile device, changing to a desktop/laptop might be worth trying. Otherwise if refreshing the page does not work, let us organizers know and we will figure it out!"
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 174,
        "question": [
            "cont.: Quiz: HPC environment. Yes, I noticed it's drag 'n' drop, and I'm working on my laptop, also I refreshed the page. But the problem remains. I can drag 'n' drop the words into the other gaps, but not into the first one. It doesn't stay, but flies back immediately... Thanks!**"
        ],
        "answer": [
            "- A: Thanks, we will investigate :)",
            "- A: Nothing seemed to be wrong, however I was able to reproduce the problem. I saved the quiz again and after that it seemed to work for me, so maybe something resetted."
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 175,
        "question": [
            "I am a bit lost about the parts where conda and python were mentioned. From what I understood is that we're working on linux terminal and we're using that, so what does python have to do with that and what is a conda? I'm not sure if my question is clear but I'm generally confused about what that is.**"
        ],
        "answer": [
            "- A: Python and conda are different things but very closely related.  As you probably know Python is a programming language. Conda is an open-source package manager  also for python packages. In other words, conda environment can be used to install python packages on e.g., Linux environment.",
            "- A: So if you are not using Python yourself, no need to worry about the Python/conda stuff! It's just fairly commonly used (and a common pitfall when it comes to the small files), and that's why it comes up in the examples.",
            "- Q: It was brought up during the installation of the software I'm using because an 'add-on' to the software needs a conda-based installation. I didn't understand what that is at the time, but if I get it correctly now, then it's just basically using the conda package manager to install that software? Is it either that OR containerization, where containerization is prefered because of the issue with 'many small files'? I'm sorry if I'm not so clear but it's a bit confusing to me. I ended up not installing it.",
            "  - A: Yes, the original instructions referred to using Conda to install the needed extra parts. In CSC HPC servers the use of Conda as-is is not allowed. Instead we have tool called Tykky that can be used to install Conda environments as containers. If you need some add-on to software, the software itself + the add-ons need to be installed with Tykky so the software can see the add-ons."
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 176,
        "question": [
            "What does it mean when it says we can \"write our own module files\"? Does this mean we just install our own software and package it into a module? What difference is that from just running a software that we installed on the puhti projappl for example?**"
        ],
        "answer": [
            "- A: when we say we can write our own module files, it means that we  install our own software and then we create lua file (Lua is a kind of scripting language) to load all dependent libraries and add executable paths for the installed software. This way, we can just load a module without needing to care for dependencies and adding paths before using it.  This is very important when there is a lot of software stack to manage. It may not be that important if we have few software programmes to manage. Alternatively, if you install a software and take care of adding binary paths and dependencies for it by yourself, that is just fine and no need to worrry about module system. In term of functionality there is no difference."
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 177,
        "question": [
            "What is the difference between needing memory and needing cores? How can I determine what resources my job needs?**"
        ],
        "answer": [
            "- A: CPU cores are what your program uses to run calculations and otherwise execute your program (from a computer's point of view, each step in your program is actually some kind of a mathematical operation or a series of them). Memory is what holds the numbers on which those calculations are currently carried out: this could be variables, arrays, images, text... Note that this is different from disk space though: even if you handle e.g. terabytes of images from disk, it might be that only one image at a time is loaded in memory if they are processed one at a time. It's not easy to give \"one size fits all\" answer that would cover all cases when it comes to determining how much resources your job needs, but here are some pointers that might help you:",
            "    - have you run your code on your own machine and either run out of memory or the execution takes up all available CPU?",
            "    - run a small test in the `test` queue on Puhti, see what `seff <your jobid>` says and adjust accordingly"
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 178,
        "question": [
            "How do I know if my job is considered as \"heavy\" computing or not? What would be the parameters (e.g. memory use) that define a job as light, moderate or heavy computing?**"
        ],
        "answer": [
            "- A: Bit difficult to suggest on a general level. Good starting point is to check application documentation where you may find whether the application is memory intensive, parallelisable or has specific resource needs. Besdes,the software itself, the amount of data you are processing can also decide what kind of resources you need. Once you get some idea about the software you can start with small data and perform some pilot experiments with minimal resources. You can then scale up the resources and data as needed. Only experience will help us better. Obviously, you can always ask the developer of the tool to know more information instantly and have a head start."
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 179,
        "question": [
            "What is considered a 'task'? In terms of the batch job script. How do I know how many tasks my job will have and how many CPUs I will need per task?**"
        ],
        "answer": [
            "- A: Slurm tasks refers to the number of MPI processes you use to run your job. How many you can use depends on your software. If your job supports MPI, you may request multiple tasks, but the optimal number depends on other factors, e.g. how big your calculation is. Note that just adding more tasks may not make the calculation faster after a certain point (this needs to be tested). If your job does not support MPI, requesting multiple tasks would just result in running the same thing multiple times.",
            "- A: By default one thread (or \"CPU\" in Slurm terminology, `--cpus-per-task`) is launched for each task. If your software implements both MPI parallelism *and* OpenMP threading, you may launch multiple tasks and also multiple threads per task. The total number of threads (CPUs) can be at most equal to the number of physical cores on a single node, or virtual cores (2*physical) if using hyperthreading. If your software implements only OpenMP, then you should use just `--ntasks=1` (but multiple `--cpus-per-task=N` is ok)",
            "- A: So for the question \"how to know?\", 1) read the documentation (both the official documentation of your software and docs.csc.fi) and 2) test!"
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 180,
        "question": [
            "The third question in the batch job quiz (where you have to move the command blocks into the right boxes) isn't working!**"
        ],
        "answer": [
            "- A: Thanks for reporting, will check!",
            "- A: Checked, and wasn't able to repeat this either :/ Although it DOES require a little bit patience, and hovering the text box above the correct area (color changes and then the text box can be dropped). ***Can you let us know which browser and OS you are using?***",
            "- I am using google chrome, but i got it to work now!"
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 181,
        "question": [
            "I didn't understand the part about the line endings. Is this something we need to worry about if we're using powershell to access puhti or?**"
        ],
        "answer": [
            "- A: It something that is good to be aware of. Sometimes when you create files with Windows there may be hidden characters added at the end of lines that will cause things to fail on Linux. Converting with `dos2unix` will solve these issues. So if you get strange errors and are not sure why, `dos2unix` is good to try!",
            "- Q: So this is something we need to worry about if we created a batch job file (or any other one) within windows that we then want to import and run on Linux, yes?",
            "- A: Exactly."
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 182,
        "question": [
            "Are we allowed to use input from one project and set the output to be in another project's scratch folder?**"
        ],
        "answer": [
            "- A: If you're a member of both projects, this is possible."
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 183,
        "question": [
            "How do we know if increasing the resources will not offer so much to our tasks?**"
        ],
        "answer": [
            "- A: It may be hard to say beforehand, this is why it is good to test. E.g. run with 1, 2, 4, 8, ... cores and see does the job always get twice as fast when you double the cores. Rule of thumb is that the job should be *at least* 1.5 faster when doubling the number of cores.",
            "- A: Regarding the memory, a good way is to do a test and then check with `seff` how much memory was used."
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 184,
        "question": [
            "What causes the slower performance of runnig batch jobs during high load times? I noticed that during Christmas break my models were running much faster than e.g. a month earlier even though the model should take similar running times.**"
        ],
        "answer": [
            "- A: Are you sure that your job really ran faster, as opposed to just finishing earlier due to having spent less time in the queue?",
            "- A: If your job is disk i/o dependent it could be related to load on file system (i.e. /scratch can get slower under heavy load). Perhaps your jobs would benefit from using local nvme disk?"
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 185,
        "question": [
            "how can we check for all these points mentioned? e.g. if cores are waiting, How do we know that, and then how do we address that? and the rest of the points as well, how do we test or check for them? If the seff doesn't see that all the memory is used, how would I know that I'm running out of memory and ralize that I need to request more?**"
        ],
        "answer": [
            "- A: Usually seff works fine enough. If your program uses more memory than allocated and is thus killed, `seff` fill show the job as `FAILED`. You can also see lines like `slurmstepd: error: Exceeded job memory limit` and `slurmstepd: error: StepId=21338959.batch exceeded memory limit (2367488 > 1048576), being killed` in the `slurm-<jobid>.out` file. When it comes to your run just being slower than you'd like, and cores idling, it's hard to give a definite answer on how to debug that: it depends a lot on what you are doing. You can always ask servicedesk@csc.fi for help in your specific case though, if you cannot figure out why your jobs aren't as efficient as they should be! Some pointers are available in performance checklist in docs.csc.fi."
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 186,
        "question": [
            "What is sacct? How is it different from srun?**"
        ],
        "answer": [
            "- A: https://docs.csc.fi/computing/running/submitting-jobs/ sacct = information about your past jobs, srun = submitting jobs in the batch script"
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 187,
        "question": [
            "\"If CPU efficiency seems too low, look at the completion time\" what does the completion time tell us?**"
        ],
        "answer": [
            "- A: This is mainly in reference in finding out the best number of CPUs for you job. If e.g. decreasing number of cores makes the job slower or increasing the number makes it faster, you should look at the completition time more than the listed CPU efficiency."
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 188,
        "question": [
            "Can we then reserve more memory but less CPU? Is a job killed if it's not using enough CPU/more memory than allocated?**"
        ],
        "answer": [
            "- A: Jobs will indeed be killed if they use more memory than is allocated: it would not be good if your program could write outside its allocated space, possibly on top of someone else's allocation. Jobs will not be killed for under utilizing CPU though (but it's still adviced to not reserve more than you need, both to shorten the time your jobs spend in the queue and to use the shared resources efficiently)"
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 189,
        "question": [
            "Where can one check the disk workload?**"
        ],
        "answer": [
            "- A: The global workload of the disks in the system can be seen e.g. on puhti.csc.fi: the usage metrics box includes disk lag. When it comes to how disk-heavy your program is, you can often get an idea just by figuring out how much data it reads from/writes to disk, and whether it's a large or small number of files. You can also test whether your program gets faster if you try using the local NVMe disks at $TMPDIR."
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 190,
        "question": [
            "What is an array job? and what does the last line in the batch script mean? /appl/soft/bio/course/sacct_exercise/test-a ${SLURM_ARRAY_TASK_ID}**"
        ],
        "answer": [
            "- A: Array jobs is something you do when you have multiple similar independent tasks to run, for example you run the same analysis steps to multiple files. https://docs.csc.fi/computing/running/array-jobs/",
            "- ${SLURM_ARRAY_TASK_ID} = looping through the array jobs",
            "- Q: How can we determine if we should use such a job instead of the other kinds?",
            "- A: As mentioned, when you have many independent similar jobs, you can use array jobs. You dont need to write any loops, array job will take care of submitting those independent jobs to cluster based on the parameters you have supplied to sbatch directive of array flag (see more information in the above link). Please note that these independent jobs should take reasonable computational time (if they are too short, it would be overkill to submit as a seperate job) and also come with limit on the number of jobs one can submit (on Puhti maximum limit is 200 jobs)"
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 191,
        "question": [
            "What does root-level mean?**"
        ],
        "answer": [
            "- A: Root-level = main level when talking about file locations. A person with root level access = \"administrator,\" \"root user\" or \"superuser.\")",
            "- A: And when talking about something being located \"at root\" or \"root level\", it means that it's location in the file system or such is directly at the \"root\" of the file system: there are no intermediate directories or such. For example a text file in someone's home directory might be found at `/home/someone/coolfile.txt`, whereas a file at root would be simply `/coolfile.txt` (but it would be unusual to have such a file in such a location)"
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 192,
        "question": [
            "Does that mean that a folder with other files in it inside a bucket is treated as one object alongside everything inside it?**"
        ],
        "answer": [
            "- A: We have a nice example tutorial about these cases: https://csc-training.github.io/csc-env-eff/hands-on/allas/allas-tutorial.html"
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 193,
        "question": [
            "How do we know if our job would need an NVMe? Why doesn't the job use scratch for space?**"
        ],
        "answer": [
            "- A: One obvious symptom suggesting that NVMe would be good is if your job runs faster on your own laptop than on Puhti. The disk on laptops is NVMe.",
            "- A: Also, knowing what your job does is of course useful. If you know beforehand that it will read/write a lot of (small) files, then NVMe may be useful (both to increase performance and avoid stressing the file system for all users).",
            "- A: Bottom-line: scratch space sits on the shared file system and performs poorly if your job does a lot of I/O. For other use scratch is just fine."
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 194,
        "question": [
            "From what I understood, loading  module loads its dependencies. Why then do we need to check with spider if there's anything else we need to load for our module?**"
        ],
        "answer": [
            "- A: module **spider** command does not load your application/sofwtare. It would only search if the searched module exists in the installations."
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 195,
        "question": [
            "Are containers and images the same thing? What about dockers and tykky? A place where all the small files are treated as one large one if I understand correcty. What was the container registry/container wrapper that was mentined in the lecture yesterday. There was a lot of new terminology for me and I didn't understand what was downloaded from the quay.io website.**"
        ],
        "answer": [
            "- A: People use containers and images interchangeably but they are not same. Container is more like an instance of image (or you can also think of container as image in action). When container is spawned, image is converted to a filesystem and you can launch as many containers as possible. Docker (also a company name) is a type of container platform like Singularity/Apptainer. Tykky is an in-house developed container wrapper at CSC to take of python-based installations of a complex software. Container registry is more like a database of images, usually hosted by organisations or companies. Dockerhub and quay.io are couple of examples of image registries. In the example, you were downloading image from the quay.io registry.",
            "- **Q: Is a container wrapper then the software that puts other applications into containers? And in this context is an image some kind of application?**"
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 196,
        "question": [
            "What are the allas-cli-... directories created when using Allas?**"
        ],
        "answer": [
            "- A: allas-cli-utils is the CSC provided toolkit for using Allas. You can download it to your linux or mac if you wish to use Allas wit the same command that waht we have in Puhti and Mahti (https://github.com/CSCfi/allas-cli-utils)"
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 197,
        "question": [
            "If file transfer takes longer than 8 hours and the Allas swift connection gets interrupted, what will happen? Will the files be corrupted?**"
        ],
        "answer": [
            "- A: In generally, you should remove the partly uploaded data and try again. There are special tools that you cann use in case of large data transfers. e.g. https://github.com/CSCfi/allas-cli-utils/blob/master/help/allas-dir-to-bucket.md We will cover this issue more deeply in the advanced course."
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 198,
        "question": [
            "Would we have a chance until the end of today or.. ? to ask any remaining questions on HackMD?**"
        ],
        "answer": [
            "- A: Yes, we will keep the HackMD open for today. After this we will close it from editing, but you will have the read-only access to the document. After that, you are most welcome to ask questions in our weekly research user sessions (every Wednesday 14:00-15:00 in Zoom: https://ssl.eventilla.com/usersupportcoffee)."
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 199,
        "question": [
            "In the breakout room with Rasmus, something was mentioned about creating .tar files. What was the question at the time and why/how would we need to create them?**"
        ],
        "answer": [
            "- A: .tar archives are useful if you want to package your directory as a single file. I guess the question was about transferring a whole folder to Allas and whether we can be sure that the folder hierarchy is preserved even though Allas only has two levels of hierarchy (buckets and objects). If you want to be sure, then creating a tar archive (and optionally compressing) it before transfer to Allas is one way. If you use a-commands (e.g. a-put) to upload to Allas, then this will actually do the archiving for you, so no need to worry about it separately! One can also add `-c` option to compress it. See https://docs.csc.fi/data/Allas/using_allas/a_commands/ and https://docs.csc.fi/support/tutorials/env-guide/packing-and-compression-tools/"
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 200,
        "question": [
            "If I installed some software on the projappl folder, and its GUI is web application that I can access from my browser via localhost, is there a way to launch this web application though the interactive partition? Would the application then be able to submit batch job scripts for the processing tasks of the software itself? The current set-up is that the web application is launched through the login node (from my understanding, that is to enable the software to submit batch jobs).**"
        ],
        "answer": [
            "- A: This sounds like a quite complicated use case. Perhaps you could send a more detailed email to servicedesk@csc.fi ? This page might also be helpful: https://docs.csc.fi/support/tutorials/rstudio-or-jupyter-notebooks/ (it is about running rstudio and jupyter, but the SSH tunneling principles should be the same for any application)"
        ],
        "source": "csc-enveff-20240424"
    },
    {
        "idx": 201,
        "question": [
            "Have I clicked the edit mode on?**"
        ],
        "answer": [
            "- A: Probably not yet.. â†–ï¸"
        ],
        "source": "csc-enveff-20240515"
    },
    {
        "idx": 202,
        "question": [
            "Slides available after the course?**"
        ],
        "answer": [
            "- A: They sure are! You will also have the access to this HedgeDoc document (save the link).",
            "    - The slides and tutorials are available here: https://csc-training.github.io/csc-env-eff/",
            "    - Access to e-lena (and the quizzes there) may discontinue.",
            "- We encourage you to share and use the material also in your own courses. The material is in git, and we welcome all feedback and edit suggestions (pull requests)!"
        ],
        "source": "csc-enveff-20240515"
    },
    {
        "idx": 203,
        "question": [
            "Has the Zoom meeting already started? I'm just gettin \"The meeting has not started\" -note.**"
        ],
        "answer": [
            "- A: Make sure you have the full Zoom link when connecting! It's long and contains the password :)"
        ],
        "source": "csc-enveff-20240515"
    },
    {
        "idx": 204,
        "question": [
            "Should we be able to access the slides in e-elena? I can only see the first slide.**"
        ],
        "answer": [
            "- A: Try navigating with the arrow keys :)"
        ],
        "source": "csc-enveff-20240515"
    },
    {
        "idx": 205,
        "question": [
            "So many credendials... Which ones do I need and where?!?**"
        ],
        "answer": [
            "- A: You need",
            "    - **CSC credentials** to Puhti/Mahti",
            "    - **Haka OR Virtu credentials** (=your university/institution credentials) to my.csc.fi",
            "    - **Haka credentials** (=your university/institution credentials) to eLena platform",
            "    - contact us if you don't have HAKA/Virtu in use"
        ],
        "source": "csc-enveff-20240515"
    },
    {
        "idx": 206,
        "question": [
            "I have forgotten my CSC username and password**"
        ],
        "answer": [
            "- A: In my.csc.fi you can check your CSC username. If you have forgotten your CSC password, check this link: https://docs.csc.fi/accounts/how-to-change-password/ -Note that it takes some time for the password to update to Puhti, if you need to change that :)"
        ],
        "source": "csc-enveff-20240515"
    },
    {
        "idx": 207,
        "question": [
            "Depending on the place you say to obtain acces to Puhti and Allas or Puhti and Mahti, should we have all three of them (just in case)?**"
        ],
        "answer": [
            "- A: On the course there is one tutorial in which you can try moving data from Puhti to Mahti. It is an optional task so you don't necessarily need Mahti."
        ],
        "source": "csc-enveff-20240515"
    },
    {
        "idx": 208,
        "question": [
            "Do you record the sessions?**"
        ],
        "answer": [
            "- A: No. However, the lecture videos, slides and hands-ons are available in the github page, and all the material + the quizzes etc can be accessed in eLena platform, so this course works well as self-study also!"
        ],
        "source": "csc-enveff-20240515"
    },
    {
        "idx": 209,
        "question": [
            "What counts as heavy I/O? Opening 10/100/1000 files?**"
        ],
        "answer": [
            "- A: We will get back to this topic tomorrow when talking about workflows and speeding up jobs. But as a rule of thumb, pay attention when opening 1000s of files and you should **not** work with 10 000s-100 000s on Lustre. Working with 10-100 files is not a problem usually."
        ],
        "source": "csc-enveff-20240515"
    },
    {
        "idx": 210,
        "question": [
            "What counts as a \"small file\"? Does the file format matter?**"
        ],
        "answer": [
            "- A: It is hard to give an exact number, but small files typically refers to kB order of magnitude. File format does not really matter."
        ],
        "source": "csc-enveff-20240515"
    },
    {
        "idx": 211,
        "question": [
            "Do the small files cause issues if they're idle, or are they only an issue if they're constantly being accessed?**"
        ],
        "answer": [
            "- A: They can also cause issues even if they are idle. Especially having a lot of files in a single directory is not recommended. Having some kind of hierarchy is better (or even better, tar and compress the files)."
        ],
        "source": "csc-enveff-20240515"
    },
    {
        "idx": 212,
        "question": [
            "Why would we use the allas-conf -k instead of establisheing an S3 connection?**"
        ],
        "answer": [
            "- A: As you would probably have realised that S3 connection is persistent, this may have security concerns. Swift connection is more secure as you refresh for every 8 hours.",
            "- A: in addition to the above, using S3 protocol is OK if you use it all the time, but if you have uploaded the data using Swfit protocol, then you should also download it with Swift protocol."
        ],
        "source": "csc-enveff-20240515"
    },
    {
        "idx": 213,
        "question": [
            "What should I do if there is data in our scratch from a previous user that I cannot delete?**"
        ],
        "answer": [
            "- A: You should send a ticket to <servicedesk@csc.fi>. Our admins can edit the permissions."
        ],
        "source": "csc-enveff-20240515"
    },
    {
        "idx": 214,
        "question": [
            "What do I do with data in scratch that we work with for more than 6 months?**"
        ],
        "answer": [
            " - A: Unfortunately, cleaning process will be in force on Puhti and is bit incovenient if you are in the middle of some analysis. As mentioned in the lecture, if you have lot of data, please move them to allas during cleaning process. To ease this process, CSC has provided some easy-to-use utility commands like lcleaner (https://docs.csc.fi/support/tutorials/clean-up-data/#using-lcleaner-to-check-which-files-will-be-automatically-removed). If the data is small enough and is mixed with program/config files, you might use /projappl space (which is more dedicated for storing compilied programs/packages/installation stuff). /projappl space will not be cleaned."
        ],
        "source": "csc-enveff-20240515"
    },
    {
        "idx": 215,
        "question": [
            "So you need to do allas-conf -k only once or again each time you login with ssh? And again if you change the project you work on?**"
        ],
        "answer": [
            " - A: Once you logout from supercomputing environement, the env variable that stores your password will be unset.",
            " - **Q: Did I get it right, if I execute batch-script and before the run starts my laptop for example loses internet connection hence the ssh connection breaks, the batch job wont have the allas access?**",
            " - A: Once you submit your batch script (which has already access to your allas credentials), that is fine. Your loosing connection to supercomputer later from laptop will not affect the already running batch job.",
            " - Q: already running batch job = batch job in que / actually running?",
            " - yes. to be precise, it is enough that you have submitted the job to the cluster.",
            " - great, thanks",
            " - you are welcome !"
        ],
        "source": "csc-enveff-20240515"
    },
    {
        "idx": 216,
        "question": [
            "Is $LOCAL_SCRATCH the NVMe?**"
        ],
        "answer": [
            " - A: Yes, that's correct.",
            " - **Q: If we reserve NVMe does the code run only there? What if we don't reserve enough and it runs out? How can we know what space we need, just the amount of storage of the files we want to process?**",
            " - You have to reserve NVMe disk space in your batch script and move your data to the local disk area (i.e., NVMe disk area) and compute there. Once you finish your heavy I/O computation there, you have to move all your results back to parallel file system area (i.e., scratch). In short, you have to intentionally do the computation on NVMe and it does not happen there just because we reserve NVMe disk space. Yes. if we dont have enough space in NVMe, your batch job fails. It is difficult to know exact amount of NVMe space needed beforehand. It depends on your application input data and resulting data from your analysis. You can request bit more disk space than you need to be on  safe side."
        ],
        "source": "csc-enveff-20240515"
    },
    {
        "idx": 217,
        "question": [
            "NVME billing is pretty expensive, why is it billed so heavily in BUs?**"
        ],
        "answer": [
            " - A: NVMe is a scarce resource on CSC supercomputers, that's why it is more expensive than regular disk space. A little bit like GPU-resources.",
            " - A: You can check NVMe availability per node type in https://docs.csc.fi/computing/systems-puhti/"
        ],
        "source": "csc-enveff-20240515"
    },
    {
        "idx": 218,
        "question": [
            "What are binaries?**"
        ],
        "answer": [
            " - A: They are programs that you can run. Like a command. Some programming languages are intepreted, so you can just run the code itself. However, many codes need to be first compiled into a program (a binary, a.k.a. executable).",
            " - **Q: By programming langauges, do you refer to the language that the progrom was written in?**",
            " - A: Yes. For example, C code needs to be compiled into a binary while Python code can be run as is."
        ],
        "source": "csc-enveff-20240515"
    },
    {
        "idx": 219,
        "question": [
            "Why should we compile software in $TEMPDIR first and then move it to projappl? Can't we do it in projappl directly? Do we also need to install a software there and then move it to projappl, or can we install it to projappl directly?**"
        ],
        "answer": [
            " - A: Because compilation usually creates a lot of I/O load. So moving that away from Lustre to the login node NVMe is recommended - it will make the compilation faster and does not cause lags in Lustre for others.",
            " - If the software is very simple and small, you could do compilation directly in /projappl as well.",
            " - No need to *install* in $TMPDIR, just *compile* it there, and then there are usually options you can give the build tools that install the software to /projappl when you run e.g. `make install`. See the tutorials for more details about this. E.g. `./configure --prefix=/projappl/...` and `-DCMAKE_INSTALL_PREFIX=/projappl/...`"
        ],
        "source": "csc-enveff-20240515"
    },
    {
        "idx": 220,
        "question": [
            "I installed the development version of a R-package from github. It's a newer version than that available in CRAN and already installed in Puhti. How do I get the development version to work in Puhti? I have got this working on my own laptop, but not in Puhti.**"
        ],
        "answer": [
            " - A: Did you check the tutorial? https://csc-training.github.io/csc-env-eff/hands-on/installing/installing_hands-on_r.html",
            " - A: if there are some errors, seeing those messages would helpful."
        ],
        "source": "csc-enveff-20240515"
    },
    {
        "idx": 221,
        "question": [
            "Is it possible to create a container that would be under the project, not under a specific user? I would wish that all project users have similar access to it.**"
        ],
        "answer": [
            " - A: Sure! Just move it to a shared directory under your project's /projappl",
            " - A: Access permissions can be edited with `chmod` commands. E.g., to give all project members execute permissions for the container, try `chmod g+x my_container.sif`",
            " - Thank you!"
        ],
        "source": "csc-enveff-20240515"
    },
    {
        "idx": 222,
        "question": [
            "Should bin be lib able to find the libhello.so? I think there's a mistake in using CMake exercise**"
        ],
        "answer": [
            " - A: Yes, sorry about that!",
            " - A: If you reload the page, this should now be fixed."
        ],
        "source": "csc-enveff-20240515"
    },
    {
        "idx": 223,
        "question": [
            "If we're queuing for a  batch job, is there a way to estimate how much we would wait or our position in the queue?**"
        ],
        "answer": [
            " - A: Yes, you can add option `--start` in you `squeue` command (e.g. `squeue --me --start`) to see an estimate of when the job will start running. However, this is only a rough estimate and it may change depending on what jobs are submitted later on that may have a higher priority."
        ],
        "source": "csc-enveff-20240515"
    },
    {
        "idx": 224,
        "question": [
            "What are the login nodes available on puhti, i couldn't find a list of them or how to access a certain node in the docs.csc website about the disk areas or elsewhere. I'm currently on node 12 for example**"
        ],
        "answer": [
            " - A: They are `puhti-login11`, `puhti-login12`, `puhti-login14` and `puhti-login15` (https://docs.csc.fi/support/wn/comp-new/#new-login-nodes-on-puhti-9112022). You can for example go to specific login node on Puhti by specifying the login node information as : *ssh username@puhti-loginxx.csc.fi* (xx-> 11,12,14,15)"
        ],
        "source": "csc-enveff-20240515"
    },
    {
        "idx": 225,
        "question": [
            "What is a container engine and what does it do?**"
        ],
        "answer": [
            " - A: It is a piece of software that is used to build and run containers",
            " - Q: If a container was built using a different engine than Apptainer that CSC uses, do we need to use this apptainer build command, or is it for something else? I'm not sure I understand the function of this command.",
            " - You can think of container engine as some management system for containers: it can for exmaple spins up, cancel your containers when needed.  It is container specific. Docker container engine just manages docker containers on host system and not for other containers.",
            " - A: However, it is easy to convert Docker containers to Apptainer containers. It is often just a single command, see e.g. https://csc-training.github.io/csc-env-eff/hands-on/singularity/singularity-tutorial_how_to_get_containers.html",
            " - Q: What do you mean by \"spin up, cancel your containers when needed\"?"
        ],
        "source": "csc-enveff-20240515"
    },
    {
        "idx": 226,
        "question": [
            "Do we need to bind a directory every time we run the software, or is this something that can be setup permamnently for a container?**"
        ],
        "answer": [
            " - A: It depends. You need to bind or mount a host file system when you are trying to write something permanently on disk. Just for running some command, you don't need to bind  a file sytem.  Host system always needs to be mounted during runtime.",
            " - Q: How could we bind it permanently? For example, to have the software always be able to acces my files when i use it to submit batch jobs.",
            " - There seems to be some misunderstanding here. Once you mount a host folder under container, it will be there as long as container is running. So one does not need to do anything. But for some reason, if container dies, one needs to bind them again.",
            " - Instead of using --bind you can set APPTAINER_BIND variable. Syntax is the same as for --bind `export APPTAINER_BIND=\"dir1,dir2:/input`. This may make the command more readable if you have complex bind."
        ],
        "source": "csc-enveff-20240515"
    },
    {
        "idx": 227,
        "question": [
            "Could you please confirm if Tykky environments are only suitable for python/conda packages?**"
        ],
        "answer": [
            " - A: Yes, you can also install other than Python/Conda stuff with tykky. `conda-containerize update` command allows you to run any installation commands (e.g. clone repo from GitHub and install something). `wrap-container` command can also be used to generate wrapper scripts for an existing container. See https://docs.csc.fi/computing/containers/tykky/#modifying-a-conda-installation and https://docs.csc.fi/computing/containers/tykky/#container-based-installations"
        ],
        "source": "csc-enveff-20240515"
    },
    {
        "idx": 228,
        "question": [
            "What output should we expect from step 2 under 'using the `apptainer wrapper` script' section in the second tutorial? It doesn't return anything for me and im not sure i understand what the second command does.**"
        ],
        "answer": [
            " - A:  *Apptainer_wrapper* command binds many host directories(``` /projappl, /scratch,$HOME... ```) by  default. So if your env variable, `$`SCRATCH refers to some directory under your project (e.g., ``` /scratch/project_xxx/$USER```), you should be able to see all the content inside of the host folder (i.e.,/scratch/project_xxx/$USER)",
            "- Can you try directly using actual path (instead of as env variable) like in the command: ```apptainer_wrapper exec ls /your/scratch/path ``` ?"
        ],
        "source": "csc-enveff-20240515"
    },
    {
        "idx": 229,
        "question": [
            "When in sinteractive and we're running a container, are we able to submit batch jobs for more processing power**"
        ],
        "answer": [
            " - A: Yes. You can submit batch jobs from interactive nodes."
        ],
        "source": "csc-enveff-20240515"
    },
    {
        "idx": 230,
        "question": [
            "If I start an array job with four arrays, the output file will only show the billing unit consumption of one array (~1/4 of the total). Is there a way to see the BU consumption of the whole array job?**"
        ],
        "answer": [
            " - A: Did you use seff command to find the BUs? By default, seff gives information on the first array job. You can also try `seff <jobid>_1, seff <jobid>_2, seff <jobid>_3 ...`   You can also try using *sacct*  command for more details on job at a time  (`sacct -o jobname,jobid,reqmem,maxrss,timelimit,elapsed,state -j <slurmjobid>`)"
        ],
        "source": "csc-enveff-20240515"
    },
    {
        "idx": 231,
        "question": [
            "Do many large files also pose a problem?**"
        ],
        "answer": [
            " - A: For large files, parallel file system is still better.  The real problem is with metadata handling. For the given data, if you supply in many small files, file system has to do many file I/O operations as comapared to the operations needed in hanlding teh same data in few bigger files.",
            " - Q: What if our data was a several thousand large files? Does the same thing apply?",
            " - Yes. Metadata operations will be increased accordingly.There are some better optimisation methods like *file striping* can be done on parallel file system to mitigate the issues. The issue is also not just the size of files, but also how many operation you are eventually doing with that big files. if you are accessing bigger files several times as part of cmputation, this is different thing than the accesing the file once during the computation.  Other things like accessing some data randomly from some part of  files do matter."
        ],
        "source": "csc-enveff-20240515"
    },
    {
        "idx": 232,
        "question": [
            "Sorry about the long question, I'd just like to confirm that I understand it correctly, does using NVMe solve the problem with the many small files? (where it's just used to store the intermediate data during processing? because in that case then the software still has to access the same amount of large files so I'm a bit confused. Or are we supposed to move the data into the NVMe beforehand?)  And that would be in terms of the data we're running. And then when it comes to the software, if it has a lot of files in and of-itself, we need to use it in a container. These two things are ways to go around the 'large number of file' problem but for two separate scenarios?**"
        ],
        "answer": [
            " - A: The idea when using NVMe would be for example this: 1) you have a tar-package containing a lot of small files you need to process. You do not want to do it on Lustre, because it would be inefficient. 2) Instead, you copy the package containing the files to NVMe and only then decompress it (so that you at no point have a lot of files on Lustre). 3) On the NVMe, you process the data. 4) When you're done, you package the results and move them back to /scratch.",
            " - A: This could be done either in an interactive session or in a batch job. But yes, the important thing is that you need to *move* the data first to NVMe, and then also remember to move it back (because the NVMe is cleaned automatically immediately when the job ends).",
            " - A container is another way to mitigate the large file number, since a container is just 1 file from the file system point of view.",
            " - **Q: Would this still be practical if we have really large files? the packing and unpacking, would it perhaps take a couple of hours for some amount of TB in data?**"
        ],
        "source": "csc-enveff-20240515"
    },
    {
        "idx": 233,
        "question": [
            "srun: fatal: SLURM_MEM_PER_CPU, SLURM_MEM_PER_GPU, and SLURM_MEM_PER_NODE are mutually exclusive.!, I get this error in the first exercise for speeding up jobs, Performing a simple scaling test. Did I typo something?**"
        ],
        "answer": [
            " - A: Probably. Did you launch the job from an interactive session? You should be on the login node, otherwise there may be some issues.",
            " - Q: Thanks that was the issue, I was in a interactive session."
        ],
        "source": "csc-enveff-20240515"
    },
    {
        "idx": 234,
        "question": [
            "Do jobs submitted with HyperQueue inherit $PATH from the session where they are launched? In other words, I have snakemake and other software in a container that I created with Tykky. I want to tell snakemake to submit jobs with HyperQueue. Will these jobs see the software that is inside the container? PS: I could also have the container as an apptainer image made from a docker image, it this would work somehow instead of the wrapper scripts from Tykky.**"
        ],
        "answer": [
            " - A: Did you check our tutorial here: https://docs.csc.fi/support/tutorials/snakemake-puhti/ ?",
            " - We also have a dedicated tutorial, combing Hyperqueue and singularity container image in snakemake workflow here: https://csc-training.github.io/csc-env-eff/hands-on/throughput/snakemake-ht.html",
            " - A: I have not, will take a look, thank you! I had tried before with snakemake+SLURM and it didn't work so I found another sub-optimal alternative. It seems that HQ should work :)",
            "- CSC docs covers different possible scenarios including the case of  snakemake+slurm. please check it. If you have too many jobs (rules) from bigger sample set, we recommend using HyperQueue.",
            "- A: Great, thank you!",
            "> **THANK YOU very much for your active participation! Questions are now closed. Feel free to ask more at servicedesk@csc.fi and/or join our weekly research user meetings**"
        ],
        "source": "csc-enveff-20240515"
    },
    {
        "idx": 235,
        "question": [
            "Have I clicked the edit mode on?**"
        ],
        "answer": [
            "- A: Probably not yet.. â†–ï¸"
        ],
        "source": "csc-enveff-20241003"
    },
    {
        "idx": 236,
        "question": [
            "Slides available after the course?**"
        ],
        "answer": [
            "- A: They sure are! You will also have the access to this HedgeDoc document (save the link).",
            "    - The slides and tutorials are available here: https://csc-training.github.io/csc-env-eff/",
            "    - Access to e-lena (and the quizzes there) may discontinue.",
            "- We encourage you to share and use the material also in your own courses. The material is in git, and we welcome all feedback and edit suggestions (pull requests)!"
        ],
        "source": "csc-enveff-20241003"
    },
    {
        "idx": 237,
        "question": [
            "Has the Zoom meeting already started? I'm just gettin \"The meeting has not started\" -note.**"
        ],
        "answer": [
            "- A: Make sure you have the full Zoom link when connecting! It's long and contains the password :)"
        ],
        "source": "csc-enveff-20241003"
    },
    {
        "idx": 238,
        "question": [
            "Should we be able to access the slides in e-elena? I can only see the first slide.**"
        ],
        "answer": [
            "- A: Try navigating with the arrow keys :)"
        ],
        "source": "csc-enveff-20241003"
    },
    {
        "idx": 239,
        "question": [
            "So many credendials... Which ones do I need and where?!?**"
        ],
        "answer": [
            "- A: You need",
            "    - **CSC credentials** to Puhti/Mahti",
            "    - **Haka OR Virtu credentials** (=your university/institution credentials) to my.csc.fi",
            "    - **Haka credentials** (=your university/institution credentials) to eLena platform",
            "    - contact us if you don't have HAKA/Virtu in use"
        ],
        "source": "csc-enveff-20241003"
    },
    {
        "idx": 240,
        "question": [
            "I have forgotten my CSC username and password**"
        ],
        "answer": [
            "- A: In my.csc.fi you can check your CSC username. If you have forgotten your CSC password, check this link: https://docs.csc.fi/accounts/how-to-change-password/ -Note that it takes some time for the password to update to Puhti, if you need to change that :)"
        ],
        "source": "csc-enveff-20241003"
    },
    {
        "idx": 241,
        "question": [
            "Depending on the place you say to obtain acces to Puhti and Allas or Puhti and Mahti, should we have all three of them (just in case)?**"
        ],
        "answer": [
            "- A: On the course there is one tutorial in which you can try moving data from Puhti to Mahti. It is an optional task so you don't necessarily need Mahti."
        ],
        "source": "csc-enveff-20241003"
    },
    {
        "idx": 242,
        "question": [
            "Do you record the sessions?**"
        ],
        "answer": [
            "- A: This time: Yes. However, we don't promise to deliver them afterwards, as they will require editing and captioning, which takes time. Slides and hands-ons are available in the github page, and all the material + the quizzes etc can be accessed in eLena platform, so this course works well as self-study also!"
        ],
        "source": "csc-enveff-20241003"
    },
    {
        "idx": 243,
        "question": [
            "There's no \"Sep24\" option in e-Learn...?**"
        ],
        "answer": [
            "- A: Good catch! Now there is, except it's Oct24, as it is actually October now :D"
        ],
        "source": "csc-enveff-20241003"
    },
    {
        "idx": 244,
        "question": [
            "Do I get a notification that my csc account will be locked soon?**"
        ],
        "answer": [
            "- A: CSC sends an e-mail once a year requesting you to reset your password. Failure to reset the password will automatically lead to the locking of your account. CSC service desk (servicedesk@csc.fi) can help you if your account is locked for any reason."
        ],
        "source": "csc-enveff-20241003"
    },
    {
        "idx": 245,
        "question": [
            "What actually are remote graphics?**"
        ],
        "answer": [
            "- A: Here, we mean any graphical application that is running on HPC environment. So it is like accessing a GUI tool (for example Jupyter notebook) on your personal computer browser (i.e., local environment) while the tool is running on a remote enviroment (e.g., HPC environment)"
        ],
        "source": "csc-enveff-20241003"
    },
    {
        "idx": 246,
        "question": [
            "Regarding the folder organization, I am lost on what is the folder ondemand? What is scratch?**"
        ],
        "answer": [
            "- A: You will learn more about these folders later. Open ondemand folder refers to the folder on supercomputer web interface. Scratch folder refers to a type of diskspace that a CSC user gets on supercomputers. (FYI, Open ondemad is a technology that allows building web interfaces to supercomputers )"
        ],
        "source": "csc-enveff-20241003"
    },
    {
        "idx": 247,
        "question": [
            "Can we have some SSH files as examples to learn how to compile commands?**"
        ],
        "answer": [
            "- A: do you perhaps mean these compile commands here: https://docs.csc.fi/computing/connecting/ssh-keys/ ? We have a good documentation on how to set up SSH keys."
        ],
        "source": "csc-enveff-20241003"
    },
    {
        "idx": 248,
        "question": [
            "Can I see the difference between the login nodes or supercomputer nodes. On the folders of my project I can't see the tmpdir**"
        ],
        "answer": [
            "- A: Once you login to supercomputer, you will be automatically in login nodes. HOME/Projappl/Scratch folders are available on login and compute nodes. so there is no difference for these folders on parallel file system. However Temporary folder is different.  You have to refer to $TMPDIR (try `echo $TMPDIR` to see the exact path) on login node and $LOCAL_SCRATCH on compute node.",
            "- A: Note that $LOCAL_SCRATCH is available only if resources for it have been specifically requested in the batch job script or when starting an interactive session. More on this tomorrow."
        ],
        "source": "csc-enveff-20241003"
    },
    {
        "idx": 249,
        "question": [
            "Which data storage should be used to install additional python libraries, virtual environments ?**"
        ],
        "answer": [
            "- A: You will learn more about it later. Short answer is /projappl area for your project."
        ],
        "source": "csc-enveff-20241003"
    },
    {
        "idx": 250,
        "question": [
            "when do we use chmod and/chown to give access to files in subfolders to other group members?:)**"
        ],
        "answer": [
            "- A: By default the files and directories you create in /projappl and /scratch are accessible to all group members. You can use chmod to reduce the prermissions to your data. E.g. you could make a directory to be \"read.only\" for other project members with command   chmod -R g-w your-directory",
            "- Having a scratch directory, where only you have an access is not a good idea because if you later on leave from the CSC project, no one will have access to the data."
        ],
        "source": "csc-enveff-20241003"
    },
    {
        "idx": 251,
        "question": [
            "How can I get files from my computer to Puhti? Can I wget them or which command should I use? Some universities don't give permission to download SSH-client to employees' own user but rather to a admin user which then again don't have the files for the analysis.**"
        ],
        "answer": [
            "- A: Next lecture is more about data transfer. We will learn more about that. There are many protocols/transfer clients that one can use to transfer data to Puhti. if the clients need admin permissions for installation, I would suggest asking admins to install them on your computer.",
            "- A: Check https://docs.csc.fi/data/moving/."
        ],
        "source": "csc-enveff-20241003"
    },
    {
        "idx": 252,
        "question": [
            "can i delete the files we used in the hands on or do we use them later?**"
        ],
        "answer": [
            "- A: They can be deleted. We won't need them later."
        ],
        "source": "csc-enveff-20241003"
    },
    {
        "idx": 253,
        "question": [
            "Will the use of Allas in this tutorial take any extra BU consumption**"
        ],
        "answer": [
            "- A: Not really. The files are small and can be deleted after the exercise. The usage is calculated by TB."
        ],
        "source": "csc-enveff-20241003"
    },
    {
        "idx": 254,
        "question": [
            "Should I always transfer local files to Allas first, then move them to Puhti/Mahti?**"
        ],
        "answer": [
            "- A: No need to move the data to allas first. But making data available in allas makes it convenient to move around different services quite quickly ad easily."
        ],
        "source": "csc-enveff-20241003"
    },
    {
        "idx": 255,
        "question": [
            "What would be the best method of moving large amount of individual files to an Allas bucket?**"
        ],
        "answer": [
            "- A: `rclone` with (-P flag, for progress ) would be one option."
        ],
        "source": "csc-enveff-20241003"
    },
    {
        "idx": 256,
        "question": [
            "Can you describe it again how to get the certificate after training sessions**"
        ],
        "answer": [
            "- A: In e-Lena e-learning portal, you need to complete all the quizzes in part1, and give the course feedback. After these steps, the certificate should show up, and you should be able to download it!"
        ],
        "source": "csc-enveff-20241003"
    },
    {
        "idx": 257,
        "question": [
            "How long can we store our data in Allas?**"
        ],
        "answer": [
            "- A: For the duration of your computing project. When the project is about to end, you need to download and store the data somewhere else, or extend the project. See https://docs.csc.fi/accounts/how-to-manage-your-project/",
            "- Note: the **course project** is only valid during the course! It's also not meant for your own use, so don't upload any own files there!",
            "- [x] **How can we search the modules that are installed in Puhti/Mahti**",
            "- A: The vast majority of all applications installed on CSC systems are implemented as modules, so the applications list in CSC documentation serves as a pretty comprehensive description of available modules.",
            "However, due to the large number of installed software, some modules may not have a documentation page. `module avail` shows all available modules (such that you can load directly without first loading some dependencies), `module spider <pattern>` allows searching for specific ones and also listing all modules, including those that cannot be loaded currently due to missing (not loaded) dependencies. See also answer below.",
            "   - A: `module spider` with no arguments shows all installed modules, but since the list is quite long it may not be that practical.",
            "   - A: In this tutorial, even though bio-related, we go though a bit how to search for applications: https://csc-training.github.io/csc-env-eff/hands-on/modules/module-exercise-with-aligners.html"
        ],
        "source": "csc-enveff-20241003"
    },
    {
        "idx": 258,
        "question": [
            "`module load StdEnv` # Load the default module environment -> what is dfault module environment**"
        ],
        "answer": [
            "- A: On Puhti it is `gcc/11.3.0`, `openmpi/4.1.4`, `intel-oneapi-mkl/2022.1.0`, `csc-tools`. In other words, it contains a specific compiler suite (gcc/11.3.0), MPI library (openmpi/4.1.4), linear algebra library (intel-oneapi-mkl/2022.1.0) and some tools developed by CSC (csc-tools).",
            "- A: `module list` shows currently loaded modules, so you can always run that before and after a `module` command to see what changed.",
            "- A: You can also try `module show StdEnv` to see what loading the module does."
        ],
        "source": "csc-enveff-20241003"
    },
    {
        "idx": 259,
        "question": [
            "So when I log into Puhti, which project's BUs am I using if I have several ongoing projects? Can I move between projects to allocate my BU usage?**"
        ],
        "answer": [
            "- A: Project BUs are consumed when you run jobs (or have extended the project's maximum available storage space). For this reason, the only obligatory argument to provide when running jobs is the accounting project i.e. the CSC project whose BUs will be used for the job. So the project to be billed must always be specified."
        ],
        "source": "csc-enveff-20241003"
    },
    {
        "idx": 260,
        "question": [
            "What are the best settings (number of CPU, memory, image quality, and compression) for using the desktop in Puhti/Mahti and running programs like ParaView with a graphical interface? It has several settings, but the default ones are not good for smooth performance.**"
        ],
        "answer": [
            "- A: For ParaView, I would recommend trying the accelerated visualization, since it allows the software to run and render on GPUs. The performance will this way be significantly improved. To see all apps with accelerated visualization available, see https://docs.csc.fi/computing/webinterface/accelerated-visualization/",
            "- A: Memory is a bit tricky to predict beforehand, this may depend on a case-by-case basis. The GPU nodes have 384 GB memory in total, so when reserving 1 GPU (out of the 4 available in total on a node), you can safely reserve up to 1/4 of the total memory (~90 GB).",
            "- A: If responsiveness is the main goal, higher compression may help in some cases. Then again, if accurate visual representation (while using the desktop) is important, compression should be as low as possible."
        ],
        "source": "csc-enveff-20241003"
    },
    {
        "idx": 261,
        "question": [
            "Can we access this HackMD file after the training ends? It would be nice to have access to it. (i.e as PDF)**"
        ],
        "answer": [
            "- A: Yes. This HackMD page stays here after the training ends. Feel free to export these HackMD pages for example to your google drive or download as HTML/markdown files"
        ],
        "source": "csc-enveff-20241003"
    },
    {
        "idx": 262,
        "question": [
            "The last question in the Quiz: Module system (load Gromacs module) appears to be out of date - it refers to \"gromacs-env/2023\" which is now \"gromacs/2024.3\", and the loaded versions of \"gcc\", \"intel-oneapi-mkl\", and \"openmpi\" are also more recent than those listed in the answers.**"
        ],
        "answer": [
            "- A: Good catch, we'll update this after the course (we cannot do it now since it will reset your progress). Thanks for reporting!"
        ],
        "source": "csc-enveff-20241003"
    },
    {
        "idx": 263,
        "question": [
            "If my batch run takes e.g. 12 hours, can I just shut off Puhti (with e.g. exit-command) and my computer and the job stays running? Or is there a special command that is needed to shut the system AND keeping it running?**"
        ],
        "answer": [
            "- A: Yes. Once you submit your job with your batch script, you can log out of the supercomputer. Your submiited job stays in queue and when it gets the requested resoures your job will be executed on the cluster (on compute nodes). You don't need to do anything else than just submitting job which will be run irrespective of whether you are logged in or out of the supercomputer."
        ],
        "source": "csc-enveff-20241003"
    },
    {
        "idx": 264,
        "question": [
            "If only there were some convenient way of monitoring job progress...**"
        ],
        "answer": [
            "- A: `tail -f slurm-<job_id>.out` (watch the output file and print any new lines written to it). You may also replace `slurm-<job_id>.out` with any other log file that your job might be outputting.",
            "- A: You can also monitor your jobs in the queue with `squeue --me` (equivalent to `squeue -u $USER`), but don't run it too often since the command can be heavy on the batch job system (especially with hundreds of users using the system at the same time)."
        ],
        "source": "csc-enveff-20241003"
    },
    {
        "idx": 265,
        "question": [
            "how do i know which queue to use? in the hands-on the test-queue**"
        ],
        "answer": [
            "- A: You can find the information about partitions/queues on CSC documentation : https://docs.csc.fi/computing/running/batch-job-partitions/. Each queue has its own specific cofigurations. Depending on the your use case, select the one that suits the best.  For example if you need GPUs for your job, there are partitions for GPUs: gpu and gputest. You can also type `sinfo -s` on the commandline terminal on Puhti to see similar information. Just for insights, these partitions are made out of different needs of diverse set of jobs that users would have in HPC environemnt."
        ],
        "source": "csc-enveff-20241003"
    },
    {
        "idx": 266,
        "question": [
            "How often do you have the Part2: Next steps-course?**"
        ],
        "answer": [
            "- A: We TRY to have Part1 and Part2 twice a year, so one round in Oct-Nov and another in April-May. My advice: register for the next round, and repeat in spring if need be :)"
        ],
        "source": "csc-enveff-20241003"
    },
    {
        "idx": 267,
        "question": [
            "I have difficulty pasting my questions into HackMD (here). Do you have some instructions on how to write here?**"
        ],
        "answer": [
            "- A: HackMD can look a bit differend depending on the view you are in. Can you see the three icons on top left corner, next to HackMD text? Thereâ€™s pencil, this side-by-side symbol, and an eye (see below for a screenshot). In eye view, you canâ€™t edit, you are just viewing. The other two reveal the markdown (MD) version of the page, which you can edit. I find it easiest to edit with the side-by-side view, but it can be a bit slow sometimes. First time opening the page, the small Edit (pencil) button might be **on right** next to the table of contents. Please note that it might make things faster if you **switch to view only mode when not editing**!",
            ":::info",
            "The bigger HedgeDoc/HackMD toolbar for switching between modes later on.",
            "The toolbar from which the different modes can be accessed. You can also switch between edit and view mode using hotkeys ctrl-alt-v and ctrl-alt-e respectively",
            "Before editing for the first time, you might find the pencil on the right side:",
            "Screenshot 2024-10-15 at 16.33.15",
            ":::",
            ":::info",
            ":bulb: **Hint:** You can also apply styling from the toolbar at the top :arrow_upper_left: of the editing area.",
            ":::"
        ],
        "source": "csc-enveff-20241107"
    },
    {
        "idx": 268,
        "question": [
            "Slides available after the course?**"
        ],
        "answer": [
            "- A: They sure are! You will also have the access to this HedgeDoc document.",
            "- We encourage you to share and use the material also in your own courses. The material is in git, and we welcome all feedback and edit suggestions (pull requests)!"
        ],
        "source": "csc-enveff-20241107"
    },
    {
        "idx": 269,
        "question": [
            "Has the Zoom meeting already started? I'm just gettin \"The meeting has not started\" -note.**"
        ],
        "answer": [
            "- A: Make sure you have the full Zoom link when connecting! It's looong."
        ],
        "source": "csc-enveff-20241107"
    },
    {
        "idx": 270,
        "question": [
            "Should we be able to access the slides in e-elena? I can only see the first slide.**"
        ],
        "answer": [
            "- A: Try navigating with the arrow keys :)"
        ],
        "source": "csc-enveff-20241107"
    },
    {
        "idx": 271,
        "question": [
            "So many credendials... Which ones do I need and where?!?**"
        ],
        "answer": [
            "- A: You need",
            "    - **CSC credentials** to Puhti/Mahti/Allas",
            "    - **Haka OR Virtu credentials** (=your university/institution credentials) to my.csc.fi and eLena platform",
            "    - contact us if you don't have HAKA/Virtu in use"
        ],
        "source": "csc-enveff-20241107"
    },
    {
        "idx": 272,
        "question": [
            "I have forgotten my CSC username and password**"
        ],
        "answer": [
            "- A: In my.csc.fi you can check your CSC username. If you have forgotten your CSC password, check this link: https://docs.csc.fi/accounts/how-to-change-password/ -Note that it takes some time for the password to update to Puhti, if you need to change that :)"
        ],
        "source": "csc-enveff-20241107"
    },
    {
        "idx": 273,
        "question": [
            "Depending on the place you say to obtain acces to Puhti and Allas or Puhti and Mahti, should we have all three of them (just in case)?**"
        ],
        "answer": [
            "- A: On the course there is one tutorial in which you can try moving data from Puhti to Mahti. It is an optional task so you don't necessarily need Mahti."
        ],
        "source": "csc-enveff-20241107"
    },
    {
        "idx": 274,
        "question": [
            "Do you record the sessions?**"
        ],
        "answer": [
            "- A: No. However, the lecture videos, slides and hands-ons are available in the github page, and all the material + the quizzes etc can be accessed in eLena platform, so this course works well as self-study also!"
        ],
        "source": "csc-enveff-20241107"
    },
    {
        "idx": 275,
        "question": [
            "Why are there so many different sites for the same information?**"
        ],
        "answer": [
            "- A: We are using the same material in several ways.",
            "- If you just wish to participate in the live online course, it's easiest to follow the schedule here (above) and the slides and exercises in the github page. This material is also used by our colleagues and collaborators, and github provides a nice way to suggest edits!",
            "- If you wish to test your learning and perhaps get a course certificate, do also the quizzes in eLena platform, where you can also download the certificate. The e-Lena platform is also used when taking the self-learning course.",
            "- This HackMD document is used for all discussion related to this specific live course. It also allows us to scale up, as multiple questions can be asked and answered simultaneosly, and everyone can learn from them. Some people also find it easier to ask questions anonymously in text format, and we of course want to make that possible!"
        ],
        "source": "csc-enveff-20241107"
    },
    {
        "idx": 276,
        "question": [
            "Maybe not related to this particular training, but which are the best ways to transfer files to and from Allas? Prefereably easy ways, since not all the users are \"computer-persons\".**"
        ],
        "answer": [
            "- A: The easiest way for non \"computer savvy\" persons is probably to use the graphical user interface in the HPC web interfaces we offer, e.g. www.puhti.csc.fi. See https://docs.csc.fi/computing/webinterface/file-browser/",
            "- A: On the command-line, using the a-tools is easier to use than e.g. rclone. See https://docs.csc.fi/data/Allas/using_allas/a_commands/",
            "- [x] **I have trouble finding these first slides from linked website.**",
            "- A: They are all linked here https://csc-training.github.io/csc-env-eff/part-2/data-io/",
            "- [x] **I use Puhti. Data size is 1GB. How should I input that data for efficient GPU usage? Local scratch, /scratch, allas, what else to take into notice?**",
            "- A: Data to be used in computing workloads should be transferred first to /scratch. Then depending on the nature of the data and workload (a lot of small files or few big ones, a lot of IO being done or not) it is ok to use /scratch or then local scratch. So short answer it depends."
        ],
        "source": "csc-enveff-20241107"
    },
    {
        "idx": 277,
        "question": [
            "What is the practical difference of using a-conmand, rclone and s3cmd?**"
        ],
        "answer": [
            "  - A: a-command and rclone can be used in swift and s3 protocols. S3cmd command uses s3 protocol. a-commands are bit safer to use and will not overrides the objects in allas. Rclone may not ask before overriding it. Please read more about different allas clients here: https://docs.csc.fi/data/Allas/using_allas/"
        ],
        "source": "csc-enveff-20241107"
    },
    {
        "idx": 278,
        "question": [
            "If I were to use th `$LOCAL_SCRATCH` in my computing nodes, what is actually more efficent, transfering from scratch or from allas object?**"
        ],
        "answer": [
            "- A: If the data is available in both, copying from `/scratch` is faster. If the data is in Allas only it's best to copy it directly to `$LOCAL_SCRATCH`."
        ],
        "source": "csc-enveff-20241107"
    },
    {
        "idx": 279,
        "question": [
            "I have recently used allas-backup. Some time ago I create a backup of one folder. Yesterday, I used allas-backup folder again, but it seems it does the entire backup again. What is the command to do the incremental copy?**"
        ],
        "answer": [
            "- A: allas-backup automatically creates a project-specific backup repository in the Allas storage service at CSC and uses it for *cumulative* backups. -- different versions of a dataset can be stored so that in the case of a new dataset version, only the changes copared to the previous version needs to be stored. https://docs.csc.fi/data/Allas/using_allas/a_backup/",
            "- More on how that happens: https://restic.readthedocs.io/en/stable/040_backup.html#file-change-detection"
        ],
        "source": "csc-enveff-20241107"
    },
    {
        "idx": 280,
        "question": [
            "Can allas-backup be used from ePouta and cPouta?**"
        ],
        "answer": [
            "- A: I am not sure about ePouta, but cPouta works as long as you install the allas-cli-utils developed by CSC in your VM in cPouta. The repo is here: https://github.com/CSCfi/allas-cli-utils/tree/master and installation instructions: https://github.com/CSCfi/allas-cli-utils/tree/master?tab=readme-ov-file#installing-allas-cli-utils",
            "- A: Comment: ePouta instances live in a separate network and may not have access to allas which is on the public network. As epouta service is a self-service model, you can obviously relax the firewalls or netwroks at your own risk. CSC has no control on it. In recent SD environment (SD Desktop), you can only access encrypted data on allas."
        ],
        "source": "csc-enveff-20241107"
    },
    {
        "idx": 281,
        "question": [
            "Can tha Allas commands be used in the exact same way from Mahti as from Puhti?**"
        ],
        "answer": [
            "- A: Yes. One can use a-commands in the same way, both on Mahti and Puhti."
        ],
        "source": "csc-enveff-20241107"
    },
    {
        "idx": 282,
        "question": [
            "As a general question, what is the difference between wclean libso / wmake libso and wclean / wmake ?**"
        ],
        "answer": [
            "- A: I guess this is some custom maketool for OpenFOAM? Seems like it is a wrapper script based on regular `make`. I am not an expert on OpenFOAM, but there is documentation available here: https://doc.cfd.direct/openfoam/user-guide-v12/compiling-applications",
            "- A: From the docs: \"The wmake script is generally executed by typing `wmake <optionalDirectory>`. The <optionalDirectory> is the directory path of the application that is being compiled. Typically, wmake is executed from within the directory of the application being compiled, in which case <optionalDirectory> can be omitted.\"",
            "- A: Furthermore: \"wmake builds a dependency list ï¬le with a .dep ï¬le extension, e.g. newApp.C.dep in our example, in a $WM_OPTIONS sub-directory of the Make directory, e.g. Make/linuxGccDPInt64Opt. If the user wishes to remove these ï¬les, e.g. after making code changes, the user can run the wclean script by typing: `wclean <optionalDirectory>`. Again, the <optionalDirectory> is a path to the directory of the application that is being compiled. Typically, wclean is executed from within the directory of the application, in which case the path can be omitted.\"",
            "- A: Note that OpenFOAM is already available on Puhti, Mahti and LUMI, so you do not need to install it yourself unless you specifically want to. See https://docs.csc.fi/apps/openfoam/"
        ],
        "source": "csc-enveff-20241107"
    },
    {
        "idx": 283,
        "question": [
            "`conda-containerize new --prefix /projappl/<project>/$USER/tykky-env env.yml  # replace <project> with your CSC project, e.g. project_2001234"
        ],
        "answer": [
            "` should this be deleted after?**",
            "- A: Yes. you can delete the directory. It will not be needed on the course."
        ],
        "source": "csc-enveff-20241107"
    },
    {
        "idx": 284,
        "question": [
            "Do we need to add the path of our tykky container to the environmental variable path only once?**"
        ],
        "answer": [
            "- A: The `export` command will only be effective for the current session. If you for example log out and log back in, all changes will be lost. If you wish to make some change permanent, you can e.g. add the `export` comamnd to your `$HOME/.bashrc` file. It should be noted that doing so may cause conflicts with other software, so it's not generally recommended. This is especially true for Tykky installed software. It's best to have only one such software in your `$PATH` at any one time, as they often conflict with each other. So only add them to your `$PATH` when using them. You can e.g. add the `export` command to your batch job script."
        ],
        "source": "csc-enveff-20241107"
    },
    {
        "idx": 285,
        "question": [
            "Im interested to get my some of my codes from puhti to be stored in private gitlab repo. What should I do? I am familiar with ssh keys but I am not sure how it goes between supercomputer and gitlab?**"
        ],
        "answer": [
            "- A: Git is available on Puhti, so pushing code to a private gitlab repo should be no issue. To use SSH keys to authenticate, you should create a key pair on Puhti and register the public key in your gitlab. Please remember to keep your private key safe (don't change its permissions, it should be `-rw-------`, i.e. `600` in numeric form)!",
            "- Note: git is available directly in the login nodes. If working in a compute node (e.g. interactive session) you need to load it as a module `module load git`."
        ],
        "source": "csc-enveff-20241107"
    },
    {
        "idx": 286,
        "question": [
            "Having a \"database\" consisting of 100k directories, each containing about 10 files, what would be the recommended way of storing these?**"
        ],
        "answer": [
            "- A: If you are working with flatfiles in database and the need for queries is very less, you might use it under Lustre (generally not advised). if the need grows, the better approach would be is to use $LOCAL_SCRATCH. Use of database in $LOCAL_SCRATCH will not cause any issues on parallel file system. It just requires that you have to stage and unstage it from local_scratch, otherwise database will be lost. However, if there is a great need for querying those files from database and you need this database more frequently, the recommendation is to use this database on cPouta/Rahti and query it from there.",
            "- Follow-up Q: Do I understand correctly, during use, I stage the data to $LOCAL_SCRATCH and access it from there. When not in use, I keep it stored on Mahti/Puhti normal scratch?",
            "- A: Yes. you can store in normal scratch if you are activley using it. Otherwise, it makes sense to move the database to ALLAS"
        ],
        "source": "csc-enveff-20241107"
    },
    {
        "idx": 287,
        "question": [
            "Would you recommend using Apptainer (vs docker) also in cPouta/ePouta?**"
        ],
        "answer": [
            "- A: Both work fine in cPouta/ePouta environment. If the VM instance is shared by many users, I would go with Apptainers. Just FYI, Apptainer is more suitable for scientific applications on HPC systems. Docker is more for microservices in more isolated environments."
        ],
        "source": "csc-enveff-20241107"
    },
    {
        "idx": 288,
        "question": [
            "Are there any geographic restrictions in accessing CSC services? In other words can I work with all of these services from abroad?"
        ],
        "answer": [
            "- A: Generally, there are no restrictions as such. But there can be some restrictions due to some state regulations from time to time (https://csc.fi/en/news/csc-prohibits-the-use-of-its-services-based-on-the-regulations-of-the-european-union/). Also note that CSC does lot of  services of which some of them may be restricted (sensitive data or similar services) worldwide  for security reasons (e.g., https://docs.csc.fi/cloud/rahti/security-guide/)."
        ],
        "source": "csc-enveff-20241107"
    },
    {
        "idx": 289,
        "question": [
            "\"Real LUMI-G node\" slide presented the CPU with two threads per core. I assume it is just to give full insight, what the CPU allows, but you are not using hyper-threading, right?"
        ],
        "answer": [
            "**Answer:** SMT is activated but not used by default excepted if you ask for it in your job script with `--hint=multithread`. The same applies to LUMI-C nodes."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 290,
        "question": [
            "Is there a way to reload all sticky modules (with one command), if you first have unloaded all sticky modules."
        ],
        "answer": [
            "**Answer:** If you have \"force purged\" all modules, you can get the default environment with `module restore` but it will not reload non-default sticky modules you may have loaded previously."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 291,
        "question": [
            "Is \"lumi-container-wrapper\" related to https://cotainr.readthedocs.io/en/latest/ by DeiC from Denmark?"
        ],
        "answer": [
            "**Answer:**",
            "-   No. It is a different tool. It's the LUMI version of the tykky tool available on the CSC Puhti and Mahti clusters.",
            "-   The cotainr tool from DeiC is also available though (see the LUMI Software Library page) but we're waiting for them to test it on newer versions of the Cray PE."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 292,
        "question": [
            "What does RPM stand for?"
        ],
        "answer": [
            "**Answer:** RPM stands for Redhat Package Manager. It is a popular tool for distributing Linux software as binaries for direct installation in the OS image."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 293,
        "question": [
            "If I installed a software and prepared a module file by myself, is there a place I can contribute my module to the LUMI user community? Maybe it can save time when a new LUMI user is struggling installing the same exact software."
        ],
        "answer": [
            "**Answer:** Yes, of course. We have a GitHub repository for that. https://github.com/Lumi-supercomputer/LUMI-EasyBuild-contrib/",
            "    Just create a pull request there and we will have a look and merge it"
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 294,
        "question": [
            "What options --try-toolchain-version=22.08 --robot help us with?"
        ],
        "answer": [
            "**Answer:** These options should work as in the regular EasyBuild. (and the `-r` I used is the abbreviation of `--robot`)",
            "In fact, because I want a reproducible build environment most EasyConfigs that we offer also use the buildtools module, but as the version I use the `toolchain_version` variable so that that dependency is automatically adapted also. We bundle several build tools not only to have less modules, but also to have to adapt fewer dependency versions..."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 295,
        "question": [
            "Which changes were made in `lumi-container-wrapper` compared to `tykky`?"
        ],
        "answer": [
            "**Answer:** It uses a different base container better suited for LUMI and for Python it is configure to build upon cray-python."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 296,
        "question": [
            "For how long will this project (project_465000522) be available?"
        ],
        "answer": [
            "**Answer:** It will stay active for the  next two days (terminates on May, 11th)."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 297,
        "question": [
            "I have installed cp2k with easybuil, but I'm not able to run any programs, because it stops running in between and error file shows Segmentation fault - invalid memory reference? This is a problem I am not figuring out how to solve. Any help regarding this."
        ],
        "answer": [
            "**Answer:** Too few details to say a lot. You may have made errors during installation though most of those errors would lead to different errors. However, you may have hit a bug in CP2K also, or a bug in the PRogramming Environment. We have several users who have successfully used our recipes, but they may have been trying different things with CP2K. Our configuration for CP2K (at least the CPU version) is also based on the configuration used on a similar Cray system at CSCS which is one of the machines used by the CP2K developers...",
            "This is also more the type of problem that is better handled through a ticket though it is unclear from the description if we can do a lot about it."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 298,
        "question": [
            "Lmod shows \"LUMI/23.03  partition/D\" as the only available pre-requisite with LUMI/23.03 for the VNC module. Yet, despite this I am able to:"
        ],
        "answer": [
            "    module load LUMI/23.03  partition/L",
            "    module load lumi-vnc",
            "Does this mean that Lmod is just not able to show all of the possible pre-requisite combinations? Or will `lumi-vnc` turn out not to work correctly with `partition/L`?",
            "**Answer:** `partition/D` should not even be shown, that is a bug in the software installation that I need to correct.",
            "But a nice find. The output is confusing because LMOD gets confused by some hidden software stacks and partitions. It actually also shows a line `init-lumi/0.2` which is a module that you have loaded automatically at login. And this in turns means that having this module is enough to use `lumi-vnc`, i.e., it works everywhere even without any `LUMI` stack or `CrayEnv`."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 299,
        "question": [
            "In general, if I set export EBU_USER_PREFIX for project directory, do I also need to set another one for the scratch?"
        ],
        "answer": [
            "**Answer:** No, it points to the root of the software installation which is one clear place. The reason to install in `.project` and not in `/scratch` is that you want the software installation to be available for your whole project. If you check the disk use policies in the storage section of the docs you'd see that data on `/scratch` is not meant to be there for the whole project but can be erased after 90 days which is not what you want with your software installation."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 300,
        "question": [
            "Is it possible to get an exemption from the the maximum 2 day allocation?"
        ],
        "answer": [
            "**Answer:** No. No exceptions are made at all. We don't want nodes to be monopolized by a user and it makes maintenance more difficult. Moreover, given the intrinsic instability of large clusters it is essential that jobs use codes that store intermediate states from which can be restarted.",
            "    Users have been using dependent jobs with success to automatically start the follow-on job after the previous one ends."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 301,
        "question": [
            "Is it not possible to use the `singularity build` command?"
        ],
        "answer": [
            "**Answer:** Not all options of `singularity build` work. Any build requiring fakeroot will fail as that is disabled due to security concerns."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 302,
        "question": [
            "Does this (binding tasks to resrouces) mean that it will be necessary to use custom bindings to align tasks with specific NUMA domains on each CPU? (Since NUMA domains seem to be a level in-between cores and sockets)"
        ],
        "answer": [
            " **Answer:** If you are using all cores on an exclusive node the standard ways in which Slurm distributes processes and threads may do just what you want.",
            "Even if, e.g., it would turn out that it is better to use only 75% of the cores and you would be using 16 MPI processes with 6 threads per process, then a creative solution is to ask Slurm for 8 cores per task and then set `OMP_NUM_THREADS=6` to only start 6 threads. There are often creative solutions.",
            "Some clusters will redefine the socket to coincide with the NUMA domain but it looks like this is not done on LUMI."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 303,
        "question": [
            "Where do we look up the specific NUMA domain / GPU correspondence? In the LUMI documentation? Or perhaps by a command in LUMI?"
        ],
        "answer": [
            "**Answer:**",
            "-   See this image from the LUMI-G hardware documentation page. It presents the numbering from the point of view of the GPUs and CPU.",
            "-   See also `rocm-smi --showtoponuma` (on a LUMI-G node)"
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 304,
        "question": [
            "If we enable hardware threads for a job/allocation, does \"--cpus-per-task\" become HW threads?"
        ],
        "answer": [
            "**Answer:** Yes:",
            "```",
            "# 2 HWT per core, all cores allocated",
            "$ srun -pstandard --cpus-per-task=256 --hint=multithread --pty bash -c 'taskset -c -p $$'",
            "pid 161159's current affinity list: 0-255",
            "# 2 HWT per core but only the first 64 cores allocated",
            "$ srun -pstandard --cpus-per-task=128 --hint=multithread --pty bash -c 'taskset -c -p $$'",
            "pid 161411's current affinity list: 0-63,128-191",
            "```"
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 305,
        "question": [
            "Is it possible to make sure a job requesting 16 cores is allocated all cores in one NUMA domain?"
        ],
        "answer": [
            "**Answer:**",
            "-   Is your question for sub-node allocations (small and small-g partitions)?",
            "    -   Yes, the question is not relevant for production runs, it was only out of interest. It is something to be aware of during scaling tests for example.",
            "        -   Our advise to users in our local compute centre who have to do scaling tests to submit a proposal for time on LUMI (or our Tier-1 systems) is to use exclusive nodes to avoid surprises and reduce randomness. The most objective way is probably if you want to do a test on 16 nodes to run 8 such tests next to one another to fill up the node. Because there is another issue also. I haven't tried if the options to fix the clock speed from Slurm work on LUMI, but depending on the other work that is going on in a socket the clock speed of the cores may vary.",
            "-   I doubt there is a way to do that for the sub-node allocation partitions. I can't find one that works at the moment. Binding really only works well on job-exclusive nodes. For me this is a shortcomming of Slurm as it doesn't have enough levels in its hierarchy for modern clusters.",
            "-    For a sub-node allocation, you will get random cores depending on which cores are available:",
            "```",
            "$ srun -psmall --cpus-per-task=16 --pty bash -c 'taskset -c -p $$'",
            "pid 46818's current affinity list: 44-46,50,52-63",
            "$ srun -pstandard --cpus-per-task=16 --pty bash -c 'taskset -c -p $$'",
            "pid 220496's current affinity list: 0-15",
            "```"
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 306,
        "question": [
            "To access /scratch/ from a container, we have to mount it. However, we need the full path and not just the symlink. Where do we find the full path?"
        ],
        "answer": [
            "**Answer:** You can use `file /scratch/project_465000522` for example. Don't try to mount the whole scratch. That will not work. The `project_*` subdirectories in `/scratch` are distributed across the 4 file systems of LUMI-P. `ls -l /scratch/project_465000522` will actually show you which file system is serving that project's scratch directory."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 307,
        "question": [
            "The documentation page states that \"Automatic cleaning of project scratch and fast storage is not active at the moment\". Is this still true, and will the users be informed if this changes?"
        ],
        "answer": [
            "**Answer:**",
            "-   This is still true today and usually users are informed about changes but with short notice. Quota were also disabled for a while due to problems after an upgrade last July but solving those problems became a higher priority when abuse was noticed, and there was only 14 days notice. So abusing scratch and flash for long-term storage is asking for increasing the priority of that part of the LUMI setup working... I'd say, don't count on it as the message may arrive as well when you are on a holiday.",
            "-   For slightly longer time storage but still limited to the lifetime of your project there is also the object storage. However, at the moment only rather basic tools to use that storage are already available."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 308,
        "question": [
            "What is the preferred way for transferring data between LUMI and some external server or other supercomputer e.g. CSC Mahti?"
        ],
        "answer": [
            "**Answer:**",
            "-   Mahti is so close to LUMI (as far as I know even in the same data centre but a different hall) that connection latency should not limit bandwidth so that you can just use sftp.",
            "    I believe the CSC documentation also contains information on how to access the allas object storage from LUMI. Using allas as intermediate system is also an option. Or the LUMI-O storage but at the moment allas is both more developed and better documented. (For readers: allas is a CSC object system only available to users that are CSC clients and not to other users of LUMI.)",
            "-   For supercomputers that are \"farther away\" from LUMI where bandwidth is a problem when using sftp, it looks like the LUMI-O object storage is a solution as the tools that read from the object storage use so-called \"multi-stream transport\" so that they can better deal with connections with a high latency. The documentation on how to access the LUMI object storage from elsewhere needs work though."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 309,
        "question": [
            "The slides say the GPUs have 128 GB mem, but when I queried the information with the HIP-framework it only returned 64 GB mem. Does it differ on different partitions or something?"
        ],
        "answer": [
            "-   Kurt will go into more detail soon. But each GPU has 128GB but each GPU conists of 2 dies (basically independent GPUs). Each of those has 64GB. Basically each LUMI-G node has in practise 8 GPUs on 4 cards."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 310,
        "question": [
            "This is maybe a better question for when you will discuss software, but seeing the AMD hardware, my question is how compatible the GPU partitions/software are with DL frameworks such as Tensorflow and PyTorch. Are the systems fully ROCm compatible? Are there downsides compared to CUDA implementations?"
        ],
        "answer": [
            "-   Let's discuss that later in detail. But short answer: ROCm is not as mature as CUDA yet but most DL frameworks work already quite well."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 311,
        "question": [
            "I believe the GPU nodes have 64 cores. But from the slides I understood that the nodes have 1 CPU with 8 cores."
        ],
        "answer": [
            "Just as a note: this is output from hip:",
            "```",
            "  Device 0:",
            "  Total Global Memory: 63.9844 GB",
            "  Compute Capability: 9.0",
            "  Max Threads per Block: 1024",
            "  Multiprocessor Count: 110",
            "  Max Threads per Multiprocessor: 2048",
            "  Clock Rate: 1700 MHz",
            "  Memory Clock Rate: 1600 MHz",
            "  Memory Bus Width: 4096 bits",
            "  L2 Cache Size: 8 MB",
            "  Total Constant Memory: 2048 MB",
            "  Warp Size: 64",
            "  Concurrent Kernels: Yes",
            "  ECC Enabled: No",
            "  Unified Memory: No",
            "```",
            "-   The slide was about the CPUs in the node. The basic element of the GPU is the \"Compute Unit\" (CU)",
            "    (the \"multiprocessor count\" in the above output).",
            "    And one CU has 4 16-wide SIMD units and 4 matrix core units.",
            "    AMD doesn't use the word core very often in the context of GPU as what NVIDIA calls a core is",
            "    actually called an Arithmetic and Logical Unit in a CPU and is only a part of a core."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 312,
        "question": [
            "what is called underneath with compiling with `hipcc`? rocm compiler I assume? - @bcsj"
        ],
        "answer": [
            "-   Actually it is AMD's branch of Clang.",
            "-   @bcsj: sidenote, I've had trouble with compiling GPU code with the `CC` compiler, which I assume calls something else underneath. The code would run, but in a profiler it showed that a lot of memory was being allocated when it shouldn't. `hipcc` compiling fixed this issue.",
            "    -   I believe `CC` is using different Clang frontend being Cray's branch of Clang with a slightly different codebase. At the end it should use the same ROCm backend. It may require more debugging to understand the problem.",
            "    -   @bcsj: possibly ... it was some very simple kernels though, so I'm pretty confident it was not a kernel-issue. The profiling was done using APEX and rocmprofiler. Kevin Huck helped me, during the TAU/APEX course.",
            "    -   The frontend that `CC` is using depends on which modules you have loaded. IF the `cce` compiler module is loaded, it will use the Cray clang frontend while if the `amd` compiler module is loaded it will use the AMD ROCm C++ compiler.",
            "-   @mszpindler Could you please post a support ticket than we can investigate memory issues, thanks.",
            "-   @bcsj: It's been a while and I restructured my environment to avoid the issue, but I'll see if I can find the setup I used before."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 313,
        "question": [
            "What is the policy on changes to these centrally installed and supported modules? Are versions guaranteed to be available for a certain period of time?"
        ],
        "answer": [
            "-   Unfortunately not at the moment. WE hope to be able to provide LTS (Long term support) versions in the future but they are not yet vendor supported. But we will always inform you about changes to the SW stack."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 314,
        "question": [
            "Are open source simulation software such as quantum espresso centrally installed on LUMI?"
        ],
        "answer": [
            "-   Please use the Software Library https://lumi-supercomputer.github.io/LUMI-EasyBuild-docs/q/QuantumESPRESSO/ to understand what the policy is.",
            "-   Short answer: No, almost no softare (except debugger, profilers) is installed globally. But you will see soon, that it is very easy to install SW yourself with Easybuild using our easyconfigs (basically building recipes)."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 315,
        "question": [
            "The program I am using and developing requires certain dependencies such as paralution, PETSc, Boost... Is it possible to manually install these dependencies if not available among the modules listed?"
        ],
        "answer": [
            "-   Yes, it is actually very easy and will be the topic of the next session (and we have some configurations on Boost on the system, try `module spider Boost`...)",
            "-   We have an installation recipe for one configuration of PETSc also. And PETSc is one of those libraries that does strange things behind the scenes that can cause it to stop working after a system update... We have not tried paralution. But basically there are an estimated 40,000 scientific software packages not counting the 300,000+ packages on PyPi, R packages, etc., so there is no way any system can support them all.",
            "-   @mszpindler Please try PETSc with recipes from https://lumi-supercomputer.github.io/LUMI-EasyBuild-docs/p/PETSc/ If you see wrong behaviour then please open support ticket."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 316,
        "question": [
            "Can I use modules that are available on CSC supercomputers?"
        ],
        "answer": [
            "-   Basically we use different set of modules and even a different primary system to manage the modules. The machines are different, the team managing the machine is different, and the licenses for software on puhti or mahti do not always allow use on LUMI, and certainly not for all users on LUMI, while we have no way to control who has access and who has not."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 317,
        "question": [
            "If i find out that my application is able to work in different programming environments (different compilers), which should I prefer? Cray?"
        ],
        "answer": [
            "-   No answer that is always right, you have to benchmark to know.",
            "    And even for packages for which we offer build recipes we cannot do that benchmarking as it requires domain knowledge to develop proper benchmarks. And the answer may even differe on the test case.",
            "    -   Ok, test with Cray first..",
            "-   Again, if you look at the Software Library https://lumi-supercomputer.github.io/LUMI-EasyBuild-docs/q/QuantumESPRESSO/ then you can see recipe (easyconfigs) versions for specific programmming environments. Those listed there, are supported."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 318,
        "question": [
            "In our own tier-1 I've always used virtual envs successfully. The reason that I do not rely on containers is that I often work on editable installs that I pull from git. Looking at the documentation (https://docs.lumi-supercomputer.eu/software/installing/container-wrapper/#plain-pip-installations) it seems that this is not supported. In other words: I would like to be able to create an environment, install an editable Python git repo, and when an updated comes to the remote repo, just pull it and keep using the env. If I understand correctly I would need to rebuild the container after every `git pull` on LUMI?"
        ],
        "answer": [
            "- I  t is actually CSC's development called Tykky https://docs.csc.fi/computing/containers/tykky/ and I believe you can try with `pip-containerize update ...` to update existing image but you need to try if it works with your specific environment.",
            "-   Something really difficult: You can spread a Python installation over multiple directories. So you could put all dependencies of your software in a container and mount a directory from outside in which you install the software you're developing. That would already reduce the load of small packages on the file system.",
            "    The speed difference can be dramatic. On the installation side I've worked on 5 or 6 big clusters on Europe. On the slowest it took me 2 hours to install a bunch of Python packages that installed in under 30 seconds on the SSD of my laptop... Measuring the speed difference while running is more difficult as I couldn't run the benchmarks on my PC and as there are other factors in play also. But CSC reports that a 30% speedup is not that uncommon from using Tykky.",
            "-   I have successfully installed Python repos as editable, by setting PYTHONUSERBASE to point to the project directory and then using both --user and --editable flags with pip.",
            "-   There is some information on containers also in the \"Additional Softwre on LUMI\" talks in our 4-day course. The latest for which notes and a recording are available is the presentation during the February 2023 course."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 319,
        "question": [
            "Sorry, probably you are going to speak about containers later. I'm just interested in the opportunity to use one particular commercial software from within, say, Singularity container on LUMI. So, if I sort out the license server issues, is it possible for virtually any kind of software or there are limitations? If yes, is it, overall, a good idea? Does it make the performance of the software run deteriorate?"
        ],
        "answer": [
            "-   Unless there is MPI involved you can try to run any, say DockerHub, container image on LUMI. For the MPI parallel software packages it may still work on a single node but do not expect them to run on multiple computing nodes.",
            "-   There is some information on containers also in the \"Additional Softwre on LUMI\" talks in our 4-day course. The latest for which notes and a recording are available is the presentation during the February 2023 course."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 320,
        "question": [
            "Looking at the Exercise 1.1 solution, I don't really see any difference between the outputs from `module spider Bison` and `module spider Bison/3.8.2`, but from how the solution reads, it seems like they shouldn't be the same?"
        ],
        "answer": [
            "-   That's because there were two different versions on the system when I prepared the exercise. It found a version generated with Spack also and it looks like the person managing that stack disabled that module again. If `module spider` finds only a single version it produces the output it would produce when called with the full version as the argument.",
            "    I wanted to use it as an example to show you how you can quickly see from the `module spider` output if a program comes from the LUMI stacks or from Spack...",
            "    -   ok, thanks",
            "-   You can see the different behaviours with for example `module spider buildtools`"
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 321,
        "question": [
            "is there a module similar to `cray-libsci` for loading/linking `hipBlas`? So far I've been doing that manually. I tried to `module spider hipblas` but that returns nothing."
        ],
        "answer": [
            "-   No, there is not. AMD doesn't polish its environment as much HPE Cray.",
            "    Also, all of ROCm is in a single module and as several of those modules are provided by HPE and AMD they don't contain the necessary information to search for components with `module spider`."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 322,
        "question": [
            "Is there a similar tool to `csc-projects` for checking info on your projects?"
        ],
        "answer": [
            "-   Yes, please try `lumi-allocations` command-line tool."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 323,
        "question": [
            "Perhaps I missed this earlier, but what is LUMI-D?"
        ],
        "answer": [
            "-   It is the name that was used for the \"Data Analytics and Visualisation partition\" which basically turns out to be two partitions that need a different setup.",
            "    It consists of 8 nodes with 4 TB of memory that really have the same architecture as the login nodes hence can use software of what we will later in the course call `partition/L`, and 8 nodes with 8 NVIDIA A30 GPUs for visualisation each."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 324,
        "question": [
            "Would it be possible to show an example of how to run R interactively on a compute node?"
        ],
        "answer": [
            "-   If you don't need X11, one way is",
            "    ```",
            "    srun -n 1 -c 1 -t 1:00:00 -p small -A project_465000XXX --pty bash",
            "    module load cray-R",
            "    R",
            "    ```",
            "    which would ask for 1 core for 1 hour. For some shared memory parallel processing you'd use a larger value for c but you may have to set environment variables to tell R to use more threads."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 325,
        "question": [
            "Is Rstudio server exist as a module on LUMI? Is it planned to be added at some point? Is it possible for normal user to install it?"
        ],
        "answer": [
            "-   We expect to be able to offer Rstudio with Open OnDemand, but no time set for now. But as any batch system, LUMI is not that well suited for interactive work as there is no guarantee that your interactive session will start immediately. Some resources will be set aside for Open OnDemand and I assume it will be possible to oversubscribe some resources. But LUST is not currently involved with the setup so there is not much we can say."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 326,
        "question": [
            "May be a beginner question.. When to work with GPU nodes and when it is less efficient to use it? Does it depend on the structure of the data or the software used? Is it possible to use GPU nodes with R scripts?"
        ],
        "answer": [
            "-   GPU comute requires software that explicitly supports compute on GPU. GPUs are never used automatically. Base R does not use GPU compute. Some R packages may as in principle some large linear algebra operations that are used in statistics may benefit from GPU acceleration. However, they will have to support AMD GPUs and not NVIDIA GPUs.",
            "    There is no simple answer when GPU compute offers benefits and when it does not as it depends a lot on the software that is being used also."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 327,
        "question": [
            "How to monitor the progress / resources use (e.g. how much RAM / # cores are actually used) of currently running and finished batch jobs?"
        ],
        "answer": [
            "-   Slurm `sstat` command can give information on running jobs. Once the job has finished, `sacct` can be used. Both commands have very customizable output but they will not tell you core per core how that core was used. Then you need to do active profiling.",
            "-   You can also attach an interactive job step to a running job (as shown on slide 9). I'm not sure `rocm-smi` works reliably in that case (it didn't always in the past), but commands like `top` and `htop` (the latter provided by the `systools` module) should work to monitor the CPU use."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 328,
        "question": [
            "Do job-arrays open program the number of processes which we specified with --ntasks=1? or it will open 16 independent jobs with 1 processes for each job?"
        ],
        "answer": [
            "-   A job array with 16 elements in the job array is 16 jobs for the scheduler if that is what you mean. If you want multiple processes in one job you'd submit a single job for the combined resources of all 16 jobs and then use srun to start 16 processes in that job.",
            "What happens if we choose job-array=1-16 and --ntasks=2? It will use 16 jobs and each jobs has 2 tasks, right?",
            "-   Yes. After all some people may want to use a job array for management where each element of the job array is an MPI program."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 329,
        "question": [
            "Is the limit \"total jobs in the queue\" or \"total concurrntly running\"?"
        ],
        "answer": [
            "-   There are two different limits that are both shown in the LUMI documentation. There is a limit on the number of jobs running concurrently and a limit on the number of jobs in the queue (which includes running jobs). That limit is also very low. LUMI is a very large cluster and the total number of jobs that a scheduler can handle of all users together is limited. Schedulers don't scale well. Which is why the limit on the number of jobs is considerably lower than it typically is on small clusters.",
            "    The right way to deal with parallelism on big systems is using a hierarchy, and this holds for job management also: The scheduler to allocate bigger chunks of the machine and then run another tool to create parallelism in the job."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 330,
        "question": [
            "Out of curiousity, how did the GPU-id and NUMA-id misalignment occur? It somehow makes me surprised that/(if?) it is consistently wired in this same \"weird\" manner all over the nodes."
        ],
        "answer": [
            "-   Luckily it is consistent. I'm pretty sure it is basically a result of the motherboard design. The connections between the GPUs determine which GPU considers itself GPU 0 during the boot procedure, while the connections between the GPU socket and CPU socket determine the mapping between CCDs and GPU dies.",
            "    -   Interesting, thanks for the answer!"
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 331,
        "question": [
            "Regarding the containers and MPI... If I want to run my software on multiple nodes from within a Singularity container with decent scalability, should I use host (LUMI's) MPI, right? Then, which MPI should I make this software work with? Open MPI, Intel MPI - don't work on LUMI, right? What should I aim at then?"
        ],
        "answer": [
            "-   Open MPI doesn't work very well at the moment. Some people got enough performance from it though for CPU codes and I worked on a recent ticket where it worked well for a user within a node.",
            "    HPE Cray is also working on a translation layer from Open MPI 4.1 to Cray MPICH.",
            "-   We know that Intel internally must have an MPI that is compatible with LUMI as they are building the software environment for the USA Aurora supercomputer that uses the same interconnect as LUMI. But so far experiments with the versions distributed with oneAPI etc. have been a disappointment. It might be possible to try to force the application compiled with Intel MPI to use Cray MPICH as they should have the same binary interface.",
            "-   But the MPI on LUMI is basically compatible with the ABI from MPICH 3.4.",
            "-   The main advise it though to avoid containers and software that comes as binaries when you want to use MPI. It is often a pain to get the software to work properly. Containers are good for some level of portability between sufficiently similar machines with a close enough OS kernel, same hardware and same kernel modules, and were never meant to be portable in all cases. They work very well in, e.g., a cluster management environment (And they are used on the LUMI management nodes) but then you know that the containers will be moving between identical hardware (or very similar hardware if the vendor provides them ready-to-run)."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 332,
        "question": [
            "Is it possible to list all nodes with their resource occupation? I want to see how many GPUs / memory is available on different nodes."
        ],
        "answer": [
            "-   All GPU nodes are identical. There is only one node type. And the majority of the GPU nodes is job exclusive so another job could not even start on them.",
            "    -   So if my application utilizes only 1 GPU, it will still hold the whole node with all GPUs?",
            "-   It depends on what partition you use. If you use `standard-g`, yes and you will pay for the full node. On `small-g` you are billed based on a combination of memory use, core use and GPU use (as if, e.g., you ask for half the CPU memory of a node you basically make it impossible for others to efficiently use half of the GPUs and half of the CPU cores, so you would be billed for half a node).",
            "    But it makes no sense to try to be cleverer than the scheduler and think that \"look, there are two GPUs free on that node so if I now submit a job that requires only 2 GPUs it will run immediately\" as the scheduler may already have reserved those resources for another job for which it is gathering enough GPUs or nodes to run."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 333,
        "question": [
            "Is it possible for us as users to see the current status of LUMI nodes (e.g. using dashboard or a command line)? I mean how many nodes are used or available to work? How many jobs are currently queuing (including other users). I just need to know what would be the expected waiting time for running my jobs."
        ],
        "answer": [
            "-   `sinfo` gives some information but that tells nothing about the queueing time for a job. Any command that gives such information is basically a random number generator. Other jobs can end sooner making resources available earlier, or other users with higher priority jobs may enter the queue and push your job further back."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 334,
        "question": [
            "Are nodes assigned exclusively for projects? I mean, If I am submitting a job for a particular project, would I have access to most of the resources or for specific nodes? Is there a quota per project and how to know how much of this quotaa is used?"
        ],
        "answer": [
            "-   Nodes are either shared among jobs or exclusively assigned to jobs depending on the partition.",
            "-   Each job is also attached to a project for billing purposes. Your resource allocator should have given information about that and billing is documented well in the LUMI documentation. Maciej showed the command to check how much you have consumed (`lumi-allocations`).",
            "-   And each job also runs as a user which determines what you can access on the system."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 335,
        "question": [
            "How priority is determined? Does submitting more jobs or jobs that consume time or memory results in lower priority? Does priority is determined per user or project?"
        ],
        "answer": [
            "-   We don't have precise details and it would make no sense either as it is a complicated formula that can be adjusted if the need arises to ensure fair use of LUMI. Priority is a property of a job, not of a project or of a user, but it can be influenced by factors that are user- or project-dependent, like fair use which is actually difficult on LUMI due to the different size of allocations, some projects have a 100 times more compute time than some others so the scheduler should also make sure that those users with huge projects can run jobs often enough."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 336,
        "question": [
            "Is it possible to submit a job that may take more than 3 days?"
        ],
        "answer": [
            "-   No, and we make no exceptions at all. This is on one hand to prevent monopolisation of a partition by a single user, and on the other hand because it creates a maintenance nightmare as a rolling update of the system can take as long as the longest running job.",
            "    Moreover, it is also a protection against yourself as large systems are inherently less stable than smaller systems so there is a much higher chance that your long running job may fail an hour before it would have finished."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 337,
        "question": [
            "If I allocate only a single GPU, will I automatically get assigned a GPU and a CPU which are \"close\" in the setup? (Assuming such an allocation is available)"
        ],
        "answer": [
            "-   Not sure about that and I even doubt it at the moment. It was definitely not the case before the upgrade, but it looks like the scheduler is still not doing a proper job. The only way to properly control binding is on exclusive nodes.",
            "    You could try to run 8 subjobs in a job each using their own GPU, but then at the moment you may be hit with another scheduler bug for which we are still waiting for a fix from HPE."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 338,
        "question": [
            "Maybe I'm missing a point, but re slide #32: why do we want to skip core 8, 16, 24, 32, ...?"
        ],
        "answer": [
            "-   For reasons of symmetry as core 0 cannot be used as it is reserved. It's a bit strange to use 7 threads on CCD 0 and then 8 threads on each of the other one.",
            "Okay, so it is becuase you don't want other tasks to behave differently from the first which only got 7 cores. Am I understanding that right?",
            "-   If these would be 8 fully independent programs it makes sense to use a different set of resources for each. But in an MPI program it is often easier if each rank has the same amount of resources. And after all the speed of your parallel process is determined by the slowest of the processes anyway.",
            "Okay, I think that makes sense to me.",
            "-   On Frontier they actually block each 8th core by default and we have asked AMD if this may have an advantage as driver threads for each GPU could then run on a different core which may be advantageous if you cannot really use these cores for applications anyway."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 339,
        "question": [
            "Follow-up in Question 31.: If I reserve 2 GPUs, can I expect them to be the two GCDs on 1 device?"
        ],
        "answer": [
            "-   No unfortunately, just as Slurm can also not guarantee on the CPU partition that all your cores would be on a single CCD (or within a single cache domain). With machines as LUMI we really need new scheduling technology that is more aware of the hierarchy in resources for sub-node scheduling.",
            "Okay, so I'd need to reserve the whole node to gain that kind of control.",
            "-   Unfortunately yes."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 340,
        "question": [
            "Rather general question:"
        ],
        "answer": [
            "Does there exist a wiki or open document that would provide suggestions and hints for best practices when using different scientific software on LUMI? The guidelines provided in this course show that LUMI can support a wide range of programming environments and compilers, as well as specific optimization tools and methods. From my understanding, these considerations can vary greatly for each application or software. Therefore, any experience with specific programs could prove invaluable in saving a significant amount of resources.",
            "-   No. There are basically too many different packages in use to even be feasible to put an effort in it, and for some packages it really depends a lot on the actual problem you are solving. Sometimes completely different settings are needed depending on which parts of the packages that are being used (some packages support different algorithms that may be parallelised completely differently) and what problem you are trying to solve.",
            "    That experience should come from user communities of a package as the same experience may be useful on several systems. And much of it may even transfer to systems that are not identical. I have seen manuals of software packages that do try to provide those insights. But, e.g., if your application is not too much restricted by the communication network than there is really no difference between running on LUMI or any other cluster with 64-core Milan CPUs of which there are more and more out these days, and it may even carry over to the newer Genoa CPU.",
            "    For the GPUs it may be a bit different as HPE Cray is the only company that can sell this particular version (the MI250X) and others can only sell the regular MI250 which connects through PCIe. But even then in most cases that experience will be the same. There aren't that many MI200-faily GPUs out in the field though, most companies seem to be waiting for its successor.",
            "Thanks. Alright, I take that the best practice should be seeked from the community of specific software."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 341,
        "question": [
            "Just a note: I've been on the receiving end of file-system downtime. First my project was down, then it went up and my user folder went down. That felt rather annoying... D: Wish both was on the same partition."
        ],
        "answer": [
            "-   We know but the maintenance was urgent. It was a bug that corrupts files. One system was hit with corruption issues and it turned out that there were more serious problems on a second one also. It would have been better though if they had taken the whole system down, and maybe they could have done the work in the time it took now to do two of the four file systems...",
            "-   It is impossible to get the home directory and project space guaranteed on the same partition. Users can have multiple projects and projects have multiple users so there is no way we can organise that. In fact, several users come in via the course project before they even have a \"real\" project on LUMI...",
            "My colleague got lucky in this regard, since we only have one project :p",
            "-   Most of us in user support have two userids, one with CSC for our LUST work and then we got another one, either from our local organisation or because we needed to have one in Puhuri also for other work (before it was possible to have a single ID for the two systems that are used for user management). One of us was blocked on both accounts simultaneously, then tried to get a third account via an appointment at another university and was unlucky to get that one also on the same file system...",
            "poor guy ...",
            "-   Moreover there is a simple formala that connects your userid and project id to the respective storage system it is on..."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 342,
        "question": [
            "What is the difference between these folders at higher hierarchy? For example, my user folder is located at `/pfs/lustrep4/users`. Each of these folders contain folders for users and projects."
        ],
        "answer": [
            "-   They are all on the same file systems but with different quota policies and different retention policies.",
            "Then may be my personal files and project files are located in different parent folder, isn't it?",
            "-   Project and scratch will always be on the same file system but is assigned independently from the user home directory, basically because what I explained above. There is a many-to-many mapping between users and projects."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 343,
        "question": [
            "Are files removed from LUMI after certain days of no activity?"
        ],
        "answer": [
            "-   The policies are an evolving thing but accounts that are not used for too long are blocked even if they have an active project because an unattended account is a security risk.",
            "-   Files on scratch and flash will be cleaned in the future after 90 or 30 days respectively (likely access date though and not creation date). This is rather common on big clusters actually.",
            "-   All files of your project are removed 90 days after the end of the project and not recoverable.",
            "-   Not sure how long a userid can exist without a project attached to it, but we already have closed accounts on LUMI with all files removed.",
            "-   And note that LUMI is not meant for data archiving. There is **no backup**, not even of the home directory. You are responsible for transfering all data to an external storage service, likely your home institute."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 344,
        "question": [
            "Re the Tallinn course: will it be recorded too, like this one? And where will this course recording be available?"
        ],
        "answer": [
            "-   The problem with the 4-day courses that there is lot of copyrighted material in there that we can only make available to users of LUMI. So far this has been done with a project that was only accessible to those who took the course but we are looking to put them in a place were all users can access the data on LUMI. The HPE lectures will never go on the web though. There is a chance that we will be allowed to put the AMD presentations on the web.",
            "    There is actually material from previous courses already on the course archive web site that JÃ¸rn referred to in his introduction, on lumi-supercomputer.github.io/LUMI-training-materials.",
            "I'm mostly interested in the GPU-profiling, but I can't attend due to other events.",
            "-   Part of that is HPE material for which we are looking for a better solution, part of it is AMD material and so far we have been allowed to put their slides on the web, but we have to ask for the recordings. We only got a access to a system capable of serving the recordings last week so we are still working on that."
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 345,
        "question": [
            "Can we cancel our tickets ourselves, if the problem \"magically\" solves meanwhile :)?"
        ],
        "answer": [
            "- I am not aware of such an option.",
            "- Simply mail that the issue is resolved and the owner of the ticket will be very happy to close it ;-)"
        ],
        "source": "lumi-1day-20230509"
    },
    {
        "idx": 346,
        "question": [
            "When we run `sbatch` with `--cpus-per-node=X` are we allocating X cores or X CCDs or X NUMA nodes ...?"
        ],
        "answer": [
            "-   You allocate cores (not threads). But admittedly the slurm nomenclature is really confusing.",
            "-   Slurm had threads (hardware threads), cores (physical cores) and CPUs. A CPU is the smallest individually allocatable unit which on LUMI is configured to be the core. Unfortunately Slurm does not fully understand the full hierarchy of a modern cluster which makes it different to request all cores in a single CCD or single NUMA domain."
        ],
        "source": "lumi-1day-20230921"
    },
    {
        "idx": 347,
        "question": [
            "I have been experiencing very long queueing times recently and have been warned that i am out of storage hours although i have more or less cleaned my output (|I am aware disk space and storage hours are not the same thing) but i am wondering if these long times are related to storage hours"
        ],
        "answer": [
            "-   It's not related. Removing files when running out of storage allocation (in TB/hours) does make the TB/hours as each file stored on LUMI will consumes these TB/hours from your allocation as long as it's present on the system. When you delete a file, it will stop being billed but the TB/hours consumed will still be definitively gone.",
            "Thanks, then what is the way to go forward as i have yet only spent 30% of my CPU hours :)",
            "-   Is your allocation granted by a consortium country or EuroHPC JU?",
            "Consortium country",
            "-   Contact the resource allocator of your country to request additional TB/hours",
            "Will do so, thanks a lot :)",
            "-   For the queue time, we are sorry, it's caused by three things:",
            "    1.  An oversubscription factor that was way to high",
            "    2.  Lots of huge jobs in the queue. To start a big job, the scheduler has to collect nodes that in the mean time can only be used for backfill by small jobs but we don't always have enough of those small jobs. So you may note a lot of idle nodes while your job still does not start quickly.",
            "    3.  A lot of CPU nodes have been down lately which with the large oversubscription factor create long queueing time",
            "    The oversubscription factor will be reduced next year and the new LUMI-C nodes will be put in production at the end of october. It should improve the situation. Resource allocators have also been asked to not use LUMI-C for extreme scale projects anymore that require running lots of very large jobs (large in terms of node counts)",
            "    -   I see, thanks then we need to live with for now :)"
        ],
        "source": "lumi-1day-20230921"
    },
    {
        "idx": 348,
        "question": [
            "Out of curiosity, if LUMI is a GPU-first system, why offer (what remains a quite large amount of) CPU-only nodes?"
        ],
        "answer": [
            "-   I think there are many answers to that question. I guess that some are political, but they idea is also to support heterogenous jobs with some parts of a workflow to run on CPU nodes with others running on GPUs.",
            "-   Additionally, LUMI performance is 2% LUMI-C and 98% LUMI-G."
        ],
        "source": "lumi-1day-20230921"
    },
    {
        "idx": 349,
        "question": [
            "Same question as 1. What about when we run `sbatch` with `--gpus-per-node=X`, what are we allocating?"
        ],
        "answer": [
            "-   One GCD, so you can ask for a maximum of 8 per LUMI-G node."
        ],
        "source": "lumi-1day-20230921"
    },
    {
        "idx": 350,
        "question": [
            "I've been communicated that GPUs process batches of 32 items in 1 cycle on Nvidia (ie. using batch size of 33 first does 32 items in one cycle and 1 item in a separate cycle). Is this the same on AMD? And is this a hardware feature, as one could assume?"
        ],
        "answer": [
            "-   AMD compute GPUs use 64-wide wavefronts but there is a catch. In practice, the wavefront will be divided in 4x16 workitems which match the compute units (CUs) architecture that feature 4x16-wide SIMD units. Each of these units are assigned a wavefront. The wavefront once assigned to one SIMD unit will be processed in 4 cycles (16 workitems/cycle). As there is 4 SIMD units per CU, 4 wavefronts can be active at the same time in a CU and the total throughput of a CU can be seen as 1x 64-wide wavefront/cycle/CU.",
            "-   See the documentation for more details about the MI250x GPUs."
        ],
        "source": "lumi-1day-20230921"
    },
    {
        "idx": 351,
        "question": [
            "How are GPU hours billed on standard-g and on small-g? Is it the number of GPU hours that you request for a job, using the argument #SBATCH --time, or is it the actual GPU usage per job, which is usually less than the requested hours?"
        ],
        "answer": [
            "-   For GPU compute, your project is allocated GPU-core-hours that are consumed when running jobs on the GPU nodes https://docs.lumi-supercomputer.eu/runjobs/lumi_env/billing/#gpu-billing",
            "-   For the standard-g partition, where full nodes are allocated, the 4 GPUs modules are billed. For the small-g and dev-g Slurm partitions, where allocation can be done at the level of Graphics Compute Dies (GCD), you will be billed at a 0.5 rate per GCD allocated.",
            "Thanks! I understand this, but if e.g. I request #SBATCH --time=24h and my job fails after 2 hours, am I billed for 24h or for 2h?",
            "-   You should be billed for 2 hours if your job is killed. Beware that there is a possibility that your job hangs when your job fails and you'd be billed for the time it hangs as well."
        ],
        "source": "lumi-1day-20230921"
    },
    {
        "idx": 352,
        "question": [
            "The slide mentioned the cray-*-ofi for OFI. Do we still need to use the AWS OFI mentioned in some compiling instructions?"
        ],
        "answer": [
            "-   Do you mean the AWS OFI plugin for RCCL? That's different for the `cray-*-ofi` module. The `craype-network-ofi` is meant to select the Cray MPI network backend. With the Slingshot-11 network only libfabric/OFI is supported. In the past we had Slingshiot-10 interconnects and the `craype-network-ucx` module can be use to select the UCX backend. **It's no longer supported**. However, the `craype-network-*` modules are still useful as it's basically a way to switch MPI on and off in the compiler wrapper:",
            "    - `module load craype-network-none`: disable MPI",
            "    - `module load craype-network-ofi`: enable MPI",
            "-   The AWS OFI plugin is a plugin for RCCL (AMD GPU collective communication library, replacement for NVIDIA NCCL). This plugin is used so that RCCL can use the Slingshot-11 interconnect as it does not support it out of the box."
        ],
        "source": "lumi-1day-20230921"
    },
    {
        "idx": 353,
        "question": [
            "If we need to compile a library with gcc to generate our executables with support for MPI, do we have to load all the corresponding Cray modules or one of the PrgEnvs and the cray MPI module?"
        ],
        "answer": [
            "-   Most of the time only loading `PrgEnv-gnu` is sufficient as the MPI module is loaded by default. The Cray compiler wrapper will automatically link to the correct MPI library for the Programming Environment you selected by loading a `PrgEnv-*` module."
        ],
        "source": "lumi-1day-20230921"
    },
    {
        "idx": 354,
        "question": [
            "What does it mean that a module is hidden?, it seems that it would be silently skipped, how we can change that state?"
        ],
        "answer": [
            "-   It means that the is not listed in any searches by default because it might have problems or incompatibilities. you can display all modules including the hidden ones by loading the `ModulePowerUser/LUMI` module."
        ],
        "source": "lumi-1day-20230921"
    },
    {
        "idx": 355,
        "question": [
            "Do we have a PyTorch module?"
        ],
        "answer": [
            "-   Yes, as user installable with EasyBuild (https://lumi-supercomputer.github.io/LUMI-EasyBuild-docs/p/PyTorch/) or in CSC software collecion (https://docs.lumi-supercomputer.eu/software/local/csc/)"
        ],
        "source": "lumi-1day-20230921"
    },
    {
        "idx": 356,
        "question": [
            "I just want to point out that the slides of today seem to be inaccessible, as are the ones from previous training days. E.g. https://lumi-supercomputer.github.io/LUMI-training-materials/1day-20230509/ and clicking on any \"slides\" link fails."
        ],
        "answer": [
            "-   Works for me. Both the old and new slides load as expected.",
            "-   I get network errors (PR_CONNECT_RESET_ERROR) in both Firefox and Chrome. Not a big issue in any case."
        ],
        "source": "lumi-1day-20230921"
    },
    {
        "idx": 357,
        "question": [
            "Same issue as above. Can the slides be hosted somewhere else so that they are accessible to everyone?"
        ],
        "answer": [
            "-   You could download it from LUMI with `wget` and then move to your computer from there.",
            "This worked for me, but accessing it with three different browsers did not work.",
            "-   I also guess that is something firewall related but weird that you can access lumi via ssh but not LUMI-O.",
            "-   I read that some browser extensions and some proxies can also cause this problem. Most of these connection problems are actually not caused by the server (LUMI-O for the slides and recordings) but somewhere else between the server and your browser or the browser itself. It's a bit weird, we've been using LUMI-O as the source for big files for a while now and never got complaints or tickets.",
            "    Sysadmins did note some random problems with LUMI-O recently and are investigating, it may or may not be related to that also. But I (KL) have been pushing lots of data to and pulling lots of data from LUMI-O in the past days from different computers and browsers while preparing this course and the upcoming 4-day course without issues."
        ],
        "source": "lumi-1day-20230921"
    },
    {
        "idx": 358,
        "question": [
            "What is the difference between `lumi-container-wrapper/cotainr` and Singularity containers?"
        ],
        "answer": [
            "-   Both our tools use singularity in the background but help you with creating the containers, so you don't have to build the container yourself."
        ],
        "source": "lumi-1day-20230921"
    },
    {
        "idx": 359,
        "question": [
            "Let's say I want to build PyTorch (with GPU support of course). Am I understanding correctly that I should load PrgEnv-amd?"
        ],
        "answer": [
            "-   Both `PrgEnv-amd`, `-cray` and `-gnu` work with `rocm` which GPU enabled codes rely on. Basically first two environmnents give you Clang/LLVM based compilers. I doubt PyTorch requires Clang as a host compiler to be compiled for AMD GPUs.",
            "-   For PyTorch the way to go is to use GNU for the host (CPU) compilation in conjunction with the `rocm` module to have access to `hipcc` for GPU code compilation. Compiling PyTorch with `PrgEnv-cray` or `PrgEnv-amd` is likely to fail due to some packages using intel flavoured inline assembly that is not supported by Clang based compilers.",
            "    - Okay. Got it. But the latest rocm module available is 5.3.3 that is very old (current that is VERY new is 5.7.0). Do I need to compile my own rocm also?",
            "-   ROCm version is related to AMD GPU driver version. With current SLES kernel version Cray OS is based on, ROCm versions > 5.4 are not supported, unfortunately.",
            "    A major system update with new AMD GPU driver will be at the end of the year at the earliest.",
            "-   You can try other ROCm versions in containers. It does turn out that some newer versions work for some users even with the current driver. E.g., one of our AMD support people has used PyTorch with ROCm 5.5 in a container. The problem with newer ROCm versions is that (a) they are based around gcc 12 and the gcc 12 on the system is broken so we cannot fully support it and (b) ROCm libraries are also used by, e.g., GPU-aware MPICH and newer versions cause problems with cray-mpich.",
            "    Recent ROCm versions show improvements in particular in the support libraries for AI so I can understand why as a user of PyTorch or Tensorflow you'd like a newer version.",
            "    We realise many users are frustrated by this. The problem is that installing ROCm on a supercomputer is not as simple as it is on a workstation. On LUMI, it triggers a chain of events. We need a newer GPU driver. That in turn needs a newer OS kernel. However, software on a supercomputer is managed through a management environment and that management environment needs an update also to support that newer kernel. So at the end updating ROCm requires updating the full software stack on LUMI and likely a week or more of downtime, and extensive testing before starting the process. There are only a few systems like LUMI in the world which also implies that the installation procedures are not thoroughly tested and that whenever a big update of LUMI is done, problems show up. Nowadays we have a test system to detect most of the problems before they are actually rolled out on the big systems, but in the early days we didn't and the first big update which was scheduled to take 3 weeks took 7 weeks in the end due to problems... So I hope you can understand why a big machine as LUMI is not the best environment if you want the very latest... It is only a pity that there is no second smaller development machine on which we could take more risks as it wouldn't matter as much if that one would be down for a few weeks.",
            "-   Our AMD support person has also been building a properly set up container for PyTorch. I'd have to check in the LUMI documentation where to find it but that may be a more easy way. Compiling PyTorch can be tricky."
        ],
        "source": "lumi-1day-20230921"
    },
    {
        "idx": 360,
        "question": [
            "Regarding the long queue times i ve asked in question 2, would using the small nodes instead of the standard nodes help as some of the runs are literally taking only 2 minutes to run, which prepare the model for the actual production run that should be run in standard nodes?"
        ],
        "answer": [
            "-   It may or may not and the answer is time-dependent. They are scheduled fairly independently. There was actually a mail after the last downtime to users to ask to use small/small-g more for jobs that can run in that partition. I'd have to check what the partition sizes are today, but the sysadmins at some point also moved some nodes from small to standard to try to balance the waiting times more. If those 2-minute jobs are also very small in node count (which I assume as you want to run them in small) and if the time you request is then also very low (like 10 minutes or so to be sure), they are ideal as backfill though and may start quickly on standard/standard-g actually and actually only use time that would otherwise been wasted. I've been rather successful with that strategy when preparing this and another course I am teaching about LUMI ;-) Sometimes my waiting times on standard/standard-g also became longer, and I assume an overloaded scheduler was partly to blame.",
            "So instead of asking standard with 48 hours request, asking small with say 1 hour (or smaller?) does not really change? another reason i use standard is because the model runs hardcored with 88 CPUs whether or not it is a preperation run or a production run.",
            "-   The element that really matters if you want your job to start quickly, is to be realistic with the time your request. If you know that it is something that finishes quickly, don't request the maximum allowed time. On standard and standard-g there is usually quite some room to run programs as backfill. The scheduler will schedule lower priority jobs on idle nodes that it is saving for a big job if it knows that that job will have finished before it expects to have collected enough nodes for that big highest priority job. If you always request the maximum wall time even if you know that it will not be needed, the job will never be used as backfill. But if you know a job will end in 5 minutes and then only request like, say, 10 minutes to have some safety margin, there is a high chance that the scheduler will select it to fill up holes in the machine. Nothing worse for a scheduler than all users just requesting the default maximum wall time rather than a realistic walltime as then it has no room to play with to fill up gaps in the machine.",
            "-   And there is even another element to be realistic with wall times. There are ways in which a job can crash where the scheduler fails to detect that the job has terminated and so keeps the allocation. It looks like in particular on LUMI some MPI crashes can remain undetected, probably because MPI fails to kill all processes involved. You will then be billed for the whole time that the job keeps the allocation, not for the time before it crashed."
        ],
        "source": "lumi-1day-20230921"
    },
    {
        "idx": 361,
        "question": [
            "Why does the `small` partition allow to allocate 256G memory per node while `debug` allows only 224G?"
        ],
        "answer": [
            "-   It's because the high-memory nodes (512GB and 1TB) are in the `small` partition. Standard nodes in both `small` and `debug` have the same amount of memory available (224GB). If you go above that in the `small` partition, you will get an allocation on one of the high-memory nodes instead of a a standard one. Note that if you go above 2GB/cores you will be billed for this extra memory usage. See here for details of the billing policy.",
            "I see, thanks. So from this I understand that when I request `--partition=small; --mem=256G;`, the job will not be assigned to a standard node. Only high-memory nodes will be available. It is not made clear on CPU nodes - LUMI-C that not all of the memory can be allocated. I assumed that I can request all 256G from a standard node.",
            "-   It's explained here but you are right it's not stated in the hardware section. The reason is that in reality, the node actually have 256GB of memory but part of it is reserved for the operating system. LUMI nodes are diskless, so we have to reserve quite a big chunk of the memory to make sure the OS has enough space.",
            "How much memory can be allocated on 512G and 1TB nodes?",
            "-   On all nodes of LUMI it is the physcial amount of RAM minus 32 GB (480 GB and 992 GB). For the LUMI-G nodes: 512 GB installed, 480 GB available."
        ],
        "source": "lumi-1day-20230921"
    },
    {
        "idx": 362,
        "question": [
            "Is the `--mail-user` functionality already working for LUMI slurm jobs? It is working on our national clusters, but so far hasn't worked for me on LUMI (with `--mail-type=all`)"
        ],
        "answer": [
            "-   Unfortunately, it's not active. The LUMI User Support Team has raised the issue multiple times (since the start of the LUMI-C pilot actually) but sysadmins never made the necessary configuration. I understand it can be frustrating for users as it's a very basic feature that should be working."
        ],
        "source": "lumi-1day-20230921"
    },
    {
        "idx": 363,
        "question": [
            "Does `--ntasks=X`  signify the number of `srun` calls, i.e. number of steps in a job?"
        ],
        "answer": [
            "-   No. It is used inside `srun`. `srun` creates one job step with multiple tasks, each task basically being a copy of a process that is started. It is possible though to ask sbatch for, e.g, 5 tasks with 4 cores each, and then use multiple `srun` commands with each `srun` asking to create 1 task with 4 cores. Unfortunately we do see problems with network configurations when trying to run multiple job steps with multple `srun` commands simultaneously (by starting them in the background with an & and then waiting untill all have ended).",
            "    You would use `--ntasks=X`, e.g., to start an MPI job with X ranks.",
            "I am confused when you could define, say `--ntasks=8` and `--cpus-per-task=2`. Are we then allocating 16 cores or 8 cores?",
            "-   16 cores. Each task can then use 2 cores which would be the case for a hybrid MPI/OpenMP job. It would also guarantee that these cores are in groups of 2,",
            "    because on `small` you would have no guarantee that all cores are on a single node. It may instead use cores from several nodes."
        ],
        "source": "lumi-1day-20230921"
    },
    {
        "idx": 364,
        "question": [
            "I just want to comment on the question on `--mail-user`. In Norway, on the Saga HPC cluster it was used until a user directed the emails from a large array to the helpdesk of Saga, that filled the helpdesk server. Then it was decided to stop the functionality."
        ],
        "answer": [
            "-   Even without users redirecting them to the help desk there are potential problems. Not that much on LUMI as we have a very low limit on the number of jobs, but no mail administrators would be happy with a user running a large array job on the cluster as each job in the array is a separate job and would send a mail. Imagine a user doing throughput computing and starting a few 1000s of jobs in a short time.... It might actually lead to mail systems thinking they're being spammed.",
            "    Another problem is failures of the receiver etc.",
            "    And another problem on LUMI is what to do if no mail user is given in the job script. Due to the way user accounts are created on LUMI (there are several channels) it is not as easy as on some university systems to link a mail address to a userID."
        ],
        "source": "lumi-1day-20230921"
    },
    {
        "idx": 365,
        "question": [
            "There was a comment that using `--gpus-per-task` was tricky. Perhaps I missed it, what was the pitfall of using it?"
        ],
        "answer": [
            "-   The problem is the way in which Slurm does GPU binding which is not compatible with GPU-aware MPI. I'm not sure how technical you are and hence if you can understand my answer, but let's try.",
            "    For CPUs Linux has two mechanisms to limit access to cores by a process or threads. One is so-called control groups and another one is affinity masks. Control groups really limit what a process can see and Slurm uses it at the job step level with one control group shared by all tasks in a job step on a node. That means that the tasks (processes) on a node can, e.g., share memory, which is used to communicate through memory. At the task/process level affinity masks are used which do not block sharing memory etc.",
            "    For GPU binding there are also two mechanisms. One is a Linux mechanism, again the control groups. The other one is via the ROCm runtime via the ROCR_VISIBLE_DEVICES mentioned during the presentation. You can compare this a little bit with affinity masks except that it is not OS-controlled and hence can be overwritten. The problem with `--gpus-per-task` is that Slurm uses both mechanisms and uses them both at the task level. The consequence is that two tasks cannot see each others memory and that hence communication via shared GPU memory is no longer possible. The funny thing is that Slurm will actually still set ROCR_VISIBLE_DEVICES also in some cases. So it is basically a bug or feature in the way Slurm works with AMD GPUs. It should use control groups only at the job step level, not at the task level, and then things could work.",
            "I don't use MPI, I only have ML applications in Python. Is this still a relevant problem?",
            "-   Yes, if you have multiple tasks. I gave MPI as the example but it holds for all communication mechanisms that go via shared memory for efficiency. RCCL for instance will also be affected. If you have something with `--ntasks=1` it should not matter though."
        ],
        "source": "lumi-1day-20230921"
    },
    {
        "idx": 366,
        "question": [
            "I am using `torch.distributed.run()` for starting my multi-GPU computation. I provide `--ntasks=1` (I use only single node). Then as a parameter to `torch.distributed.run`, I give `--nproc_per_node=#NUM_GPUS`. AFAIK, the torch.distributed.run then starts #NUM_GPUS processes. Does this cause binding problems? If so, can I somehow provide a custom mapping for this setup?"
        ],
        "answer": [
            "-   Torch will have to do the binding itself if it starts the processes. Our PyTorch expert is not in the call though, I'm not sure about the right answer.",
            "    Does it start #NUM_GPUS processes because it has that many GPUs or because it has that many cores? If it is the former I would actually consider to give Torch access to all CPU cores.",
            "    I suspect Torch could benefit from a proper mapping, not only a proper mapping of CPU-to-GPU but also even a correct ordering of the GPUs. I understand that RCCL often communicates in a ring fashion so it would be usefull to exploit the fact that there are rings hidden in the topology of the node. But I don't think that anybody in our team has ever experimented with that.",
            "One process per GPU. Thanks! Something that I will have to look into.."
        ],
        "source": "lumi-1day-20230921"
    },
    {
        "idx": 367,
        "question": [
            "What does the numbers in `srun --cpu-bind` option represent `fe00` etc?"
        ],
        "answer": [
            "-   These are hexadecimal numbers where each bit represents a core (actually hardware thread to be precise) with the lowest order bit representing core 0. So for `fe00`: do not use core 0-7 (the last two 0's, so 8 zero bits), then the e corresponds to the bit pattern `1110` so do not use core 8 but use core 9, 10 and 11, and `f` corresponds to the bit pattern `1111` which is then use cores 12, 13, 14 and 15. So effectively: this concrete example means use CCD 1 (they are numbered from 0) except for the first core of that CCD which cannot be used because it is set aside for the OS and not available to Slurm."
        ],
        "source": "lumi-1day-20230921"
    },
    {
        "idx": 368,
        "question": [
            "Adding to the previous question: would this specific example cpu binding scheme also work for jobs on the small-g partition?"
        ],
        "answer": [
            "-   Only if you request the whole node with `--exclusive`. Manual binding is only possible if you have access to the full node as otherwise you cannot know which subset of cores is assigned to your application, and as currently Slurm is not capable to make sure that you get a reasonable set of cores and matching GPUs on the small-g partition. Which is one of the reasons why the `small-g` partition is so small: It is not a very good way to work with the GPUs."
        ],
        "source": "lumi-1day-20230921"
    },
    {
        "idx": 369,
        "question": [
            "To which Numa domain I should bind which GCD? I remember it was _not_ Numa domain 0 to GCD 0, etc."
        ],
        "answer": [
            "-   Good question. There are examples of valid masks in the GPU examples in the LUMI documentation but that is not the clearest way to present things. There is a graphical view on the GPU nodes page in the LUMI documentation. I've put the information in tabular form in the notes I made for my presentations."
        ],
        "source": "lumi-1day-20230921"
    },
    {
        "idx": 370,
        "question": [
            "Will we get some information today on how to (in practice) profile (and afterwards, improve) existing code for use on LUMI?"
        ],
        "answer": [
            "-   No. Profiling is a big topic on our 4-day courses that we have two or three times a year. However, if you have a userid on LUMI you have access to recordings of previous presentations. Check material from the course in Tallinn in June 2023 and we'll soon have new material after the course in Warsaw in two weeks. That course is on October 3-6 but I'm not sure if it is still possible to join. On-site is full I believe and online is also pretty full.",
            "    There is also some material from a profiling course in April 2023 but especially the HPE part there was a bit more \"phylosophical\" discussing how to intepret data from profiles and how to use that to improve your application.",
            "Thank you very much, that material will be very useful!",
            "-   If you are interested in GPU profiling there are also some examples on Rocprof and Omnitrace here https://hackmd.io/@gmarkoma/lumi_training_ee#Rocprof (it is a part of materials from course in Tallin)."
        ],
        "source": "lumi-1day-20230921"
    },
    {
        "idx": 371,
        "question": [
            "What is the default striping behaviour if we write a file without calling `lfs setstripe`?"
        ],
        "answer": [
            "-   By default only a single OST will be used. This is to avoid problems with users who don't understand LUSTRE and create lots of small files. The more OSTs a file is spread over, the more servers the metadata server has to talk to when opening and closing a file, and if these are not used anyway this is a waste of resources. It may not seem logical though on a system that is built for large applications and large files...",
            "    However, I'm sure there are plenty of people in the course who in practice dump a dataset on LUMI as thousands of 100 kB files and refuse to do the effort to use a structured file format to host the whole dataset in a single file. And then there are those Conda installations with 100k small files."
        ],
        "source": "lumi-1day-20230921"
    },
    {
        "idx": 372,
        "question": [
            "Does striping matter only if I/O is the bottleneck?"
        ],
        "answer": [
            "-   Mostly. But then we have users who write files that are literally 100s of GB and then it really matters. One user has reported 50 GB/s on LUMI P after optimising the striping for the file..."
        ],
        "source": "lumi-1day-20230921"
    },
    {
        "idx": 373,
        "question": [
            "I just checked. My python venv seems to contain ~6k files. Did not know about the possibility to containerize it before today. Is it worth doing in this case, or if not, how many files should I have before containerizing?"
        ],
        "answer": [
            "-   It's hard to put a hard number on it as it also depends on how the files are used. We tend to consider 6k as still acceptable though it is a lot.  It also depends on how you use them. If you run jobs that would start that Python process on 100's of cores simultaneously it is of course a bigger problem than if you have only one instance of Python running at a time.",
            "    But as a reference: HPE Cray during the course mentions that one LUMI-P file system is capable of probably 200k metadata operations per second which is not much and surprising little if you compare that to what you can do on a local SSD in a laptop. IOPS don't scale well when you try to build larger storage systems.",
            "    If your venv works well with lumi-container-wrapper it may not be much work though to test if it is worth trying.",
            "-   It is also not just a LUMI thing but a problem on all large supercomputers. When I worked on a PRACE project on a cluster at BSC,",
            "    I had a Python installation that took 30 minutes to install on the parallel file system but installed in 10s or so on my laptop..."
        ],
        "source": "lumi-1day-20230921"
    },
    {
        "idx": 374,
        "question": [
            "Does LUMI support Jupyter notebooks or has a Jupyter hub? As one of the task of my project is to create catalogs / jupyter notebooks for the data generated in the project."
        ],
        "answer": [
            "-   No official support but we know that users have gotten notebooks to work. Something will come with Open OnDemand but not date set yet for availability of that. After all, LUMI is in the first place a system to process large batch jobs and not a system for interactive work or workstation replacement, so it does not have a high priority for us.",
            "-   Since the data cannot stay on LUMI after your project - LUMI is not a data archive solution - I wonder if LUMI is even the ideal machine to develop those notebooks or if that should be done with the machine where the data will ultimately land on?",
            "-   And if the data is really important to you: Please be aware that there are **no backups** on LUMI!"
        ],
        "source": "lumi-1day-20230921"
    },
    {
        "idx": 375,
        "question": [
            "Does LUMI has interactive nodes through VNC (as in the exercise) to use the visualization nodes interactively?"
        ],
        "answer": [
            "-   VNC is available through the lumi-vnc module which contains help about how it works. But otherwise the visualisation nodes are still pretty broken, not sure if vgl-run actually works. As for the previous question, it is not a very high priority at the moment, not for LUST and not for CSC who has to do much of the basic setup. Support should improve when Open OnDemand becomes available which is being worked on by CSC.",
            "    Light VNC sessions can run on the login nodes, but you can always start an interactive job on a compute node, start VNC there and the `start-vnc` script will actually tell you how you can connect to the VNC server from outside using either a VNC client (the server is TurboVNC) or via a web browser (less efficient though for heavy graphics)."
        ],
        "source": "lumi-1day-20230921"
    },
    {
        "idx": 376,
        "question": [
            "Are the tickets publicly viewable? Is there any plan to add some public issue tracker system? We have something like this on our local cluster, and it's quite nice for seeing what the current problems are and what is being done about them."
        ],
        "answer": [
            "-   There are no such plans at the moment. Security and privacy are big issues. And since LUMI accounts come onto the system via so many ways organising login is also not easy as we have to interface with multiple IdM systems. We do have a status page at https://www.lumi-supercomputer.eu/lumi-service-status/ but that one is also limited."
        ],
        "source": "lumi-1day-20230921"
    },
    {
        "idx": 377,
        "question": [
            "One question regarding SLURM job scripts: on our clusters, I am using the command `seff $SLURM_JOBID` at the end of the file to get output on the consumed resources. But I think `seff` is not available on LUMI?"
        ],
        "answer": [
            "-   It is not on LUMI. It is actually an optional command and not part of core Slurm. We've tested `seff` and it turns out that the numbers that it produces on LUMI are wrong because it doesn't deal correctly with the way we do hyperthreading and report about that in the Slurm database.",
            "    If you really want to try: seff in the LUMI Software Library but don't send us tickets about the wrong output. We know the output is wrong in most cases."
        ],
        "source": "lumi-1day-20230921"
    },
    {
        "idx": 378,
        "question": [
            "What does CCD stand for?"
        ],
        "answer": [
            "-   Core complex dies. There are 8 CCDs per processor with 8 cores each. LUMI-C has 2 processors (CPUs) per node while LUMI-G nodes have one."
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 379,
        "question": [
            "What is the use of NUMA"
        ],
        "answer": [
            "-   It is a way of designing CPUs. It means that not all cores have the same memory access time",
            "    with regards to L3 cache. So data stored in one L3 cache (shared by 8 cores) can be accessed",
            "    very efficiently by those 8 cores but takes longer to be access by the other 56 cores in that CPU.",
            "    WikiPedia article on NUMA"
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 380,
        "question": [
            "Can you say something about storage to GPU data path..."
        ],
        "answer": [
            "-   Can you elaborate a bit what you want to know?",
            "To get data to GPU, should that be read to RAM first or is there any majic like the slingshot GPU connection",
            "-   Is your question: \"is there an AMD equivalent of NVIDIA's GPU direct storage?\"",
            "yes",
            "-   Unfortunately, no.",
            "Is there any benchmarking results on, reading data to GPU from HDD Vs sending data in one GPU to another GPU in",
            "another machine via slingshot, i.e. what is the best way to distribute 128Gb across GPUs.",
            "-   I don't think we did any benchmark of data loading from the file system to the GPU memory but GPU-to-GPU",
            "    communication will always be faster than File system-to-GPU data loading."
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 381,
        "question": [
            "What is the reasoning behind choosing AMD GPUs vs NVIDIA GPUs? Are we going to get AMD's MI300 GPUs at"
        ],
        "answer": [
            "LUMI as well? Is it because of cheaper and environmental reasons?",
            "-   The AMD offer was better compared to the NVIDIA offer during the procurement of LUMI.",
            "    NVIDIA knows they are in a quasi-monopoly position with their proprietary CUDA stack",
            "    and tries to exploit this to raise prices...",
            "-   MI300: Not at the moment but it can't be excluded if an extension of LUMI occurs at some point"
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 382,
        "question": [
            "Is it possible to visit LUMI supercomputer in Kajaani?"
        ],
        "answer": [
            "-   Rather not but it of course depends on what is the reason and context.",
            "    Send us a ticket with some more info and we will come back to you. https://lumi-supercomputer.eu/user-support/need-help/"
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 383,
        "question": [
            "The GNU compilers do not have OpenMP offload to GPUs, ok. But can we use them with HIP?"
        ],
        "answer": [
            "-   Not to compile HIP code but we have build applications mixing HIP and Fortran",
            "    using the Cray/AMD compilers for HIP and GNU gfortran for the fortran part.",
            "    HIP code can only be compiled using a LLVM/clang based compiler like the",
            "    AMD ROCm compilers or the Cray C/C++ compilers.",
            "    But this is precisely why you have to load the `rocm` module when using the GNU or Cray",
            "    compilers to compile for the GPUs..."
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 384,
        "question": [
            "Are the modules ```LUMI/22.08 (S,D)    LUMI/22.12 (S)    LUMI/23.03```... tool chains ?"
        ],
        "answer": [
            "-   They are software stacks. Kurt will discuss them in the software stacks session."
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 385,
        "question": [
            "What are differences between GNU GCC compiler and Cray compilers?"
        ],
        "answer": [
            "-   For general differences between the compilers there are many sources in internet.",
            "    On LUMI, there are pages in our docs for Cray and GNU compilers:",
            "    - Working with CCE",
            "    -   Working with the GNU compilers",
            "-   They are totally different code bases to do the same thing. They are as different as Chrome and Firefox are:",
            "    Just as these are two browsers that can browse the same web pages, the Cray and GNU compilers are two sets",
            "    of compilers that can compile the same code but have nothing in common otherwise.",
            "    The Cray compilers are based on Clang and LLVM technology. Most vendors are actually moving to that",
            "    code base for commercial compilers also. All GPU compilers are currently based on LLVM technology,",
            "    also for GPUs from NVIDIA and Intel. The new Intel compilers are also based on Clang and LLVM",
            "    (and just as Cray they use their own frontend due to lack of an open source one of sufficient quality)."
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 386,
        "question": [
            "Are these `craype-...` modules loaded automatically when you loadthe software stack?"
        ],
        "answer": [
            "-   By default, when you log in, PrgEnv-cray is loaded. It includes the Cray compilers, cray-mpich and cray-libsci (BLAS, LAPACK, ...)",
            "-   I'll come back to that in the software stack presentation."
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 387,
        "question": [
            "How do software stacks, Programming Env, tool-chains are related to each other conceptually?"
        ],
        "answer": [
            "-   Basically Programming Env is compiler (C,C++,Fortran), it's runtime libraries and entire set",
            "    of libraries built against the compiler (AMD environment lacks Fortran compiler);",
            "    Software Stack is entire application collection built with possibly all Programming Environments",
            "    in a given release version (toolchains); Toolchain is technical concept for a specific",
            "    Programming Env version and fixed set of related libraries.",
            "-   Software Stack could be `CrayEnv` (native Cray Programming Environment), `LUMI` or `Spack`",
            "-   In practice you can select Programming Env with either `PrgEnv-` (`gnu`, `cray`, `amd`) modules (Cray's native)",
            "    or `cpeGNU`, `cpeCray`, `cpeAMD`; these are equivalent but latter ones are used in LUMI toolchains",
            "-   Toolchain is a concept used with LUMI Software Stack and they are `cpeGNU/x.y` or `cpeCray/x.y` or `cpeAMD/x.y`",
            "    where `x.y` stands for specific `LUMI/x.y` release which in turn follows `x.y` release of the Cray Programming Environment."
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 388,
        "question": [
            "What kind of support is there for Julia-based software development? Do I need to install julia and Julia ML libraries like Flux.jl locally?"
        ],
        "answer": [
            "-   We have some info in our docs here: https://docs.lumi-supercomputer.eu/runjobs/scheduled-jobs/julia/",
            "-   Alternatively, you can use a julia module provided by CSC: https://docs.lumi-supercomputer.eu/software/local/csc/",
            "-   Setting up proper Julia development environment might be quite complex on LUMI. One of the possible ways is to use Spack (which is available as an alternative LUMI Software Stack).",
            "-   Basically the Julia developers themselves advise to not try to compile Julia yourself and give",
            "    very little information on how to do it properly. They advise to use their binaries..."
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 389,
        "question": [
            "`module av` seems to be quite slow, am I missing something?"
        ],
        "answer": [
            "-  It happend to me as well first time, but subsequent calls are faster, may be some caching ? (let me try... yes that's right)",
            "-  The Lmod cache is purged every day. The first `module av` of the day will always be slow but subsequent",
            "   commands should be way faster."
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 390,
        "question": [
            "Is there any guide to help to quickly find a desired module (e.g. LAMMPS)?"
        ],
        "answer": [
            "It seems that `module av | grep -i lammps` or `module spider LAMMPS` cannot help.",
            "-   There is the Software Library page",
            "    from which you can at least easily see what modules are available",
            "-   We have very few modules preinstalled but as Kurt will explain soon.",
            "    It is very easy to install them yourself using EasyBuild based on the recipes",
            "    listed on the above mentioned software library."
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 391,
        "question": [
            "Are you going to install more scientific packages in future, or it's on users to install them via the EasyBuild?"
        ],
        "answer": [
            "-   You can see from the LUMI software library",
            "    what is pre-installed or installable with EasyBuild. More EasyBuild recipes are constantly developed",
            "    by the user support team. Does this answer to your question?",
            "I think so, from the link I see that it is mostly on users to install their own packages if possible.",
            "-   Yes, the collection of pre-installed software is kept small for a reason. The current presentation enlightens this."
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 392,
        "question": [
            "Do you encourge users to use conda even for installing non-python packages due to (large)"
        ],
        "answer": [
            "storage space they probably take on user home directory (e.g. `~/.conda`)?",
            "-   You can use Conda but not natively as you are used to from your laptop and maybe other clusters.",
            "    We do not encourage native conda installations (just using `conda create`) as this creates many",
            "    tens to hundreds of thousands of files and puts quite some pressure on the filesystem.",
            "    Instead we offer two tools to create a conda environment inside a container.",
            "    One of them is cotainr",
            "Do mean like using `Singularity` container?",
            "-   Yes",
            "I'm not quite sure that using `Singularity` env works well for all cases.",
            "For example, what if a user develops code on ondemand/jupyter and wants to use",
            "his/her own Singularity-based conda env as a custom kernel?",
            "-   Open OnDemand is actually heavily based on containerised software itself...",
            "-   We really encourage users to use software installed properly for the system via EasyBuild or Spack,",
            "    but that is not always possible because sometimes the dependency chains of especially bioinformatics",
            "    software are too long. For PyTorch and TensorFlow we advise to try to build on top of containers",
            "    provided by AMD and discusses in the LUMI Software Library.",
            "    The size of a software installation in terms of number of gigabytes is not a problem for a computer",
            "    as LUMI. What is a problem is the number of files, and in particular the number of files that is being",
            "    read while starting/using the package, and that determines if it is better to put it in a container."
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 393,
        "question": [
            "How does EasyBuild manage versions of our custom software?"
        ],
        "answer": [
            "-   Do you mean EB recipes that you install from LUMI software stack, or EB recipes that you have developed/modified yourself?",
            "The ones that I develop myself",
            "-   The ones you have developed yourself are managed the same way as the ones from LUMI software stack,",
            "    if you just locate your own recipes in a correct place. This is documented shortly in",
            "    the lumi documentation EasyBuild page.",
            "Thanks",
            "-   I'm not sure what to write about this without just repeating the documentation, but please ask if something is unclear",
            "I understand it now, I'm not very used to use EB."
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 394,
        "question": [
            "I will need Netcdf-c and netcdf-fortran compiled with the GNU toolchain"
        ],
        "answer": [
            "(my application only works with that, not with other compilers) is that available as modules already",
            "or will I have to install them myself with Easybuild?",
            "-   `cray-netcdf` modules (part of the Cray Programming Environment) are recommended to use unless",
            "    other specific version is required. They combine the C and Fortran interfaces in a single module,",
            "    not in 3 different modules like some default EasyBuild installations do.",
            "OK so I found a combination of module which seems \"compatible\":",
            "`module load LUMI/22.08  partition/C  gcc/12.2.0  craype cray-mpich/8.1.27  cray-hdf5-parallel/1.12.1.5  cray-netcdf-hdf5parallel/4.8.1.5`",
            "but it does not have pnetcdf,",
            "-   Parallel netCDF is served by another module called `cray-parallel-netcdf`",
            "There is this combination: module load LUMI/22.08  partition/C  gcc/12.2.0  cray-mpich/8.1.25 cray-parallel-netcdf/1.12.2.5",
            "but it still has not got pnetcdf:   `--has-pnetcdf   -> no`",
            "Parallel netcdf and pnetcdf are two different things",
            "-   `cray-parallel-netcdf/1.12.2.5` does not have the `nc-config` command so you likely have some other",
            "    module loaded that provides that command. All I can find in that module is `pnetcdf-config` that",
            "    declares it is \"PNetCDF 1.12.2\".",
            "That would be great if there was a netcdf-c/netcdf-fortran that was built with it, is there?",
            "All I need is a set netcdf-c/netcdf-fortran built with pnetcdf in the gcc \"familly\", so maybe",
            "It is netcdf-c and netcdf-fortran I need, my application does not use pnetcdf directly",
            "but the netcdf has to be build with pnetcdf, otherwise the performance is very bad",
            "`module keyword netcdf pnetcdf` finds 3 matches:",
            "```",
            "cray-netcdf: cray-netcdf/4.8.1.5",
            "cray-netcdf-hdf5parallel: cray-netcdf-hdf5parallel/4.8.1.5",
            "cray-parallel-netcdf: cray-parallel-netcdf/1.12.2.5",
            "```",
            "and none of the has both netcdf and pnetcdf, strange, no?",
            "-   Not so strange I think. Isn't PNetCDF a rather old backend?",
            "No, it is maintained, and still used a lot (all the latest releases of netcdf use it)",
            "-   The other two netCDF modules provided by Cray use HDF5 in different configurations",
            "    (one of them parallel) as the backend. That should also give very good parallel I/O performance when used in the proper way.",
            "     But it shows the point Kurt made in the talk: A central software stack is not practical anymore as",
            "     too many users want specialised configurations that are different from others...",
            "     You'll probably have to compile your own versions if the C and Fortran interface provided by `cray-parallel-netcdf` is different.",
            "Maybe should I build it myself, if there is an Easybuild recipe available?",
            "-   There is none at the moment as so far the 3 Cray-provided configurations have been enough for everybody.",
            "    There is also none with the common EasyBuild toolchains. It is just as the Cray modules: Either netCDF-C etc.",
            "    with HDF5 backend, or PnetCDF as a separate package comparable in configuration to `cray-parallel-netcdf`.",
            "    Spack seems to support building netCDF-C/-Fortran with PnetCDF but it is also not the default configuration.",
            "OK, to start with I will try with load LUMI/22.08  partition/C  gcc/12.2.0  craype cray-mpich/8.1.27  cray-hdf5-parallel/1.12.1.5  cray-netcdf-hdf5parallel/4.8.1.5 (that is without pnetcdf)"
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 395,
        "question": [
            "I wanted to install some modules in EasyBuild. I did this:"
        ],
        "answer": [
            "```",
            "module load LUMI/23.09 partition/C",
            "module load EasyBuild-user",
            "eb ncview-2.1.9-cpeCray-23.09.eb -r",
            "eb CDO-2.3.0-cpeCray-23.09.eb -r",
            "eb NCO-5.1.8-cpeCray-23.09.eb -r",
            "```",
            "and then I loaded everything and worked, but when I try it in a new tab it does not work. Does anyone know why?",
            "```",
            "jelealro@uan01:~> module load ncview/2.1.9-cpeCray-23.09",
            "Lmod has detected the following error:  The following module(s) are unknown:",
            "\"ncview/2.1.9-cpeCray-23.09\"",
            "Please check the spelling or version number. Also try \"module spider ...\"",
            "It is also possible your cache file is out-of-date; it may help to try:",
            "  $ module --ignore_cache load \"ncview/2.1.9-cpeCray-23.09\"",
            "```",
            "-   You have to load the same version of the software stack that you used to compile.",
            "    I.e. `module load LUMI/23.09 partition/C` then you can find the modules with `module avail`.",
            "    Alternatively, `module spider NCO` will still list the package and show you how to load it.",
            "I opened a new tab and did this:",
            "```",
            "module purge",
            "module load LUMI/23.09 partition/C",
            "module load ncview/2.1.9-cpeCray-23.09",
            "module load CDO/2.3.0-cpeCray-23.09",
            "module load NCO/5.1.8-cpeCray-23.09",
            "```",
            "but the error still remained",
            "-   Did you add `export EBU_USER_PREFIX=...` to your bashr? Otherwise lmod doesn't know where your modules are.",
            "When I built the Easybuld I did it at my home just for testing, here. `export EBU_USER_PREFIX=/users/jelealro/my_easybuild`.",
            "And no, I don't have it in my bashrc.",
            "-   Try logging in again, then do",
            "    ```",
            "    export EBU_USER_PREFIX=/users/jelealro/my_easybuild",
            "    module load LUMI/23.09 partition/C",
            "    module av NCO",
            "    ```",
            "It worked! thanks you. I was missing the first line `EBU_USER_PREFIX:...`",
            "-   As discussed it is best to have Easybuild install into your project",
            "    (but if you only have the training project now, your home is also okay for testing).",
            "    Put the line in your .bashrc then it will always find your installed modules.",
            "Noted it, I will just re do it in the project folder. Thank you!"
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 396,
        "question": [
            "I am a bioinformatician and don't really understand all of the computer science behind LUMI."
        ],
        "answer": [
            "I have used PBS job submission at Oak Ridge National Lab, so I have some background to begin",
            "(not entirely lost), however I have no idea where to start with LUMI to download my program and submit jobs.",
            "Is this covered at a beginner level in this section about slurm submission?",
            "-   I hope you will find it helpful to start. But you may need a more elementary course like the",
            "    ones that the local organisation should give to train beginners. This course is too fast-paced for beginners.",
            "-   And what system at ORNL still uses PBS, or do you mean Slurm?",
            "-   If you are familiar with Slurm, I'd suggest to see some of the",
            "    LUMI specific examples from [the documentation.",
            "    If you are not familiar with Slurm, a basic Slurm tutorial at first could be helpful.",
            "    E.g. DeiC (Denmark) has developed this Slurm learning tutorial.",
            "    About what to do on LUMI to get your software in use, it depends what software you are using.",
            "    If you can't find your software in the LUMI software library",
            "    or from local stack by CSC or",
            "    otherwise have any questions of how to proceed in practice, you can also",
            "    [open a ticket]https://www.lumi-supercomputer.eu/user-support/need-help/)."
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 397,
        "question": [
            "Should we reserve 8 GPUs per node when submitting a SLURM job, considering that 4 GPUs act like 8?"
        ],
        "answer": [
            "-   Yes, Slurm thinks of one GCD (Graphics Compute Die) (each MI250X consists of two GCDs) as one GPU.",
            "    So ask for 8 gpus if you want to book the whole node.",
            "Does this apply for LUMI C, LUMI G, and so on?",
            "-   Only LUMI-G nodes have GPUs, so it only applies to slurm partitions on LUMI-G (`standard-g`, `dev-g`, `small-g`)"
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 398,
        "question": [
            "Follow up to the previous question."
        ],
        "answer": [
            "I got following error when book 8 gpus per node: `Node 0: Incorrect process allocation input. Do I miss something?",
            "-   Can you show me what slurm parameters you use? Which partition?",
            "Sure:",
            "```bash",
            "#SBATCH --partition=standard-g",
            "#SBATCH --nodes=1",
            "#SBATCH --ntasks-per-node=8",
            "#SBATCH --gpus-per-node=8",
            "```",
            "-   Strange, but it also doesn't look like a slurm error.  Probably best to open a ticket. https://lumi-supercomputer.eu/user-support/need-help/",
            "Aha, OK - just wanted to ensure if I am doing some wrong when booking.",
            "-   You may need to limit GPU visibility to each task if your application expects one GPU per task (MPI rank)",
            "Thanks for the suggesstion. I am doing it by setting the `ROCR_VISIBLE_DEVICES=\\$SLURM_LOCALID` at runtime.",
            "-   So it is likely not the case.",
            "I guess if the error is not related to Slurm, then I must look into application configuration parameters. Thanks."
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 399,
        "question": [
            "Is it possible to run and debug a GPU dependent code without submitting it as a batch job, during development and small testing phase."
        ],
        "answer": [
            "-   You have to use a slurm job but you can use an interactive job to just get a bash on a LUMI-G node.",
            "    Link in the LUMI documentation"
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 400,
        "question": [
            "Why use `salloc` instead of just providing all the options to `srun`?"
        ],
        "answer": [
            "-   About what usage scenario are you thinking? Interactive runs or job scripts?",
            "Interactive runs. im not used to run `salloc` first and then use srun to reach a compute resource.",
            "Usually I provide everything as options to srun. nodes, cores, memory, time, partitions, projects, etc..",
            "-   You can do both ways, and somewhat this is a matter of preference, I think.",
            "    I've understood that `salloc` would be more useful in some more complex cases, though.",
            "-   `salloc` is a command to create an *allocation*. The `srun` command is meant to create a",
            "    *job step* in an allocation. It has a side effect though: If it is run outside an allocation",
            "    it will create an allocation. However, some options for creating an allocation and for a job",
            "    step have a different meaning for both tasks. And this can lead to unexpected side effects",
            "    when you use `srun` to create the allocation and start a job step with a single command.",
            "    `srun` is particularly troublesome if you want an interactive session in which you can then",
            "    start a distributed memory application."
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 401,
        "question": [
            "If we submit a slurm script with `--partition=standard-g` but without requesting any GPUs,"
        ],
        "answer": [
            "which resources are billed? The CPU or GPU hours ?",
            "-   You will be billed GPU hours, and in the case of `standard-g` you effectively get the whole node,",
            "    whether you use it or not, so you will be billed 4 GPU hours for every hour you use the node.",
            "    It is only normal: On LUMI you are billed for resources that others cannot use because of your request,",
            "    whether you use them or not. Likewise, if you would ask for resources on `small-g` you will be billed",
            "    based on the amount of cores, amount of GPUs and amount of memory you request. If you request a",
            "    disproportional amount of one resource, you'll be billed for a similar amount of the other resources.",
            "    So if you would ask for half of the cores or half of the memory, you'd still be billed for 4 GCDs",
            "    (so 2 GPU hours per hour use) as you effectively make 4 GCDs unusable for others.",
            "The output of `lumi-allocations` command is:",
            "```",
            "Project             |                    CPU (used/allocated)|               GPU (used/allocated)|           Storage (used/allocated)",
            "--------------------------------------------------------------------------------------------------------------------------------------",
            "project_465000961   |         12/10000000   (0.0%) core/hours|          0/1000   (0.0%) gpu/hours|             0/10   (0.0%) TB/hours",
            "```",
            "which means so far we only used CPU-resources (?)",
            "-   Maybe you've not done what you think, but also, `lumi-allocations` is not immediate.",
            "    The data of a job has to be processed first offline and the tables that `lumi-allocations`",
            "    shows are updated only a few times per day because of this. :thumbsup:",
            "-   According to the billing pagesin the documeentation",
            "    this will be billed in GPU hours, even if you only use CPU hours."
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 402,
        "question": [
            "Can you run a CPU-GPU hybrid code on GPU partition?"
        ],
        "answer": [
            "-   Sure. You have 56 cores available on each G node.",
            "    You could also do heterogenous slurm jobs with some part (some MPI ranks) run on C nodes and",
            "    some on G nodes. But this is a bit more advanced."
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 403,
        "question": [
            "Do we need to have \"module load\" things in the job file?"
        ],
        "answer": [
            "-   That's a matter of preference if you want to load necessary modules before sending your job script to queue, or in the job script",
            "-   I would recommend putting all module loads into the job script to make it more obvious what is",
            "    happening and more reproducible. We get enough tickets from users claiming that they ran exactly",
            "    the same job as before and that it used to work but now doesn't work, and often it is because",
            "    the job was launched from a different environment and does not build the complete environment",
            "    it needs in the job script."
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 404,
        "question": [
            "So, if I'm running a job on 100 nodes, with --exclusive and I want to use all the memory on the nodes,"
        ],
        "answer": [
            "with --mem=0 it can lead to strange behaviour?",
            "-   There have been some problems in the past with nodes that had less memory available than expected",
            "    due to memory leaks in the OS. By asking explicitly for nodes with 224G (LUMI-C) or 480G (LUMI-G)",
            "    you ensure that you don't get nodes where less is available due to a memory leak."
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 405,
        "question": [
            "How do I run `gpu_check`? I loaded"
        ],
        "answer": [
            "```",
            "module load LUMI/23.09",
            "module load lumi-CPEtools",
            "```",
            "and allocated resources with `salloc` and when I do `srun gpu_check -l` (as shown in the slides) I get",
            "   `slurmstepd: error: execve(): gpu_check: No such file or directory`",
            "-   At least you seem to be missing loading the `partition/G` ?",
            "Indeed, I was missing `partition/G`. thanks!",
            "- As `gpu_check` can only work on LUMI-G nodes, I did not include it in the other versions for `lumi-CPEtools`."
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 406,
        "question": [
            "Can I get different thread count for different tasks in the same job with one binary ?"
        ],
        "answer": [
            "-   Heterogeneous jobs can do that. Or you take the largest number that you want",
            "    for each task and use, e.g., OpenMP functions in your code to limit threads depending",
            "    on the process, but that may be hard. Is there a good use case for that?",
            "    A single binary that takes an input argument to behave differently depending",
            "    on the value of that input argument?"
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 407,
        "question": [
            "What's the difference between `ROCR_VISBLE_DEVICES` and `HIP_VISBLE_DEVICES`?"
        ],
        "answer": [
            "-   I found this discussion about the differences",
            "    or this doc page.",
            "-   `HIP_VISBLE_DEVICES` seems to only affect device indices exposed to HIP applications while `ROCR_VISBLE_DEVICES`",
            "    applies to all applications using the user mode ROCm software stack.",
            "So, in principle, can one use them interchangeably for HIP application?",
            "-   I wouldn't do so because Slurm already uses `ROCR_VISIBLE_DEVICES`. If they get conflicting values",
            "    you may be in for some painful debugging..."
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 408,
        "question": [
            "To be safe is it better to not bind to closest and do it explicitly? I'm not sure if, e.g., for PyTorch, there's direct communication between GPUs."
        ],
        "answer": [
            "-   It is safer indeed. PyTorch uses RCCL as far as I know so yes, it will",
            "    do direct communication between GPUs and given that many GPU configurations",
            "    used for AI have much slower communication via the CPU than direct communication",
            "    between GPUs (NVIDIA links between GPUs are really fast compared to PCIe,",
            "    and the external bandwidth between LUMI GPU packages is also 250 GB/s compared",
            "    to 72 GB/s to the CPU) having good direct communication may be essential for performance."
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 409,
        "question": [
            "If I submit a 256 cores job on 2 nodes without hyperthreading, and if I use the `multi_prog` option of `srun`,"
        ],
        "answer": [
            "what should my program configuration file look like ? I want to be sure that my tasks are on both nodes,",
            "and I am confused by the numbering (does it change depending on the hyperthreading option?).",
            "```",
            "0    ./prog_1.exe",
            "...",
            "127 ./prog_2.exe",
            "128 ./prog_2.exe",
            "...",
            "255 ./prog_2.exe",
            "```",
            "or",
            "```",
            "0    ./prog_1.exe",
            "...",
            "127 ./prog_2.exe <--- Sure? Shouldn't it be prog_1? No. Well, the distribution of the programs among the tasks is another question, but for starter I just want to be sure that I use the 2 nodes",
            "256 ./prog_2.exe",
            "...",
            "383 ./prog_2.exe",
            "```",
            "-   If you want to be sure, I recommend using the tools in the `lumi-CPEtools` module to check how tasks and threads are allocated...",
            "    That's what we also do before we give answers to such questions as we are not a dictionary either that",
            "    know all ins and outs of Slurm without checking things."
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 410,
        "question": [
            "When I bind the CPU using these hex values, do I always use the same mask? This assumes allocation to a full node?"
        ],
        "answer": [
            "In case I'm not using the full node, should I use bindings?",
            "-   All binding parameters only work with the `--exclusive` flag set (which is done implicitely on standard-g).",
            "    You can't affect the binding on small-g (except if you set `--exclusive`.",
            "-   The mask uses 7 cores and one GPU per task and 8 tasks, if you want to use less cores or less GPUs you have to adapt it.",
            "    But if your program uses OpenMP threads on the CPU side, you can still use the \"large\" mask and further",
            "    restrict with the OpenMP environment variables (`OMP_NUM_THREADS`)."
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 411,
        "question": [
            "Refering to slide 36 here,"
        ],
        "answer": [
            "is there a reason why NUMA and GPU numbering are completely independent ? Wouldn't it make more sense,",
            "for simpler usability, to have similar numbering, or if the default binding was the optimal one ?",
            "-   Yes, that is quite annoying but there seems to be some HW reason for that.",
            "    I don't know why it is not possible to map it, so that you don't see it as a user.",
            "-   CCDs get their numbering from the position in the CPU package. GCDs in a package get their numbering",
            "    from their position in the GPU packages, and between GPUs I think some order in communication links will",
            "    determine a numbering when booting the node.",
            "    Now the problem is really to lay all the connections on the circuit board. I'm sure there would be an ordering",
            "    so that they number in the same way, but that may not be physically possible or would require a much more",
            "    expensive circuit board with more layers to make all connections between GCDs and between GCDs and CCDs."
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 412,
        "question": [
            "Probably this depends on the application, but roughly, how much worse is the performance if one does not do the correct CPU --> GPU binding ?"
        ],
        "answer": [
            "-   I believe most spectacular difference we have seen is almost double. It is probably more important",
            "    for HIP codes and GPU to GPU communication.",
            "-   The heavier traffic between CPU and GPU, the larger the difference will be..."
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 413,
        "question": [
            "What if I want to modify one of these provided containers to add some application. How should we do it?"
        ],
        "answer": [
            "-   One possible approach is with singularity overlays https://docs.sylabs.io/guides/3.11/user-guide/persistent_overlays.html"
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 414,
        "question": [
            "Is there anyway to measure the energy/power consumed by the application? +1"
        ],
        "answer": [
            "-   No. In theory it should be possible at the node level, but even that is not implemented at the moment. On a shared node it is simply impossible.",
            "-   ROCm tools can report some numbers but they are known to be unreliable.",
            "Are node-level measurements also not possible on --exclusive booked node?",
            "-   We simply don't have the software that could be called with user rights to gather the data",
            "    from the counters in the node and service modules. And even then the data is very coarse",
            "    and hard to reproduce as on modern computers there is a lot of variability between nodes.",
            "    To get as good a result as possible on the Linpack benchmark for the Top500 they actually",
            "    needed to play with individual power caps for nodes and GPUs to make all of them about as",
            "    fast as it is the slowest GPU that determines progress of the parallel benchmark, while",
            "    they also had to stay within a certain power consumption limit per rack to avoid overheating.",
            "    If you could measure, don't be surprised that when your application runs on a different node,",
            "    power consumption could differ by 20% or more..."
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 415,
        "question": [
            "Is there a way (example, a script) to get the cpu and memory performance of a finished job?"
        ],
        "answer": [
            "-   There is some very coarse information stored in the Slurm accounting database that you can request via `sacct`.",
            "    But this is only overal use of memory and overall consumed CPU time.",
            "When I use `sacct --account=<SLURMJOBID>, it is basically printing the headings but no information related to the job.",
            "May I know what I am missing?",
            "-   If you want to give a jobID the option is `-j` or `--jobs` and not `--account`.",
            "    Moreover, you'll have to specify the output that you want with `-o` or `--format`.",
            "    There is a long field of possible output fields and some examples in the",
            "    `sacct` manual page.",
            "    Often `sacct` only searches in a specific time window for information so depending on",
            "    the options that you use you may have to specify a start and end time for the search."
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 416,
        "question": [
            "We are supposed to use Cray MPI, but when working with containers we need the `singularity-bindings`, correct? I have an open ticket regarding these bindings, and apparently they are not working. Do we have an ETA for when they'll be available again?"
        ],
        "answer": [
            "-   This should be an easy fix. Can you provide the ticket number?",
            "Sure: LUMI #3552",
            "-    Oh, OK, your ticket was in the hand of a member of the team who quit recently so it was not progressing. I will take it."
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 417,
        "question": [
            "How do you deal with hierarchical file formats such as zarr (which have many subfolders and small files) on LUMI?"
        ],
        "answer": [
            "-   I don't know for sure for zarr specifically and how it works with the file system,",
            "    so the answer may not be entirely accurate.",
            "    If those subfolders are INSIDE a big file Lustre only has to deal with the big file and",
            "    it should work well. If it is one of those things that thinks that it should simply dump those",
            "    files and folders as regular files and folders, then it is not a technology that is suitable for",
            "    HPC clusters with parallel file systems. If my quick googling returned the right information",
            "    then it is doing the latter and simply not made for HPC systems. It compares itself with netCDF",
            "    and HDF5 but these are proper technologies for HPC that do the work themselves INSIDE a big file",
            "    rather than letting the regular file system deal with it.",
            "    From all the information I have at the moment, zarr is the perfect example of something mentioned",
            "    in the architecture presentation of the course: Not all technologies developed for workstations or",
            "    for cloud infrastructures, work well on HPC systems (and vice-versa). Zarr is an example of a",
            "    technology built for a totally different storage model than that used on the LUMI",
            "    supercomputer. It may be perfect for a typical cloud use case, where you would be using a fat",
            "    node as a big workstation or a small virtual cluster with its own local file system,",
            "    but at first it looks terrible for a parallel file system shared across a large HPC cluster",
            "    like LUMI.",
            "    On systems the size of LUMI you have no other solution than to work with hierarchies.",
            "    It is the case for the job system: Slurm cannot deal with hundreds of thousands of minute-sized",
            "    jobs but you need to use a hierarchical scheduling system for that. And it is the case for data",
            "    formats. Lustre cannot deal with hundreds of thousands of small files, but you need a hierarchical",
            "    approach with a file system inside a big file. You'd need file system that costs several times more",
            "    per PB to deal with those things at the scale of LUMI."
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 418,
        "question": [
            "What Block Size do you have on the LUSTRE Filesysten? i want to generate one billion 2 byte files"
        ],
        "answer": [
            "-   You're simply not allowed to generate one billion 2 byte files and will never get the file quota for that.",
            "    On the contrary, this will be considered as a denial-of-service attack on the file system and abuse of LUMI",
            "    with all consequences that come with that.",
            "    2 billion 2 byte numbers belong in a single file on an HPC cluster, and you read that file as a whole",
            "    in memory before using the data.",
            "    If the reason to use 1B files is that you want to also run 1B small processes that generate those files",
            "    and that therefore you cannot use a single file: That is also a very, very bad idea, even if you",
            "    use a subscheduler such as HyperQueue",
            "    as just starting those 1B small processes may stretch the metadata service a lot.",
            "    Scaling software is not starting more copies of it, and just starting more copies of a program is not",
            "    what a supercomputer like LUMI is built for. You need a different and way more expensive type of infrastructure",
            "    for that. Scaling would be turning that program into a subroutine that you can call in a loop to generate",
            "    a lot of those 2-byte numbers in a single run and store those intelligently in a single file."
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 419,
        "question": [
            "How can we specify the location such as scracth to store the experiment results (>20GB) generated during the execution?"
        ],
        "answer": [
            "-   The output is usually automatically located at the same directory location from where you submit the job.",
            "-   Hopefully you're not pumping 20GB of output via printf to stdout? That is not a good strategy to get a good I/O bandwidth.",
            "    You should write such files properly with proper C/Fortran library calls. And then it is your program",
            "    or probably the start directory of your program that will determine where the files will end up.",
            "They are HDF5 files. Could you please specify which #SBATCH option you mentioned above to redirect them?",
            "-   No we can't, because it is your specific application that determines where the files will land, not Slurm.",
            "    Maybe they will land in the directory where the application is started (so go to that directory with `cd`",
            "    will do the job), maybe your application does something different. You cannot redirect arbitrary files",
            "    in Slurm, you can only redirect the stdout and stderr devices of Linux.",
            "-   About redirecting stdout and stderr, please see the `sbatch` manual page",
            "    (e.g. `#SBATCH -o /your/chosen/location/output.%a.out`) but indeed this doesn't actually redirect the output created by the application"
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 420,
        "question": [
            "Letâ€™s assume I have one HDF5 file (~300GB), which stores my entire dataset, consisting of videos (~80k)."
        ],
        "answer": [
            "I store each video as a single HDF5 dataset, where each element consists of the bytes of the corresponding",
            "video frame. I spawn multiple threads (pinned to each processor core), which *randomly* access the videos.",
            "What would be some rules of thumb to optimise the Lustre stripping for better performance?",
            "-   I think we need to ask HPE for advice on that and even they may not know.",
            "Besides going for sequential access (e.g., webdataset), is there anything a user can do to limit the I/O bottleneck",
            "involving random access (i.e., typical machine learning workflow)?",
            "-   Random I/O in HDF5 will already be less of a bottleneck for the system than random access to data in",
            "    lots of individual files on the file system. I'd also expect the flash filesystem to perform better",
            "    than the hard disk based file systems. They are charged at 10 times the rate of the hard disk based ones,",
            "    but there is a good reason for that: they were also 10 times as expensive per PB...",
            "-   I think general rule is to use high stripe-count value for such a large dataset files. For instance `-1` will use all OSTs. There are 12 OSTs."
        ],
        "source": "lumi-1day-20240208"
    },
    {
        "idx": 421,
        "question": [
            "Can we get energy consumption from `sacct` on LUMI ?"
        ],
        "answer": [
            "-   No, but you can read them from `/sys/cray/pm_counters/`.",
            "    You need to read those counters before and after the job (meaning before and",
            "    after the `srun` command), then do the math and you can have the energy consumption.",
            "    There are several of them, cpu, gpu and memory.",
            "    They are only available on compute nodes.",
            "    Note though that it only makes sense when using whole nodes for the job, and that",
            "    there are also shared elements in a cluster whose power consumption cannot be",
            "    measured or assigned to individual jobs, e.g., storage and the interconnect."
        ],
        "source": "lumi-2day-20240502"
    },
    {
        "idx": 422,
        "question": [
            "Are there plans to have LUMI as a Globus end point ?"
        ],
        "answer": [
            "-    We don't have it at the moment and unfortunately, we don't have the man power to add it."
        ],
        "source": "lumi-2day-20240502"
    },
    {
        "idx": 423,
        "question": [
            "Do you you other tools e.g., iRODS for data transfer/storage management?"
        ],
        "answer": [
            "I am curious to know if projects use some automation to move data from/to LUMI,",
            "or users do it themselves (irrespective of the size oftheir data)",
            "-   We currently don't have the iRODS tools on LUMI. There was a request from my home country",
            "    (Flanders/Belgium) so their local support team is looking into it. As we shall also discuss",
            "    in the support talk, we are open to help from local support teams for such things so I hope",
            "    they will do the installation in a way that we can then offer the recipe to other users also.",
            "    What we as the LUMI support team cannot do is tell users how it should be configured to access,",
            "    e.g., the VSC or Surf iRODS infrastructures as we are not managing those infrastructures."
        ],
        "source": "lumi-2day-20240502"
    },
    {
        "idx": 424,
        "question": [
            "Can we submit EasyBuild config files to the GitHub official"
        ],
        "answer": [
            "repo",
            "for package we want to share with other users ?",
            "-   Sure. We work with pull requests"
        ],
        "source": "lumi-2day-20240502"
    },
    {
        "idx": 425,
        "question": [
            "Are there plans to bump the available rocm version in the near future ?"
        ],
        "answer": [
            "-   It is planned in summer",
            "-   But LUMI will never be on the bleeding edge. It is a small effort to do so",
            "    on a workstation because it has to work only for you so you upgrade whenever",
            "    you are ready. It is a big upgrade on a supercomputer as it has to work with",
            "    all other software also. E.g., users relying on GPU-aware MPI would not like",
            "    that feature to be broken just because some AI people want the latest ROCm to",
            "    use the latest nightly PyTorch build. GPU software stacks consist of lots of",
            "    libraries running on top of a driver, and you can have only one driver on the",
            "    system and each driver support only a limited range of library versions.",
            "    So you can see that it is hard to keep everybody happy...",
            "    Basically we have to wait until it is supported in all components that are",
            "    important in LUMI for all users."
        ],
        "source": "lumi-2day-20240502"
    },
    {
        "idx": 426,
        "question": [
            "Why do project numbers change every 6 months? This is not convenient"
        ],
        "answer": [
            "-   It depends on your resource allocator. 6 months is short. But the rules of LUMI are that project have a maximum lifetime of 12 months. There are a couple of reasons for this decision, one being that unused data is removed frequently which also so far has let to the scratch not being actively cleared. Another one (I think) is that resources are consumed in the allocated time but this could also be done on the slurm level without closing projects. Maybe Kurt knows more about that.",
            "-   You also have to realise that you are playing with a lot of money. So each project allocator also wants to be able to reconsider decisions: You may have one of the best projects today but next year someone else may have a more brilliant project that deserves the compute time more. LUMI projects can represent a lot of money. I know of EuroHPC Extreme Scale projects that are worth over 2M EURO...",
            "-   And we can also not say precisely how much compute time we will have available in two years. As everybody involved in operatin LUMI learns over time and as use evolves, so does the amount of compute time that can be allocated. Each large supercomputer starts from the assumption that not all users will consume all their compute time so there is often more allocated than available, but how much changes over time as when people get more familiar with a computer, they tend to use a larger fraction of their allocation. In the first year of LUMI operations, we used a large so-called overallocation as too much compute time remained unused, but this has been lowered gradually to keep the lengths of the queue a bit under control."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 427,
        "question": [
            "Why is it impossible to prolong projects or ask for extra resources? It is possible in other CSC servers"
        ],
        "answer": [
            "-   As mentioned in the question above, it was decided to limit project lifetimes to 12 months. It is possible to add extra resources to a project, whether CSC does that or not is their decision (but I'm quite sure they do that if there are good reasons). Of course, CSC can't hand out more allocations than its share of LUMI."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 428,
        "question": [
            "I don't fully understand the difference between quota and storage billing units, since quota can be handled by LUST, but storage billing units not. Thanks!"
        ],
        "answer": [
            "-    The idea is that quota is a hard constraint, you cannot go over that. billing units are used to measure continuous usage. so you can \"save\" billing units by deleting unused files. The idea is that you won't get \"max quota * project length\" billing units, it is usually less. And you have to manage it (e.g. you can load data, do simulation, copy back output and delete everything to keep billing units low). Is a compromise to force users to do some cleanup and don't leave useless data on the machine, but at the same time allow large \"bursts\" of data when running the actual simulations.",
            "-    A project may have a high use but only for a short term. If we would work with quota only, we have to be careful not to distribute too much quota as we never know how much people will use. By also billing actual use, we can use more liberal volume quota and rest assured that people will clean up anyway. In fact, it works so well that we don't need the automatic cleanup of scratch and flash at the moment.",
            "-    Storage billing units are also distributed among the partners proportional to their participation in LUMI and for industrial customers can correspond to actual money. It is not up to LUST to spend the \"money\" of, e.g., EuroHPC, if one of their users wants more storage billing units."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 429,
        "question": [
            "Then do you suggest to use LUMI-O as a staging place to transfer large files temporarily and then transfer to other LUMI file systems?"
        ],
        "answer": [
            "-   Yes, that is often a good approach. Either to store large datasets or to transfer data in and out of LUMI."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 430,
        "question": [
            "When you use 1 GPU for 1 hour, do you consume half GPU core core (due to 2 GPU chip) and plus you also are billed for CPU hours attached to GPU chip or not?"
        ],
        "answer": [
            "-   No, you are not billed for the CPU usage on the GPU nodes. One GCD (in most instances that shows as one GPU), will be billed as 1/2 GPU hour as there are 2 GCDs on each AMD MI250X GPU card."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 431,
        "question": [
            "Is there some documentation/guidance somewhere on explicitly using the entirely of the gpu - i.e. both chiplets?"
        ],
        "answer": [
            "-   The GCDs communicate using MPI or RCCL (but with higher bandwidth than between GPU cards), so usign 2 GCDs is basically the same approach as scaling up to multiple GPUs on one node (or even acroos nodes). It is not possible to use both GCDs as if they are truly one GPU; the bandwidth of the connection is way to slow (total of 200 GB/s per direction compared to 1.6 TB/s peak for the memory of one GCD, and let alone the bandwidth in the caches...)"
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 432,
        "question": [
            "In my `bashrc` I load a lot of module, is there a nice way (beside defining an environment variable) to supres the processing of the `bashrc`?"
        ],
        "answer": [
            "-    as far as i know, `!#/bin/bash` is not processing the bashrc. to launch a new shell that will process the bashrc you need to use `!#/bin/bash -l` . Note that the new shell that you start, even with `!#/bin/bash`, will inherit the environment from the parent (that is why it seems that you run the bashrc there). My personal suggestion is to keep `bashrc` clean, and have a different bash function to load all the modules or by creating an environment file that you can source after login. (Kurt) This also what I use as I have different environments and each even has some aliases defined to quickly go to the directories of the corresponding project."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 433,
        "question": [
            "Why is it possible to submit only 2 dev jobs? And why limit amount of submitted jobs at the first place, can they all just sit in the queue?"
        ],
        "answer": [
            "-   2 is already a lot. The dev partition is meant for actively debugging and profiling. I can't see how a single user can be actively debugging more than one application at a time. It is really for interactive work and if we allow users to use it as any other partition, it doesn't make sense to have a `dev-g`  partition. If we note abuse for running things which are really autonomous \"production\" runs to bypass queues, it is considerd breaking the conditions of use of LUMI.",
            "-   And a limit on the number of jobs submitted: There is a limit to the number of jobs that Slurm can handle. Even if they are just in the queue, because Slurm is continuously re-evaluating permissions. Too many jobs in the queue and (a) it becomes for admins impossible to get an overview of the queue and (b) Slurm would slow down a lot. So the more users a system has, the lower the number of jobs that will be allowed per user. This is just another example of what was said yesterday: Use the right size of system for your work.",
            "-   Moreover, Slurm is meant to schedule significant fractions of a cluster. If you need very fine-grained scheduler, you really need to use a \"scheduler-within-the-scheduler\": A subscheduler in your job to manage your small work."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 434,
        "question": [
            "I have been granted with resources only on standard and standard-g, can I use dev-g for interactive profiling?"
        ],
        "answer": [
            "-    Yes. You receive CPU hours or GPU hours, and you can spend them on all different partitions (standard, small, dev)"
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 435,
        "question": [
            "Is this slide the most recent or the website: https://docs.lumi-supercomputer.eu/runjobs/scheduled-jobs/partitions/ (the dev-g numbers are different)?"
        ],
        "answer": [
            "-   Looks like the slide was not adapted. The number in the docs is currently the correct one. But these numbers do change over time.",
            "-   You can control the actual values using `scontrol show part dev-g`"
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 436,
        "question": [
            "Does this mean that if you run lots of jobs, your next job will be \"lower priority\"? (Or did I misunderstand this? :)"
        ],
        "answer": [
            "-    Ideally, yes. slurm tends to share resources between projects and users. And this is one of the reasons why your job in the queue may be overtaken by one that is more recently submitted."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 437,
        "question": [
            "How can I find the tasks in LUMI that have been finished/calculated and ran in the past two days/24h? Can you give me some tips and guidance?"
        ],
        "answer": [
            "-   The `sacct` command is your friend and was discussed at the end of the presentation."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 438,
        "question": [
            "Where can I find more about scheduling policy of SLURM that prioritize the job? Is it covered in the documentation somewhere?"
        ],
        "answer": [
            "-   No, and it is also not important. It may change without notice to keep the ocupation of the system optimal. Moreover, the conditions of use of LUMI forbid trying to exploit weaknesses in the system for your own advantage, and specifically overengineering jobs for that purpose is one example of this. If you want a job to start quickly, basically be reasonable. Don't ask for more nodes than you need because big jobs tend to increase the waiting time for everybody. Don't ask for more walltime than needed as then your job so that it becomes a better candidate for backfill."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 439,
        "question": [
            "For my understanding of `salloc` and `srun`; If I run `salloc` I will end up on a node, where I have a potentially access on the full node (if I am on a shared node), for the sake of argument assume the node has 10 cores and I requested 5, so when i run an MPI program with 10 ranks, then it would associate each rank with one core. However, if I would go through `srun` then two ranks would go to one core. Is this correct or do I miss something?"
        ],
        "answer": [
            "-   No, `salloc` does not run any job. it just creates the allocation. you will still be on the login node after that. You need to combine `salloc` and `srun` to run on the compute node. `salloc` -> create allocation `srun` -> start the mpi job on the created allocation.",
            "Q: So when I run `salloc` on the login node, then I will be send back to the login node, but not a shell on the compute node. When I then run `srun` on the login node, it would run the command - remotely - on the compute node, where I got my allocation?",
            "-   You are not really sent to compute node with salloc. so technically no going back. salloc command sends a request to scheduler \"hey, i need this resources, please do reserve them\". when the scheduler makes those resources available, the command ends. And you are still in the same place where you executed that command. To login interactively there are different techniques. Note that is possible to go on those allocated node, but via a srun command (e.g. launching a shell script on one of the allocated nodes)."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 440,
        "question": [
            "How I can debug my batch job if slurm refuses to start it due to some mess in parameters? It just says smth like \"resources cannot be allocated\" but does not explain if I asked for too much memory or too many jobs or what's the problem."
        ],
        "answer": [
            "-   Too many jobs is a different error message. And if Slurm complains about the number of resources you asked, there is no other way than to check your script by hand and look into the documentation. For memory there is a rule that is valid on all of LUMI: The amount of physical RAM - 32GB is available to users. Slurm error messages are not very precise, but that is also due to the many different ways in which resources can be requested, and parameters can also conflict with one another."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 441,
        "question": [
            "Is it possible in lumi to `ssh` to a running node to monitor resources ? like running `htop`?"
        ],
        "answer": [
            "-    Not with `ssh`, but yes.",
            "-    The reason for not allowing `ssh` is that everything you start this way, is not under the control of the resource manager. Even though there is an extension to Slurm to ensure that you can only log on to nodes on which you have jobs and that will take care of (trying to) kill all processes you start that way when your last job on a node ends, this is still not safe when multiple users are allowed on the node. The work you start in that `ssh` session is not subject to all your resource limits for your jobs, so you can eat into the resources that were meant for another user and cause trouble for that user."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 442,
        "question": [
            "Maybe a bit offtopic, but can I monitor GPUs load when I run the job? Like you have shown us running htop on CPU example. I mean some analog of `nvtop` but for AMD and for 8 GPUs."
        ],
        "answer": [
            "-   For simple metrics on your GPU usage you can use rocm-smi. For more information you can use profiling tools like rocprof. In the LUMI training materials archive you can find more information. For example, in the AI course we have a session on understanding GPU usage: https://lumi-supercomputer.github.io/LUMI-training-materials/ai-20241126/extra_04_CheckingGPU/. An extensive introduction to rocprof can be found here: https://lumi-supercomputer.github.io/LUMI-training-materials/4day-20241028/extra_3_09_Introduction_to_Rocprof_Profiling_Tool/. If you look at the sessions of the rest of the 4 day course, you can also find OmniTrace and Omniperf to get even more information on your application performance and GPU usage.",
            "-   When using `rocm-smi`, check not only that the load is high (close to 100%) but also that the power consumption is around 300W (if only one GCD of a MI250X is used) or around 500W if using both GCDs per GPU. This is important as a GPU can be busy but in reality it just waits for a data or synchronization without actually doing anything.",
            " -   And there is actually a port of `nvtop` for AMD GPUs and we",
            "     do have a user-installable EasyConfig for it."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 443,
        "question": [
            "Allas storage can be called and streamed directly from the code. Does LUMI-O have the same feature? I mean, python code, that actually process these data, without copying them into the project folder."
        ],
        "answer": [
            "-   You can acces LUMI-O from the compute nodes, so it is possible to access your objects from your application if that is supported.",
            "-   I think `boto3` would be your friend, but there are certainly other options also. Since the last system update it might even be possible to use `rclone` with FUSE (`fusermount3`) to mount it as a filesystem, but for performance that may not be the best idea.",
            "    I have a user in Belgium who is streaming training data for their AI training from a \"Zarr file\" from LUMI-O to LUMI and is happy about the performance. It are questions about zarr from this and another group in my home country that made me look deeper into it for this presentation..."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 444,
        "question": [
            "When using pytorch on LUMI everything looks as if it has cuda. How is it possible?"
        ],
        "answer": [
            "-   It is legacy from when CUDA was the only language for that. AMD tried to map everything 1:1 to nvidia, to ease the adoption of his toolchain so python people didn't even bother to change names"
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 445,
        "question": [
            "Are there any profiling tools available on the gpus, or any that are particularly recommended?"
        ],
        "answer": [
            "-   In our documentation we cover some tools that HPE offers: https://docs.lumi-supercomputer.eu/development/profiling/strategies/.",
            "-   AMD ROCm also comes with tools specifically for the AMD GPUs. In ROCm 6.2 they have been improved a lot, and some that were considered research projects are now production tools. We do have a module on the system for ROCm 6.2, but cannot guarantee full compatibility with the current compilers and MPI library. Our AI users are already using ROCm 6.1 and 6.2 extensively in containers.",
            "-   If you want to learn more about profiling, in October we had a course that covered profiling. The recordings and material can be found in our training material archive: https://lumi-supercomputer.github.io/LUMI-training-materials/Profiling-20241009/. Likely we will have a course like this somehwere in 2025. There is also a lot of material in our Advanced LUMI course about the HPE and AMD tools."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 446,
        "question": [
            "Is the OpenMP implementation buggy for the Fortran compiler or also for the C++? Sorry, I didn't quite get that. Kurt just mentioned some bugs in the OpenMP implementation, but I didn't catch whether it was Fortran-specific or not. No worries! :) thanks"
        ],
        "answer": [
            "-    Which compiler? in general, there may be different in cray compiler or gnu compiler or amd compiler. They use different backend for openmp (gnu uses libgomp, clang uses libomp, and cray has craymp.) you cannot really know if something has bugs, from a SW engineering perspective that is a undecidable problem so you have to think that it may always happen. It is not really common though, as they are tested. The one i think Kurt mentioned is the omp-offload for the gnu stack, which is not even active because there is not great support from amd side for gnu compilers. And i think this applies to all gnu compiler (fortran, c, c++), never tested though.",
            "-    (Kurt) We do have a lot of problems with OpenMP offload and OpenACC in the Cray Fortran compiler. Yet this is still the best option for some users as currently there is no support for that in our GNU compilers (and performance would be very poor anyway) and this is also a work-in-progress on the AMD LLVM-based compilers, where really the big step forward is expected from the switch to the new Flang codebase. I believe that with ROCm 6.3 they will start offering these compilers publicly as an alternative, and maybe by the end of next year or so they may become the default. With the HLRS system that will be AMD (Stuttgart Germany) and the large Fortran codebase they have there, AMD is becoming more and more serious about improving their Fortran support."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 447,
        "question": [
            "Do I need to load a corresponding craype for gpus then when I want to compile in login node? Because on login node it is craype-x86-rome, but I want to run my app on GPU nodes that as I understand needs to have craype-x86-milan?"
        ],
        "answer": [
            "-   This process is called cross-compiling. You can compile on the login nodes for the LUMI-C nodes or for the LUMI-G nodes. For the LUMI-C nodes, all you need to do is `module load craype-x86-milan` and your code will be optimised for zen3. For the GPU nodes it is a little bit more tricky. You need to load `module load craype-x86-trento` (though not doing so wouldn't harm much), but you're likely also need to load the `rocm/6.0.3` module to get access to all the GPU-related stuff. E.g., the HIP compiler for HIP code if you are using the GNU compilers for the CPU part of the code. The Cray compiler can compile HIP code but needs the ROCm module also for tools it uses internally and I bleieve even during compilation for GPU-aware MPI. And with the AMD compilers, you'd be using `PrgEnv-aocc` on the CPU nodes but you can still use `PrgEnv-amd` to compile GPU code on the login nodes."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 448,
        "question": [
            "As I understand we should always use `cc`, `CC` or `ftn` when we compile and configure them, i.e. modifying the default values of some flags. However, what is if we are using something like `cmake` or something that performs jiting (that uses CMake under the hood)? My solution was always to use `export CC=cc` but is this enough?"
        ],
        "answer": [
            "-   Either this or the right CMake -DCMAKE_... variables (I don't know them by hand as I usually let EasyBuild manage them when I prepare instructions for users). But the same holds if you would not be using the wrappers as otherwise CMake would most likely be using the ancient system gcc compilers which are GCC 7.5. For a well-behaving installer this should be enough, but sometimes it may depend a bit on the program if CMake is only used under the hood. Python, e.g., may try to enforce the compilers that were used when compiling Python and try to overwrite whatever you specify.",
            "-   And if you don't use the wrapper, the main difficulty is finding out and specifying paths to libraries. The regular `mpicc` etc. wrappers are provided nowadays in the `cray-mpich` modules and will take care of MPI, but certainly for GNU I would double-check if they use the right compilers and not the system ones.",
            "-   I've had a recent case where there was an issue with the mpi cmake compiler, so keep in mind that it may also be needed to configure cmake with the -DCMAKE_MPI_C_COMPILER=cc etc."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 449,
        "question": [
            "What is a tier 0, 1, 2 supercomputer? How do I know what I am working with?"
        ],
        "answer": [
            "-   Europe has a pyramid model for organising supercomputers. Tier-2 is the smaller supercomputers, often locally available, e.g., at your university. Tier-1 are then larger often national systems, built for larger jobs. Tier-0 are then the very largest computers and so expensive that except for some large countries, they have to be built and operated at an international scale.",
            "-   Each size of machine also has its strengths and weaknesses. Obviously you cannot run a very large job on a small machine. But on the other hand, not all properties scale nicely. E.g., the number of files that a filesystem can process per second does not grow well with the size of the cluster which is why you will hear about expected behaviour on Lustre tomorrow afternoon. LUMIs filesystems have huge bandwidth, but the number of file metadata operations: Open and close, e.g., may not be that much higher than on a much smaller cluster. So you have to work with large files and parallelism in the access of the data within the file. Which is why Emanuele said that it is important to select the right infrastructure for your job.",
            "-   And LUMI is clearly tier-0. There are also a number of petascale machines in the EuroHPC portfolio (currently Meluxina, Karolina, Vega, Discoverer and Deucalion) which are more Tier-1 level."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 450,
        "question": [
            "How do I understand one NUMA node or 2 NUMA node with reference to the compute node?"
        ],
        "answer": [
            "-   A compute node is really the unit that is running its own operating system image. Inside a compute node, all cores, can also access all data.",
            "-   But not every core can access all memory at the same speed. Some memory can be accessed faster than other memory, and that is called Non-Uniform Memory Access or NUMA. A NUMA domain is a group of cores that have equal access to some part of the memory, and the associated memory. The speed different is high depending on whether the memory is on the same socket or a different socket, but within a socket there is also a 20% or so access time difference between memory in the same NUMA node and the other NUMA nodes on that socket."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 451,
        "question": [
            "Is multithreading is off by default because most of the used apps are limited by memory bandwith? Thank you."
        ],
        "answer": [
            "-   It is actually on a the hardware level, but off by default at the level of the scheduler, but only for job steps started with `srun`. Not all programs are memory bandwidth limited. But in fact, hyperthreading helps most for applications that are memory latency limited or have a lot of unpredictable jump instructions. They may also be useful for running a communication thread for each compute thread in the background.",
            "-   There is a very small penalty by turning this on in hardware, but it is negligible, and you are in full control whether you want to use it in your application or not."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 452,
        "question": [
            "What is the difference between amd and rocm modules?"
        ],
        "answer": [
            "-   The `rocm` module allows you to use ROCm only with any other environment while `amd` module is to use AMD compilers with the Cray Programming Environment, including proper versions of LibSci and other libraries."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 453,
        "question": [
            "What is the best way to clean cache after installing a new module?"
        ],
        "answer": [
            "-   `rm -rf ~/.cache/lmod` and the cache will be rebuilt automatically at the next module command that needs it"
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 454,
        "question": [
            "I am a bit confused about how the \"Cray Compiling Environment\" and the \"Cray Programming Environment\" are related to one another. Furthermore, the different compilers, like amd, GNU and so on. I assume that `PrgEnv-GNU` is essentially kind of a wrapper that loads several module in the right way and tells the \"system\" (whatever that is) to use a certain set of compilers."
        ],
        "answer": [
            "-   Cray Programming Environment is the name that is used for the whole software stack that Cray provides for programming. This consists of software they have wholly or partially written themselves, and some software that is really just a repackaging of third party software. Cray Compiling Environment or Cray Compilation Environment (I see both depending on the sources), abbreviated CCE, is the name they use for their C/C++ and Fortran compilers that they provide themselves, with the C/C++ compiler being regular clang/LLVM with some extra plugins, and the Fortran compiler their own on top of LLVM for the actual code generation.",
            "-   `PrgEnv-gnu` may look a bit complex if you check the coded, but it only loads some other modules and sets an environment variable that is probably used by the wrappers. The magic is more in the other modules, that set several modules that are picked up by the wrappers. The wrappers themselves are provided by the `craype` module which is one of the modules load by `PrgEnv-gnu`."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 455,
        "question": [
            "How to check which version is available when I tried `module spider bison`. It gives different version of buildtools? Which one to choose?"
        ],
        "answer": [
            "-    It is showing the buildtools that are associated to the different available stacks, from 24.03 (the latest one), to 22.08. which version of the buildtools you want to use is related to which stack you are using. In this way you can load the buildtools module compiled with the correct stack when you are using that stack to work.",
            "-   There is basically only one other version available, Bison 3.8.2, as Bison doesn't change quickly anymore. You see that at the top of the output of `module spider Bison`, but it is in a lot of versions of the `buildtools` module. Of course, if you load one of these module combinations, you could then also try `bison --version` if you don't believe that the version we used in the module extension Bison is actually the version of the `bison` program."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 456,
        "question": [
            "Which version of modules is loaded by default, the latest? Can I just say 'module load systools' instead of 'module load systools/24.03'?"
        ],
        "answer": [
            "-    After first loading `CrayEnv`, you'll have several `systools` modules but if you would do a `module av` after loading `CrayEnv`, you'd see that the 24.03 version has a `(D)` behind its name. Lmod always tries to take the highest version number and is rather good at figuring out which version this is, even with some rather strange version numbering. In `LUMI/24.03` it ven doesn't matter at the moment as there is only one `systools` module.",
            "-    But loading without a version is dangerous when a new version is installed, or when the default changes. We can also define a different version as the default, and that is something that we often do when a new programming environment is installed on the system. The corresponding `LUMI` module is often still too incomplete so we keep an older one as the default."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 457,
        "question": [
            "A silly question, but when I do `module show PrgEnv-gnu` then I see the line `load(\"gcc-native\")`. On the other hand when I run `module show gcc-native` I see the line `load(\"PrgEnv-gnu\")`, which looks like an infinite loop, but it works, how is that? I know this is a super specific question, but it just very interesting."
        ],
        "answer": [
            "-    To be honest, i don't think is an infinite loop. it just mean that one needs the other so the moment you load one of them, you automatically need to load the other one. is not like when you load the other you unload the first.",
            "-    For me the idea beyond this logic is to prevent using `gcc-native` module with other programming environmnents than `PrgEnv-gnu`; if you really want to do such a mixture load `gcc-native-mixed` module instead. Although actual module script avoids circular loading."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 458,
        "question": [
            "That sounds rather depressing... Often I simply don't know where to get help and just drop the whole (sub)project for that reason. E.g. when I try to reproduce someone's else work from github and simply cannot install all dependencies to make it work."
        ],
        "answer": [
            "-   You are welcome to post support request for a specific piece of software. We usually serve a set of recipes to ease installation process of custom software projects.",
            "-   If is a complex out-of-the-box installation it may still not be supported by LUST, but if you have an EuroHPC-JU project you can try to use the EPICURE support for that (https://epicure-hpc.eu/support-services/)",
            "-   But the problem with such code is often that it is also research code which has",
            "    been tested on just a single system and is also only important to a few users, and is old and not updated for a long time so that it doesn't work anymore with new versions of dependencies, so no support organisation can do miracles there... (I'm myself the author of such a package during my Ph.D. and only made it public with a lot of caveats, but then that one had at least documentation which is now often missing.) Some bigger research groups hire a so-called \"Research Software Engineer\" precisely for such work. It is the computer equivalent for a lab assistant for groups who do experimental research. But somehow the feeling in the compute community is always that it should come for free and someone else should pay the bill... When I did my Ph.D. we had a support group in the department that was about 60% the size of LUST for roughly 100 people instead of 3000....",
            "    In LUMI one of the rules of thumb that we use when we have to select which software installations we can support and which we can't, is that if software hasn't been maintained for 2 years, we don't do it. Even very well known packages run into trouble as they age. E.g., we failed to install GROMACS 2021 versions with the newest compilers on LUMI and had to tell the user that they had to use a newer version."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 459,
        "question": [
            "Which MPI tool would you recommend using? Yes, sorry. Thanks!!"
        ],
        "answer": [
            "-    Do you mean library? the cray-mpich. mpich also is compatible, but does not have access to some optimizations.",
            "-    And we do have a build recipe for Open MPI 4.1.6, but we cannot fully support that one. Moreover, if you use that one, you cannot use any library that is already on the system that uses MPI. It has proven useful though to get a package called ORCA to work on LUMI. But we recommend using cray-mpich whenever possible."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 460,
        "question": [
            "So file size of 15-50 mb is ok? in terms of striping."
        ],
        "answer": [
            "-   It will probably not kill Lustre unless you try to read or write a lot of those files simultaneously from different nodes, but it is still too smal to come anwhere close to what Lustre can deliver. 1 MB is what Cray tells us as the minimal stripe size, but our experience is that it may have to be a lot larger. But if you mean a stripe size of 15-50 MB and not a file size of 15-50 MB, that would be good values to start with.",
            "    Ideally, you also have different processes writing to different chunks in the file but that is not always easy to organise...."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 461,
        "question": [
            "Will there be some exercises to cover some use cases on changing the stripe size and stripe count affect data transfer?"
        ],
        "answer": [
            "-   No unfortunately. If you do this in group, it could become a denial-of-service attack on the Lustre filesystem... In the 4-day course there are some but not to the extent that you ask. And really, the ideal stripe size also depends on how the data is written. It really requires some experimenting with a specific code...",
            "I understand the concern, but I want to know as a user how can I know/experiment with my example, per say if I am running a vasp calculation or an MD run in GROMACS should I tune these parameters for effective data storage?",
            "-   If your job is not very I/O bound, it is not the first thing to tune. I am not a VASP expert and unfortunately our VASP expert has left the team some time ago already. But he turned off an option in VASP in our EasyBuild configuration files because it causes too much trouble on Lustre, and the \".build2\" recipes you will find in our LUMI Software Library is also because of issues between VASP and Lustre (I think VASP opened and closed the same file a bit too often). I hope the next major update of VASP will solve some of these issues... For GROMACS, I haven't really heard of many issues."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 462,
        "question": [
            "If we have for example a 100GB sqlite database where we need 'random' access (e.g. based on index), would you have any suggestions on first steps to improve performance on Lustre? (Is it even possible/feasible in such a case?)"
        ],
        "answer": [
            "-    The database is a single file right? if so, i don't think that is an addressable problem, sadly. That pattern just can't use the underlying parallelism. Note that at least that won't hammer MDS (so is not evil, just inefficient)",
            "TY! It's what we thought, so redesign of the strategy it is! =)",
            "-    And the locking may not work well in Lustre, but that only matters for writing to that database."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 463,
        "question": [
            "Out of curiosity and related to the previous question - Do you have any experience / has anyone done anything with Apache Spark (e.g., via pyspark) for something like that? As in - can we get that to play nicely with Lustre? (Might be a bit of an overkill, but it was something we considered for the redesign of the pipeline)"
        ],
        "answer": [
            "- From all I know, this is more a platform for use in the cloud than on an HPC cluster. In Flanders we had a couple of users for it on one of the HPC cluster, but as far as I know, they have given up on that platform. I didn't see a new installation for a while."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 464,
        "question": [
            "Question related to Hybrid MPI/OpenMP: `ntasks-per-node` defines the number of cores or MPI ranks and `cpus-per-task` defines number of threads per task or per rank? You mentioned that there are two threads per core so how come 16 threads per task is possible? May be I am missing something!"
        ],
        "answer": [
            "-   `ntasks-per-node` does exactly what it says: Tasks per node, so MPI ranks in most cases. It says nothing about cores available to each MPI rank (default is 1). `cpus-per-task`  then says how many cores should be reserved for each task."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 465,
        "question": [
            "So in case of `nodes=1`, `ntasks-per-node = 8` and `cpus-per-task=16` how many cpu hours will be billed if this runs for 1 hour?"
        ],
        "answer": [
            "-   In this case it is easy because you are filling entire nodes on LUMI-C, so the amount of memory that you consume is not a factor. It will be 128 CPU hours per hour that the job runs."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 466,
        "question": [
            "For adjusting the `Makefile` for lumi, does one need to add the corresponding `CFLAGS` line in the `Makefile`?"
        ],
        "answer": [
            "-   That depends. There are other ways to set environment variables that a Makefile can use. And it depends on how the rules are defined in the Makefile. Is it for one of the exercises of this course?",
            "Yes it is the advanced exercise I was trying.",
            "-   They didn't specify an optimisation level while it is typically safer to add that, but apart from that the options are OK for the Cray compiler (`PrgEnv-cray`). For this particular program, which runs in a fraction of a second, optimisation level is of course not that important...",
            "However, to follow the exercise to make this for `lumi`, does one need to replace `-D__HIP_ARCH_GFX90A__` flag to corresponding `rocm` architecture?",
            "-   No, that is why I asked to compile with `make LMOD_SYSTEM_NAME=\"frontier\"`, as that should enable the proper line in the Makefile to do that. The test on line 18 should then ensure that the code on line 19 is executed which adds the proper flags. Frontier has almost exactly the same node type as LUMI, only 3 times more...",
            "-   Just to be more clear, the gpu architecture on lumi is the GFX90A"
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 467,
        "question": [
            "In the example of the Distributed Learning, there are 8 tasks per node requested, shouldn't it be 7, because one is reserved for the OS?"
        ],
        "answer": [
            "-   No, each node contains 8 GCDs. Typically you would run one task per GCD, so 8 in total. I think you may be confused with the number of CPUs that is available per GPU. On a LUMI-G node we have 64 CPU cores and 8 GPUs. So there are 8 CPUs per GPU, however since we operate the nodes in \"low-noise mode\" we reserve 1 CPU core for the OS. As a result you can only request 7 CPUs per GPU."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 468,
        "question": [
            "I have also heard of MPI trampoline which promises more universal access to MPI in the system when installing a software by forwarding mpi related variables. Or is this not well developed for using on LUMI yet?"
        ],
        "answer": [
            "-   All those translation libraries don't do miracles. And many work in the wrong direction:",
            "    You link to the translation library and that one then give you access to several specific implementations. MPI trampoline is of that type: You link MPI trampoline in the application and then when you run you can chose between different MPI implementations. It also does not support Fortran according to its GitHub which already excludes some codes.",
            "    A similar example is FlexiBLAS for BLAS: You link to the FlexiBLAS libraries, but then at runtime you can chose between a number of options. So that only helps us in the container story if the applications in the container would be built with such an MPI library. To be fair, there are some that do runtime translation where the application is linked with one of the regular MPI libraries, and the portability library then pretends to be that library and translates calls instead to another MPI library. The `mpixlate` module on LUMI contains such a tool.",
            "    One problem with tools like MPI trampoline and FlexiBLAS is that you must ensure that when you compile an application, it picks up MPI trampoline and FlexiBLAS instead of the regular MPI libraries and whatever BLAS library you have on the system. Which may be less than trivial as few if any installer knows about those libraries and will auto-detect them as the first candidate,",
            "    Another such example is wi4mpi which is used on the Irene Joliot-Curie supercomputer at TGCC. It actually supports both modes of operation."
        ],
        "source": "lumi-2day-20241210"
    },
    {
        "idx": 469,
        "question": [
            "So I should always use these magic numbers from the slides for cpu binding? I there is a way to derive them somehow from the system?"
        ],
        "answer": [
            "-   The easiest way to find these numbers is indeed from the examples in the documentation. You can derive them also from the diagrams that we use for the LUMI architecture. If you want to get these numbers on the system, it is possible but hard. In that case you can have a look at notes that were made by Kurt (who gave this presentation before). The \"more technical examples\" show how to use `lstopo` to understand what SLURM is doing and how it maps the hardware on the nodes.",
            "-   `lstopo` on an exclusive node in the batch job step will show you the whole structure of the node without interference of Slurm. That is also what I used when writing the `gpu_check` command (based also on some code from ORNL/Frontier)"
        ],
        "source": "lumi-2day-20241210"
    }
]